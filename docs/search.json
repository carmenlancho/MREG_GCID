[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelos de Regresión",
    "section": "",
    "text": "Prefacio\nLos modelos estadísticos han emergido como herramientas fundamentales en la era de la información, donde la capacidad de analizar y predecir comportamientos a partir de datos se ha convertido en una habilidad esencial. En este contexto, los modelos para la predicción juegan un papel crucial al permitirnos describir y cuantificar las relaciones entre variables, así como anticipar resultados futuros. Este libro está diseñado para proporcionar una comprensión profunda y práctica de estas técnicas, basándose en el contenido de la asignatura impartida en el Grado en Ciencia e Ingeniería de Datos.\nA lo largo de los capítulos, encontrarás una combinación de teoría rigurosa y aplicaciones prácticas. Se abordarán temas como la regresión lineal simple y múltiple, métodos de selección de variables y regularización, ingeniería de características y modelos generalizados, entre otros. Además, todos los conceptos se ilustrarán con ejemplos en R, permitiéndote aplicar lo aprendido a conjuntos de datos reales.\nEl objetivo de este libro es doble: por un lado, proporcionar herramientas avanzadas para analizar relaciones sujetas a incertidumbre y, por otro, capacitarte para elegir el método más apropiado para resolver problemas de predicción o explicación, analizando la naturaleza de las variables y sus posibles interacciones. Al finalizar, habrás desarrollado una comprensión sólida de los modelos estadísticos y estarás preparado para enfrentar desafíos en el análisis predictivo con confianza y creatividad.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#filosofía-pedagógica-del-volumen",
    "href": "index.html#filosofía-pedagógica-del-volumen",
    "title": "Modelos de Regresión",
    "section": "Filosofía pedagógica del volumen",
    "text": "Filosofía pedagógica del volumen\nLa filosofía que subyace a la obra es un enfoque “teórico-práctico” deliberado y sin concesiones. No nos conformamos con una mera aplicación de “recetas” o una guía de funciones de software. Buscamos fomentar una comprensión profunda del modus operandi de cada modelo y método. Perseguimos un equilibrio entre la técnica estadística y la estrategia de resolución de problemas, bajo la firme convicción de que la labor práctica se desarrolla con mayor fluidez, creatividad y éxito cuando se cimienta en una comprensión robusta de los principios matemáticos y estadísticos subyacentes, tal y como defiende (Harrell 2015) en su influyente obra.\n\n¿Qué aprenderás con este libro?\nAl completar este recorrido, habrás desarrollado habilidades clave para:\n\nModelar la dependencia entre una variable respuesta y múltiples predictores en conjuntos de datos complejos.\nResolver problemas con iniciativa y creatividad, eligiendo las técnicas estadísticas más adecuadas para cada caso.\nEvaluar de forma crítica las ventajas e inconvenientes de diferentes alternativas metodológicas.\nImplementar estos modelos utilizando software estadístico profesional como R.\nInterpretar correctamente los resultados, proponer mejoras y tomar decisiones basadas en datos.\nAdquirir las competencias y la autonomía necesarias para emprender con éxito estudios de posgrado o proyectos profesionales en ciencia de datos.\n\nAgradecemos a los profesores y colegas que han contribuido al desarrollo de esta asignatura y a la elaboración de este libro. Su dedicación y conocimiento han sido fundamentales para la creación de este recurso.\nEsperamos que esta guía te resulte útil y enriquecedora.\n¡Comenzamos!\n\n\n\n\n\n\nGrado en Ciencia e Ingeniería de Datos\n\n\n\nEste libro presenta el material de la asignatura de Modelos de Regresión del Grado en Ciencia e Ingeniería de Datos de la Universidad Rey Juan Carlos.\n\n\n\n\n\n\n\n\nConocimientos previos\n\n\n\nEs altamente recomendable que los alumnos que cursen esta materia manejen con soltura los conocimientos adquiridos en las asignaturas de Probabilidad y Estadística Matemática, así como herramientas de cálculo univariante, multivariante y álgebra lineal.\n\n\n\n\n\n\n\n\nSobre los autores\n\n\n\nNatalia Madrueño Sierro es graduada en Matemáticas e Ingeniería del Software por la URJC, Master universitario en Ingeniería de Sistemas de Decisión por la URJC, doctora en Tecnologías de la Información y las Comunicaciones por la URJC y profesora del departamento de Informática y Estadística de la URJC. Miembro del grupo de investigación de alto rendimiento en Fundamentos y Aplicaciones de la Ciencia de Datos, DSLAB, de la URJC.\nCarmen Lancho Martín es graduada en Matemáticas y Estadística por la Universidad Complutense de Madrid, doctora en Tecnologías de la Información y las Comunicaciones por la Universidad Rey Juan Carlos y profesora del departamento de Informática y Estadística de la Universidad Rey Juan Carlos. Miembro del grupo de investigación de alto rendimiento en Fundamentos y Aplicaciones de la Ciencia de Datos, DSLAB, de la URJC. Pertenece al grupo de innovación docente, DSLAB-TI\n\n\nEsta obra está bajo una licencia de Creative Commons Atribución-CompartirIgual 4.0 Internacional.\n\n\n\n\nHarrell, Frank E., Jr. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. Second. Springer.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "tema0.html",
    "href": "tema0.html",
    "title": "1  Introducción a los modelos de regresión",
    "section": "",
    "text": "1.1 Predecir vs. explicar\nEste tema inaugural tiene como misión construir el andamiaje conceptual y filosófico sobre el que se asienta el modelado estadístico moderno. A lo largo de estas páginas, contextualizaremos la regresión no solo como una técnica, sino como un marco de pensamiento indispensable en la ciencia de datos y en cualquier disciplina de investigación cuantitativa. Exploraremos en profundidad su propósito dual, desgranaremos sus componentes axiomáticos hasta el último detalle, y ofreceremos una visión panorámica, rica en matices, de la vasta familia de modelos de regresión. El objetivo es preparar al lector, con solidez y sin prisas, para las inmersiones técnicas que seguirán en los capítulos posteriores. Como lectura complementaria que comparte esta filosofía de aprendizaje profundo pero aplicado, recomendamos encarecidamente la obra de (James et al. 2021).\nEl modelado de regresión constituye una de las herramientas más potentes y flexibles del arsenal estadístico. Ofrece un marco metodológico riguroso para investigar y cuantificar las relaciones entre un conjunto de variables, y su aplicabilidad abarca un espectro extraordinariamente amplio de disciplinas: desde la física de partículas y la ingeniería aeroespacial, donde se usa para modelar sistemas complejos, hasta la econometría, la psicometría, la epidemiología o las finanzas, donde es fundamental para entender mercados y comportamientos.\nAunque en la práctica ambos objetivos a menudo se entrelazan, conceptualmente, el modelado estadístico se orienta hacia uno de dos polos, una dicotomía fundamental articulada brillantemente por (Shmueli 2010): la predicción o la inferencia (explicación). Comprender esta distinción es el primer paso para convertirse en un modelador eficaz.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema0.html#sec-proposito",
    "href": "tema0.html#sec-proposito",
    "title": "1  Introducción a los modelos de regresión",
    "section": "",
    "text": "Predicción: El objetivo principal es la precisión. Se busca construir un modelo que pueda estimar con el menor error posible el valor de una variable de interés (la respuesta) basándose en la información proporcionada por otras variables (las predictoras). En este paradigma, el modelo puede ser tratado como una “caja negra” (black box). Su funcionamiento interno o la interpretabilidad de sus componentes son secundarios, siempre y cuando sus predicciones sean consistentemente fiables y robustas en datos no observados previamente.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nUna entidad financiera quiere predecir la probabilidad de que un cliente incurra en impago de un crédito. Utilizan variables como la edad, ingresos, nivel de estudios y historial crediticio. El banco no necesita necesariamente entender la “causa” exacta del impago; su principal interés es tener un modelo que clasifique correctamente a los futuros solicitantes como de alto o bajo riesgo para minimizar pérdidas.\n\n\n\n\nInferencia: El foco se desplaza radicalmente hacia la comprensión y la interpretación. El objetivo no es solo predecir, sino dilucidar la naturaleza de las interdependencias entre las variables. Se busca cuantificar cómo un cambio en una variable predictora influye, ya sea de forma causal o asociativa, en la variable de respuesta. Aquí, la interpretabilidad del modelo es primordial. El interés reside en la magnitud, el signo y, crucialmente, la incertidumbre estadística (expresada mediante errores estándar, intervalos de confianza y p-valores) de los parámetros estimados.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nUna epidemióloga investiga los factores de riesgo de una enfermedad cardíaca. Modela la presión arterial en función de variables como el índice de masa corporal (IMC), el consumo diario de sal y las horas de ejercicio semanales. Su objetivo no es solo predecir la presión arterial de un paciente, sino entender y cuantificar la relación: “¿En cuántos mmHg aumenta la presión arterial, en promedio, por cada gramo adicional de sal consumido al día, manteniendo constantes el IMC y el ejercicio?”. La respuesta a esta pregunta tiene implicaciones directas para la salud pública y las recomendaciones dietéticas.\n\n\n\n\n\n\n\n\n\nUna relación simbiótica\n\n\n\nAunque conceptualmente distintos, ambos objetivos no son mutuamente excluyentes; a menudo se benefician el uno del otro. Un modelo con una base inferencial sólida, que captura relaciones causales o asociativas verdaderas, suele tener un buen rendimiento predictivo. A la inversa, un modelo que demuestra una alta precisión predictiva en datos nuevos nos da confianza en que las relaciones que ha aprendido no son meras casualidades del conjunto de datos de entrenamiento, sino que probablemente reflejen patrones reales y generalizables. La tensión entre interpretabilidad y precisión es uno de los debates más fascinantes en la ciencia de datos moderna.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema0.html#sec-componentes",
    "href": "tema0.html#sec-componentes",
    "title": "1  Introducción a los modelos de regresión",
    "section": "1.2 Anatomía de un modelo de regresión: los componentes axiomáticos",
    "text": "1.2 Anatomía de un modelo de regresión: los componentes axiomáticos\nTodo modelo de regresión, desde el más simple hasta el más sofisticado, se construye sobre tres pilares fundamentales. Estos componentes, definidos en textos clásicos como el de (Kutner et al. 2005), son los ladrillos con los que edificaremos todo nuestro conocimiento.\n\n1.2.1 La variable de respuesta\nTambién designada como variable dependiente, variable de salida, target, variable objetivo o variable explicada. Representa el fenómeno o la característica principal cuyo comportamiento se busca modelar, comprender o predecir. La naturaleza de esta variable es, quizás, el factor más determinante a la hora de elegir el tipo de modelo de regresión. Puede ser:\n\nContinua: Una variable que puede tomar cualquier valor dentro de un rango. Ej: temperatura, altura, precio de una acción, concentración de un compuesto químico.\nDiscreta de Conteo: Una variable que representa un número de eventos. Ej: número de accidentes en una intersección, número de clientes que entran en una tienda, número de mutaciones en un gen.\nBinaria o Dicotómica: Una variable con solo dos resultados posibles. Ej: éxito/fracaso, enfermo/sano, compra/no compra, spam/no spam.\nCategórica: Una variable que representa grupos o categorías. Si no tiene orden, es nominal (ej: tipo de sangre, partido político); si tiene un orden intrínseco, es ordinal (ej: nivel de satisfacción “bajo/medio/alto”, estadio de una enfermedad “I/II/III/IV”).\n\n\n\n1.2.2 Las variables predictoras\nConocidas indistintamente como variables independientes, explicativas, regresoras, covariables o características (features). Son las magnitudes, atributos o factores que se postula que influyen o están asociados con el comportamiento de la variable de respuesta. Al igual que la variable de respuesta, pueden ser de diversa naturaleza (continuas, categóricas, etc.). La selección de estas variables es una de las fases más críticas del modelado, requiriendo una combinación de conocimiento del dominio, análisis exploratorio de datos y técnicas estadísticas formales.\n\n\n1.2.3 El término de error aleatorio\nEste componente, a menudo subestimado, es conceptualmente crucial. Simboliza la variabilidad intrínseca de la variable de respuesta que no es capturada o explicada por las variables predictoras incluidas explícitamente en el modelo. El término de error \\(\\epsilon\\) no es un simple “error” en el sentido de equivocación; es un componente estocástico que amalgama múltiples fuentes de variabilidad:\n\nVariables Omitidas: Ningún modelo es perfecto. Siempre habrá factores que influyen en \\(Y\\) pero que no han sido medidos o incluidos en el modelo (variables latentes).\nError de Medición: Las mediciones de \\(Y\\) (y también de \\(X\\)) pueden no ser perfectamente precisas.\nAleatoriedad Intrínseca: Muchos fenómenos naturales y sociales tienen un componente de variabilidad irreducible. Dos individuos con idénticos valores en todas las variables predictoras pueden, aun así, tener valores distintos en la variable de respuesta.\n\nFormalmente, la relación fundamental de la regresión se expresa como la descomposición de la variable de respuesta en una parte sistemática y una parte aleatoria:\n\\[Y = \\underbrace{f(X_1, \\ldots, X_k)}_{\\text{Componente Sistemática}} + \\underbrace{\\epsilon}_{\\text{Componente Aleatoria}}\\]\ndonde \\(f(\\cdot)\\) denota la componente sistemática (o determinística) del modelo, que representa el valor esperado de \\(Y\\) para unos valores dados de las \\(X\\). La función \\(f\\) es lo que intentamos estimar a partir de los datos. Por su parte, \\(\\epsilon\\) es la componente aleatoria, y gran parte del diagnóstico y la inferencia en regresión se basa en verificar los supuestos que hacemos sobre la distribución de este término (ej: que su media es cero, que su varianza es constante, etc.).\n\n\n\n\n\n\nLinealidad en los parámetros, no en las variables\n\n\n\n\n\nUna característica que define a los modelos de regresión lineal (y que se extiende a muchos otros tipos de modelos) es que la función \\(f(\\cdot)\\) mantiene una relación lineal con respecto a sus parámetros desconocidos (los coeficientes beta, \\(\\beta_j\\)). Es crucial enfatizar que esta “linealidad en los parámetros” no impone una restricción de linealidad en las variables predictoras mismas.\nPor el contrario, es común y metodológicamente válido incorporar transformaciones no lineales de los predictores o interacciones complejas entre ellos para capturar relaciones más sofisticadas. Por ejemplo, el siguiente modelo es un modelo de regresión lineal:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 \\log(X_2) + \\beta_4 (X_1 \\cdot X_2) + \\epsilon\\]\nAunque la relación entre \\(Y\\) y las variables \\(X_1\\) y \\(X_2\\) es claramente no lineal (es cuadrática en \\(X_1\\), logarítmica en \\(X_2\\) e incluye una interacción), el modelo es lineal en los parámetros \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4\\). La función \\(f\\) es una combinación lineal de estos coeficientes. Esta flexibilidad es una de las razones de la enorme potencia de los modelos lineales.\nEl siguiente bloque de código en R genera un ejemplo visual. Simulamos datos que siguen una relación cuadrática y luego ajustamos un modelo lineal que incluye un término cuadrático (\\(X^2\\)). Como se puede observar en la figura, la línea de regresión (azul) captura perfectamente la curvatura de los datos, demostrando que un modelo lineal en sus parámetros puede modelar relaciones no lineales en sus variables.\n\n# Cargar la librería necesaria para la visualización\nlibrary(ggplot2)\n\n# 1. Simulación de datos\nset.seed(42) # Para reproducibilidad\nn &lt;- 100 # Número de observaciones\nx &lt;- runif(n, -5, 5)\n# La relación verdadera es cuadrática: y = 1.5 + 0.5*x + 0.8*x^2 + error\ny &lt;- 1.5 + 0.5 * x + 0.8 * x^2 + rnorm(n, mean = 0, sd = 5)\ndatos &lt;- data.frame(x, y)\n\n# 2. Ajuste del modelo lineal\n# Usamos I(x^2) para indicar que tratamos x^2 como una variable\nmodelo_cuadratico &lt;- lm(y ~ x + I(x^2), data = datos)\n\n# 3. Visualización con ggplot2\nggplot(datos, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"gray40\") + # Puntos de los datos originales\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = FALSE, color = \"#0072B2\", size = 1.2) + # Línea del modelo ajustado\n  labs(\n    title = \"Modelo Lineal con Término Cuadrático\",\n    x = \"Variable Predictora (X)\",\n    y = \"Variable de Respuesta (Y)\"\n  ) +\n  theme_classic(base_size = 14)\n\n\n\n\n\n\n\nFigura 1.1: Ejemplo de un modelo lineal en los parámetros que captura una relación no lineal (cuadrática) en los datos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema0.html#sec-tipos-modelos",
    "href": "tema0.html#sec-tipos-modelos",
    "title": "1  Introducción a los modelos de regresión",
    "section": "1.3 Un viaje preliminar por el universo de los modelos de regresión",
    "text": "1.3 Un viaje preliminar por el universo de los modelos de regresión\nLa regresión lineal clásica, que será el objeto de estudio de los primeros capítulos, es el punto de partida y la piedra angular sobre la cual se erige una prolífica y fascinante gama de metodologías estadísticas avanzadas. Este volumen se dedicará a desentrañar con rigor las siguientes extensiones y especializaciones, que permiten al analista abordar una variedad casi infinita de problemas.\n\n1.3.1 Modelos lineales (LMs)\nConstituyen el paradigma fundamental, el alfabeto sobre el que se escribe el lenguaje del modelado estadístico. Son mucho más que una simple técnica para ajustar una recta a una nube de puntos; son el laboratorio donde se forjan y se comprenden los conceptos esenciales que nos acompañarán durante todo nuestro viaje. Es aquí donde aprenderemos a:\n\nEstimar parámetros e interpretar su significado en el contexto del problema.\nCuantificar la incertidumbre de nuestras estimaciones mediante errores estándar e intervalos de confianza.\nRealizar contrastes de hipótesis para evaluar si la relación entre nuestras variables es estadísticamente significativa o fruto del azar.\nDiagnosticar la “salud” de un modelo, examinando si los supuestos sobre los que se construye son razonables para nuestros datos.\n\nEn su forma más clásica, el modelo lineal asume que la variable de respuesta (y, por consecuencia, el término de error aleatorio) sigue una distribución Normal o Gaussiana. Esta asunción es la clave que desbloquea todo el elegante aparato de la inferencia estadística, permitiéndonos realizar pruebas exactas y derivar propiedades matemáticas bien conocidas. Técnicas tan ubicuas en la ciencia como el Análisis de la Varianza (ANOVA) o el Análisis de la Covarianza (ANCOVA) no son más que casos particulares de la gran familia de los modelos lineales, un hecho que unifica campos de la estadística que históricamente se estudiaban por separado. Dominar los LMs es, sencillamente, un requisito indispensable.\n\n\n1.3.2 Modelos lineales generalizados (GLMs)\nSi los LMs son el alfabeto, los GLMs son la gramática que nos permite construir frases complejas y con significado en una variedad de contextos mucho más amplia. Introducidos en el influyente y verdaderamente revolucionario trabajo de (Nelder y Wedderburn 1972), los GLMs representan un salto conceptual que expande de forma masiva el universo de problemas que podemos abordar. Suponen una generalización elegante que nos permite escapar de la “tiranía” de la distribución Normal y modelar respuestas con una variedad mucho más amplia de naturalezas y escalas.\nEsta flexibilidad se logra mediante la combinación de dos ingeniosos mecanismos que son el corazón de la teoría:\n\nLa familia exponencial de distribuciones: Los GLMs no funcionan con cualquier distribución, sino con aquellas que pertenecen a una “familia” matemática con propiedades muy convenientes: la familia exponencial. Este “club” de distribuciones es muy selecto, pero incluye a miembros tan importantes como la Normal, la Poisson (para datos de conteo), la Binomial (para datos de proporciones o binarios), la Gamma (para datos continuos positivos y asimétricos) o la Binomial Negativa. Su estructura matemática común permite desarrollar una teoría unificada para la estimación de parámetros, lo que es un logro teórico de primer orden.\nLa función de enlace (link function): Este es el verdadero golpe de genialidad. El predictor lineal de nuestro modelo, \\(\\boldsymbol{X\\beta}\\), puede tomar cualquier valor en la recta real, desde \\(-\\infty\\) hasta \\(+\\infty\\). Sin embargo, la media de nuestra variable de respuesta, \\(E[Y] = \\mu\\), a menudo está restringida. Por ejemplo, una probabilidad (\\(\\mu\\) en un modelo binomial) debe estar entre 0 y 1; un conteo (\\(\\mu\\) en un modelo de Poisson) debe ser positivo. La función de enlace, \\(g(\\cdot)\\), actúa como un “traductor” o un “puente” que conecta estos dos mundos. Transforma la media restringida de la respuesta para que pueda ser modelada por el predictor lineal no restringido. La relación fundamental es, por tanto, \\(g(E[Y]) = g(\\mu) = \\boldsymbol{X\\beta}\\).\n\n\nPara datos de conteo (Poisson), se usa un enlace logarítmico (\\(g(\\mu) = \\log(\\mu)\\)). Esto garantiza que, al invertir la función para obtener la media (\\(\\mu = \\exp(\\boldsymbol{X\\beta})\\)), el resultado será siempre positivo, como debe ser un conteo.\nPara datos binarios (Binomial), se usa un enlace logit (\\(g(\\mu) = \\log(\\frac{\\mu}{1-\\mu})\\)). Esta función toma una probabilidad \\(\\mu\\) en el rango (0, 1) y la proyecta sobre toda la recta real, permitiendo que sea modelada por \\(\\boldsymbol{X\\beta}\\).\n\nGracias a los GLMs, podemos usar el mismo marco conceptual de la regresión lineal para modelar una gama de fenómenos increíblemente diversa, desde predecir la cantidad de ciclistas en una ciudad (Poisson) hasta la probabilidad de que un paciente responda a un tratamiento (logística).\n\n\n1.3.3 Modelos de efectos mixtos (Mixed Models)\nSu desarrollo responde a la necesidad crítica de analizar datos que exhiben estructuras de dependencia o correlación, como agrupamientos, anidamientos o jerarquías. En datos estándar, asumimos que las observaciones son independientes, pero esta asunción se viola en casos como: * Medidas repetidas sobre los mismos sujetos (ej: medir la presión arterial de un paciente cada mes). * Datos longitudinales (un tipo de medida repetida a lo largo del tiempo). * Datos agrupados (ej: estudiantes anidados dentro de clases, que a su vez están anidadas dentro de colegios). Estos modelos, detallados en obras como la de (Pinheiro y Bates 2000), introducen explícitamente una estructura de correlación en el término de error mediante la incorporación de efectos aleatorios, que permiten capturar la variabilidad entre los diferentes grupos o individuos, además de los efectos fijos que representan a la población general.\n\n\n1.3.4 Modelos aditivos generalizados (GAMs)\nRepresentan una extensión natural y altamente flexible de los GLMs que relaja el supuesto de linealidad entre el predictor transformado y las covariables. Los GAMs, cuya implementación moderna se debe en gran parte al trabajo de (Wood 2017), permiten modelar estas relaciones mediante funciones suaves no paramétricas (como splines), manteniendo al mismo tiempo la estructura aditiva del modelo. La forma general es \\(g(\\mu) = \\alpha + f_1(x_1) + f_2(x_2) + \\ldots + f_p(x_p)\\), donde las \\(f_i(\\cdot)\\) son funciones suaves de los predictores estimadas a partir de los datos. Esto permite capturar patrones no lineales complejos sin necesidad de especificar una forma funcional paramétrica a priori, logrando un equilibrio excepcional entre flexibilidad e interpretabilidad.\n\n\n\n\n\n\nR como lenguaje del modelado estadístico\n\n\n\n\n\nEste compendio no es un texto puramente teórico. Fusiona intrínsecamente la exposición de los conceptos con su aplicación computacional directa a través del lenguaje y entorno estadístico R. R se ha consolidado como el estándar de facto en la investigación estadística y la ciencia de datos académica por su potencia, flexibilidad y el inmenso ecosistema de paquetes contribuidos por la comunidad científica. Se presupone en el lector una familiaridad operativa básica con R, y se fomenta activamente el desarrollo de una fluidez progresiva mediante la reproducción, modificación y experimentación con los numerosos ejemplos y fragmentos de código presentados.\nLa capacidad de ejecutar análisis en R es fundamental para todo el ciclo de vida del modelado:\n\nLa exploración de datos y la visualización inicial.\nLa estimación de parámetros y el ajuste de los modelos.\nEl diagnóstico riguroso de la adecuación del modelo y la validación de sus supuestos.\nLa producción de gráficos y tablas de alta calidad para comunicar los resultados.\n\nEn R, las herramientas fundamentales para la regresión lineal (lm()) y los modelos lineales generalizados (glm()) están incluidas en el paquete stats, que es uno de los paquetes base y se carga automáticamente con cada sesión. Por lo tanto, no necesitamos instalarlo ni cargarlo.\nA lo largo del libro, extenderemos esta funcionalidad base con paquetes especializados que sí requieren instalación y carga. Entre los más importantes que usaremos se encuentran:\n\nmgcv: La implementación de referencia para GAMs, mantenida por su creador, Simon Wood, y citada en (Wood 2017).\nlme4 y nlme: Los dos paquetes fundamentales para el ajuste de modelos de efectos mixtos, desarrollados por los pioneros en el campo (Pinheiro y Bates 2000; Bates et al. 2015).\nrms: Un paquete y una filosofía de trabajo para implementar estrategias de modelado de regresión robustas, como se detalla en la obra de (Harrell 2015).\ngamair: Contiene numerosos conjuntos de datos que acompañan al libro de (Wood 2017), ideales para practicar con GAMs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema0.html#sec-historia",
    "href": "tema0.html#sec-historia",
    "title": "1  Introducción a los modelos de regresión",
    "section": "1.4 Una breve crónica del desarrollo de la regresión",
    "text": "1.4 Una breve crónica del desarrollo de la regresión\n\n1.4.1 Los orígenes: Galton y la “regresión a la mediocridad”\nLa gestación de la metodología de regresión se traza hasta las investigaciones pioneras de Sir Francis Galton, un polímata de la era victoriana. A finales del siglo XIX, estudiando la herencia de la estatura, Galton recopiló datos de padres e hijos y notó un fenómeno curioso: los padres muy altos tendían a tener hijos altos, pero, en promedio, no tan altos como ellos. Análogamente, los padres muy bajos tenían hijos bajos, pero no tan bajos como ellos. Acuñó el término “regresión a la mediocridad” (hoy diríamos “regresión a la media”) para describir esta tendencia de las características de la descendencia a “regresar” hacia la media de la población, en lugar de perpetuar los extremos de los progenitores (Galton 1886).\n\n\n\n\n\n\nEstudios de Galton sobre estatura\n\n\n\n\n\nDatos recopilados\n\nGalton recopiló datos sobre las estaturas de 928 hijos y sus respectivos padres.\nLas medidas fueron expresadas en pulgadas (1 pulgada = 2.54 cm).\n\nEn sus análisis, utilizó el promedio de las estaturas de ambos padres, conocido como estatura media parental, para compararlo con la estatura de los hijos.\n\nPrincipales hallazgos\n\nRelación lineal entre padres e hijos:\nGalton observó que existe una relación positiva entre la estatura de los padres y la de los hijos. Los padres altos tienden a tener hijos altos, y los padres bajos tienden a tener hijos bajos. Esta relación puede modelarse con una línea recta, lo que inspiró la formulación de la regresión lineal.\nRegresión a la media:\n\nAunque los hijos de padres altos son, en promedio, más altos que el promedio general de la población, también tienden a ser menos altos que sus padres.\n\nDe manera similar, los hijos de padres bajos son más bajos que el promedio general, pero suelen ser menos bajos que sus padres.\n\nEste fenómeno, que Galton llamó “regresión a la media”, ocurre porque las características extremas tienden a suavizarse en la siguiente generación debido a la influencia de múltiples factores genéticos y ambientales.\n\nEcuación de la recta de regresión:\nGalton ajustó una recta para describir la relación entre la estatura media parental (\\(X\\)) y la estatura de los hijos (\\(Y\\)): \\[\nY = \\beta_0 + \\beta_1 X\n\\] Donde:\n\n\\(\\beta_0\\): Intercepto, representa la estatura promedio de los hijos cuando la estatura parental es promedio.\n\\(\\beta_1\\): Pendiente, indica cómo cambia la estatura de los hijos por cada unidad de cambio en la estatura media parental.\n\n\nImportancia en la Estadística\n\nRegresión lineal:\nEste estudio introdujo el concepto de recta de regresión, que describe cómo varía la media de una variable dependiente en función de una variable independiente.\nCorrelación:\nGalton también estudió el grado de relación entre variables, precursor del concepto de coeficiente de correlación desarrollado posteriormente por Karl Pearson, un discípulo suyo.\nRegresión a la media:\nEl término y la idea detrás de “regresión a la media” surgieron de estos estudios y son hoy fundamentales en estadística y genética.\n\nEjemplo Gráfico\nGalton representó sus datos en gráficos de dispersión, mostrando cómo los puntos (pares de estatura media parental y estatura de los hijos) se agrupan alrededor de la recta de regresión, ilustrando la tendencia general de la relación.\n\n# Cargar los paquetes necesarios\nlibrary(ggplot2)\nlibrary(HistData)\n\n# Cargar los datos de Galton\ndata(\"GaltonFamilies\")\n\n# Crear el modelo de regresión lineal para obtener los coeficientes\nmodelo &lt;- lm(childHeight ~ midparentHeight, data = GaltonFamilies)\n\n# Crear la etiqueta para la ecuación de la recta de forma más limpia\n# Usamos sprintf() para un formato más controlado y legible\neq_label &lt;- sprintf(\"y = %.2f + %.2f * x\", coef(modelo)[1], coef(modelo)[2])\n\n# --- Gráfico Mejorado ---\n# Usamos un tema más limpio y colores más suaves para una apariencia profesional.\n# geom_jitter() es mejor que geom_point() para estos datos, ya que evita la superposición de puntos.\nggplot(GaltonFamilies, aes(x = midparentHeight, y = childHeight)) +\n  \n  # 1. Puntos de datos: Usamos geom_jitter para visualizar mejor los puntos superpuestos\n  #    y añadimos transparencia (alpha) para ver la densidad.\n  geom_jitter(alpha = 0.3, color = \"gray50\", width = 0.1, height = 0.1) +\n  \n  # 2. Línea de regresión: En un color azul profesional y más gruesa para que destaque.\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#0072B2\", size = 1.2) +\n  \n  # 3. Anotación: Añadimos la ecuación de la recta de forma elegante,\n  #    usando el mismo color que la línea para crear cohesión visual.\n  annotate(\n    \"text\",\n    x = 66, y = 74, # Posición ajustada para mejor visibilidad\n    label = eq_label,\n    color = \"#0072B2\", # Mismo color que la línea\n    size = 4.5, # Tamaño de la fuente\n    fontface = \"italic\" # Cursiva para la ecuación\n  ) +\n  \n  # 4. Títulos y etiquetas: Mejorados para mayor claridad y contexto.\n  #    Añadimos un subtítulo y una fuente.\n  labs(\n    title = \"Regresión de la Estatura de Hijos vs. Padres\",\n    x = \"Promedio de Estatura de los Padres (pulgadas)\",\n    y = \"Estatura del Hijo/a (pulgadas)\",\n    caption = \"Fuente: Paquete HistData de R\"\n  ) +\n  \n  # 5. Tema: Usamos un tema limpio y profesional como base.\n  theme_classic(base_size = 14)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nDatos históricos del estudio sobre la ‘regresión a la media’.\n\n\n\n\n\n\n\n\n\n1.4.2 La formalización matemática: Legendre y Gauss\nAunque Galton sentó las bases conceptuales e introdujo el término, la formalización matemática de la estimación de parámetros en modelos lineales se atribuye a dos de los más grandes matemáticos de la historia. Adrien-Marie Legendre publicó en 1805 el “Método de los mínimos cuadrados” como un procedimiento numérico para ajustar observaciones astronómicas. Pocos años después, Carl Friedrich Gauss no solo publicó que había desarrollado el mismo método de forma independiente años antes, sino que lo dotó de una profundidad teórica mucho mayor, conectándolo con la teoría de la probabilidad y derivándolo bajo el supuesto de errores distribuidos normalmente, convirtiéndolo en la técnica fundamental para la estimación en modelos lineales que sigue siendo hoy.\n\n\n1.4.3 El desarrollo moderno: la revolución de los GLMs\nA lo largo del siglo XX, la regresión experimentó un desarrollo explosivo. Sin embargo, el hito que probablemente más ha influido en la práctica estadística moderna fue la publicación del artículo sobre Modelos Lineales Generalizados (GLMs) por John Nelder y Robert Wedderburn en 1972 (Nelder y Wedderburn 1972). Esta obra seminal fue revolucionaria porque unificó bajo un mismo paraguas conceptual y computacional diversas clases de modelos que hasta entonces se trataban por separado: la regresión lineal para datos normales, la regresión logística para datos binarios y la regresión de Poisson para datos de conteo. Esto estimuló enormemente el desarrollo de software y la aplicación del modelado estadístico a una nueva y vasta gama de problemas.\n\n\n1.4.4 La evolución contemporánea\nEste legado continúa evolucionando a un ritmo vertiginoso, con la inclusión de modelos jerárquicos y bayesianos, métodos no paramétricos y de machine learning como los árboles de regresión, y la adaptación de la regresión al análisis de datos masivos (big data). La regresión ha evolucionado desde una observación sobre la herencia biológica hasta convertirse en una de las herramientas más versátiles y poderosas del arsenal analítico moderno.\n\n\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, y Steve Walker. 2015. «Fitting Linear Mixed-Effects Models Using lme4». Journal of Statistical Software 67 (1): 1-48.\n\n\nGalton, Francis. 1886. «Regression towards mediocrity in hereditary stature». The Journal of the Anthropological Institute of Great Britain and Ireland 15: 246-63.\n\n\nHarrell, Frank E., Jr. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. Second. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2021. An Introduction to Statistical Learning with Applications in R. Second. Springer.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.\n\n\nNelder, John Ashworth, y Robert WM Wedderburn. 1972. «Generalized linear models». Journal of the Royal Statistical Society Series A: Statistics in Society 135 (3): 370-84.\n\n\nPinheiro, José C., y Douglas M. Bates. 2000. Mixed-Effects Models in S and S-PLUS. New York: Springer.\n\n\nShmueli, Galit. 2010. «To Explain or to Predict?» Statistical Science 25 (3): 289-310.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction with R. Second. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema1.html",
    "href": "tema1.html",
    "title": "2  El modelo de regresión lineal simple",
    "section": "",
    "text": "2.1 Exploración inicial: visualización y cuantificación de la relación\nLa regresión lineal constituye uno de los pilares fundamentales de la modelización estadística. Es, a menudo, el primer y más importante modelo predictivo que se aprende, no solo por su simplicidad e interpretabilidad, sino porque los conceptos que exploraremos aquí son la base sobre la que se construyen técnicas mucho más avanzadas, como el modelo de regresión lineal múltiple, los modelos lineales generalizados (GLM) o incluso conceptos utilizados en algoritmos de machine learning (Draper 1998; Kutner et al. 2005; James et al. 2021).\nEn este capítulo, daremos el primer y más crucial paso en nuestro viaje por el modelado predictivo: el estudio del modelo de regresión lineal simple. Para ello, seguiremos el ciclo de vida completo de un proyecto de modelado: comenzaremos con la exploración visual y cuantitativa de los datos, formalizaremos después nuestras observaciones mediante el lenguaje matemático del modelo y sus supuestos, aprenderemos a estimar sus parámetros, realizaremos inferencias sobre ellos y, finalmente, diagnosticaremos la validez de nuestro modelo (Fox y Weisberg 2018; Harrell 2015).\nLa comprensión profunda que desarrollaremos aquí es esencial, ya que los principios de estimación, inferencia y diagnóstico que aprenderemos son directamente escalables al modelo de regresión lineal múltiple, que exploraremos en el siguiente capítulo.\nAntes de sumergirnos en la teoría de la regresión, debemos hacer lo que todo buen analista hace primero: observar y cuantificar la relación en los datos. Este paso exploratorio es fundamental para formular hipótesis y justificar la elección de un modelo lineal.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#exploración-inicial-visualización-y-cuantificación-de-la-relación",
    "href": "tema1.html#exploración-inicial-visualización-y-cuantificación-de-la-relación",
    "title": "2  El modelo de regresión lineal simple",
    "section": "",
    "text": "2.1.1 Visualización: el gráfico de dispersión\nLa herramienta más potente para examinar la relación entre dos variables continuas es el gráfico de dispersión (scatterplot). Nos permite intuir visualmente la forma, la dirección y la fuerza de la relación. Una inspección visual es siempre el punto de partida.\n\n\n2.1.2 Cuantificación de la asociación: covarianza y correlación\nUna vez que la visualización sugiere una tendencia, necesitamos métricas para cuantificarla.\n\n2.1.2.1 Covarianza\nLa covarianza es una medida de la variabilidad conjunta de dos variables aleatorias, \\(X\\) e \\(Y\\). Nos indica la dirección de la relación lineal. La covarianza muestral, calculada a partir de nuestras observaciones \\((x_i, y_i)\\), es:\n\\[\n\\text{Cov}(x, y) = s_{xy} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\n\\]\nEl principal inconveniente de la covarianza es que su magnitud depende de las unidades de las variables, lo que la hace difícil de interpretar.\n\n\n2.1.2.2 Coeficiente de correlación de Pearson\nPara solucionar el problema de la escala, estandarizamos la covarianza, dividiéndola por el producto de las desviaciones típicas de cada variable. El resultado es el coeficiente de correlación de Pearson (\\(r\\)):\n\\[\nr = r_{xy} = \\frac{s_{xy}}{s_x s_y}\n\\]\nEste coeficiente es adimensional y siempre varía entre -1 y 1, lo que permite una interpretación universal de la fuerza de la asociación lineal.\n\n\n\n\n\n\nEjemplo práctico: Horas de estudio vs. Calificaciones\n\n\n\n\n\nVamos a plantear un problema que nos acompañará durante todo el capítulo: queremos saber si el tiempo de estudio semanal influye en las calificaciones finales.\n\nlibrary(ggplot2)\nset.seed(123) # Para reproducibilidad\n\n# Simulación de datos\ndatos &lt;- data.frame(\n  Tiempo_Estudio = round(runif(100, min = 5, max = 40), 1)\n)\ndatos$Calificaciones &lt;- round(5 + 0.1 * datos$Tiempo_Estudio + rnorm(100, mean = 0, sd = 0.5), 2)\n\n# Visualización\nggplot(datos, aes(x = Tiempo_Estudio, y = Calificaciones)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  labs(\n    title = \"Relación entre Tiempo de Estudio y Calificaciones\",\n    x = \"Tiempo de Estudio (horas/semana)\",\n    y = \"Calificaciones (promedio)\"\n  ) +\n  theme_classic(base_size = 14)\n\n# Cuantificación (los objetos se guardan para usarlos en el texto)\ncovarianza &lt;- cov(datos$Tiempo_Estudio, datos$Calificaciones)\ncorrelacion &lt;- cor(datos$Tiempo_Estudio, datos$Calificaciones)\n\n\n\n\n\n\n\nFigura 2.1: Relación entre tiempo de estudio y calificaciones.\n\n\n\n\n\nEl gráfico muestra una clara tendencia lineal positiva. La covarianza toma un valor de 9.82, y el coeficiente de correlación de Pearson es de 0.9. Ambos valores confirman que la asociación lineal es, además de positiva, muy fuerte. Esta evidencia visual y numérica nos da una base sólida para proponer un modelo de regresión lineal.\n\n\n\n\n\n\n\n\n\n¡Correlación no implica causalidad!\n\n\n\nEl haber encontrado una fuerte correlación positiva entre el tiempo de estudio y las calificaciones (0.9) no nos autoriza a concluir que una cosa causa la otra. La regresión lineal puede demostrar que las variables se mueven juntas y nos permite predecir una a partir de la otra, pero no explica el porqué de la relación.\nPodría existir una tercera variable oculta (p. ej., el interés del alumno en la materia) que influya tanto en las horas de estudio como en las calificaciones. Establecer causalidad requiere un diseño experimental riguroso (asignando aleatoriamente a los estudiantes a diferentes tiempos de estudio), no solo un análisis observacional.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#formulación-teórica-del-modelo",
    "href": "tema1.html#formulación-teórica-del-modelo",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.2 Formulación teórica del modelo",
    "text": "2.2 Formulación teórica del modelo\nUna vez que la exploración sugiere una relación lineal, el siguiente paso es formalizarla matemáticamente. Aquí es donde definimos la estructura teórica del modelo y los supuestos bajo los cuales operará.\n\n2.2.1 El modelo poblacional y sus componentes\nEl modelo poblacional postula que la relación verdadera entre la variable respuesta \\(Y\\) y la predictora \\(X\\) sigue una línea recta, aunque contaminada por cierta aleatoriedad. Para cualquier individuo \\(i\\) de la población, esta relación se describe como:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\nEn esta ecuación, \\(\\beta_0\\) y \\(\\beta_1\\) son los parámetros poblacionales (el intercepto y la pendiente verdaderos pero desconocidos), y \\(\\varepsilon_i\\) es el error aleatorio, un componente fundamental que captura todas las fuentes de variabilidad que el modelo no puede explicar por sí solo. Específicamente, este término incluye:\n\nVariables omitidas: Factores que también afectan a las calificaciones (como la calidad del sueño, la motivación del estudiante o su conocimiento previo) y que no están en el modelo.\nError de medida: Pequeñas imprecisiones al medir las variables (p. ej., un estudiante podría reportar 20 horas de estudio cuando en realidad fueron 19.5).\nAleatoriedad inherente: La variabilidad puramente estocástica o impredecible en el comportamiento humano.\n\nComo nunca observamos la población entera, nuestro trabajo consiste en usar una muestra para estimar el modelo muestral:\n\\[\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]\nAquí, los “gorros” (\\(\\hat{\\cdot}\\)) denotan estimaciones calculadas a partir de la muestra. La diferencia entre el valor real y el predicho, \\(e_i = y_i - \\hat{y}_i\\), se conoce como residuo.\n\n\n2.2.2 Los supuestos del modelo lineal clásico (Gauss-Markov)\nPara que el puente entre nuestro modelo muestral y la realidad poblacional sea sólido, debemos asumir que los errores teóricos \\(\\varepsilon_i\\) se comportan de una manera predecible y ordenada. Estos supuestos, conocidos como condiciones de Gauss-Markov (Kutner et al. 2005; Weisberg 2005), son fundamentales para las propiedades óptimas de los estimadores de mínimos cuadrados.\n\nLinealidad: La relación entre \\(X\\) y el valor esperado de \\(Y\\) es, en promedio, una línea recta: \\(E[Y_i | X_i] = \\beta_0 + \\beta_1 X_i\\).\nIndependencia de los errores: El error de una observación no está correlacionado con el error de ninguna otra: \\(\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0\\) para \\(i \\neq j\\).\nHomocedasticidad: La varianza del error es constante (\\(\\sigma^2\\)) para todos los valores de \\(X\\): \\(Var(\\varepsilon_i | X_i) = \\sigma^2\\). Esto significa que la dispersión de los datos alrededor de la línea de regresión es la misma a lo largo de todos los valores de la variable predictora. La violación de este supuesto se conoce como heterocedasticidad, donde la dispersión de los errores cambia (p. ej., aumenta a medida que \\(X\\) crece).\n\nCuando el objetivo no es sólo estimar la recta, sino inferir con ella, entonces se asume una hipótesis más: la normalidad de la variable respuesta, o lo que es lo mismo, del error aleatorio:\n\nNormalidad de los errores: Para la inferencia, se asume que los errores siguen una distribución Normal con media cero y varianza \\(\\sigma^2\\): \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\).\n\nEstos supuestos son esenciales para garantizar la validez de las estimaciones y conclusiones derivadas del modelo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#estimación-de-los-parámetros",
    "href": "tema1.html#estimación-de-los-parámetros",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.3 Estimación de los parámetros",
    "text": "2.3 Estimación de los parámetros\nNecesitamos un método para encontrar la “mejor” recta de ajuste. El Método de Mínimos Cuadrados Ordinarios (MCO/OLS) nos proporciona este criterio.\n\n2.3.1 El criterio de mínimos cuadrados\nMCO busca la recta que minimice la Suma de los Cuadrados del Error (SSE), es decir, la suma de las distancias verticales al cuadrado entre los puntos observados y la recta de regresión:\n\\[\n\\text{SSE}(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i-\\hat{y})^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\n\\]\n\n\n2.3.2 Derivación matemática de los estimadores\nPara encontrar los valores de \\(\\beta_0\\) y \\(\\beta_1\\) que minimizan esta función, recurrimos al cálculo. Tratamos la SSE como una función de dos variables y calculamos sus derivadas parciales, igualándolas a cero para encontrar el mínimo.\n\\[\n\\frac{\\partial \\text{SSE}}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\]\n\\[\n\\frac{\\partial \\text{SSE}}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\]\nLa resolución de este sistema de dos ecuaciones (conocidas como las ecuaciones normales) nos proporciona las fórmulas para los estimadores de MCO:\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{s_{xy}}{s_{xx}}\n\\] \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\]\n\n2.3.2.1 Interpretación práctica de los coeficientes\nUna vez estimados, los coeficientes tienen una interpretación muy concreta y útil:\n\nPendiente (\\(\\hat{\\beta}_1\\)): Representa el cambio promedio estimado en la variable respuesta \\(Y\\) por cada aumento de una unidad en la variable predictora \\(X\\). En nuestro ejemplo, sería el número de puntos que se espera que aumente la calificación final por cada hora adicional de estudio semanal.\nIntercepto (\\(\\hat{\\beta}_0\\)): Es el valor promedio estimado de la variable respuesta \\(Y\\) cuando la variable predictora \\(X\\) es igual a cero. La interpretación del intercepto solo tiene sentido práctico si \\(X=0\\) es un valor plausible y se encuentra dentro del rango de nuestros datos. De lo contrario (como en nuestro ejemplo, donde nadie estudia 0 horas), a menudo se considera simplemente un ancla matemática para la recta de regresión.\n\n\n\n\n\n\n\nMinimización de SSE\n\n\n\n\n\nLa obtención de los estimadores de mínimos cuadrados para la regresión lineal simple se basa en minimizar la suma de los cuadrados de los residuos (\\(SSE\\)). Este método, desarrollado por Legendre y Gauss a principios del siglo XIX (Galton 1886; Weisberg 2005), es fundamental en la estadística moderna. Aquí está el proceso paso a paso:\nPara minimizar \\(SSE\\), derivamos parcialmente con respecto a \\(\\beta_0\\) y \\(\\beta_1\\) y resolvemos el sistema de ecuaciones.\n\nPrimera derivada con respecto a \\(\\beta_0\\):\n\n\\[\n    \\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n\n    \\left(y_i - (\\beta_0 + \\beta_1 x_i)\\right).\n  \\]\nIgualando a cero: \n\\[\n    \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1\n    x_i\\right) = 0.\n  \\]\nReordenando: \n\\[\n    n\\beta_0 + \\beta_1 \\sum_{i=1}^n x_i = \\sum_{i=1}^n y_i. \\tag{1}\n  \\]\n\nPrimera derivada con respecto a \\(\\beta_1\\):\n\n\\[\n    \\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n x_i\n    \\left(y_i - (\\beta_0 + \\beta_1 x_i)\\right).\n    \\]\nIgualando a cero: \n\\[\n    \\sum_{i=1}^n x_i \\left(y_i - \\beta_0 -\n    \\beta_1 x_i\\right) = 0.\n   \\]\nReordenando: \n\\[\n    \\beta_0 \\sum_{i=1}^n x_i + \\beta_1 \\sum_{i=1}^n x_i^2 = \\sum_{i=1}^n x_i y_i. \\tag{2}\n   \\]\nResolución del Sistema de Ecuaciones\nEl sistema está dado por las ecuaciones (1) y (2):\n\n\\(n\\beta_0 + \\beta_1 \\sum_{i=1}^n x_i = \\sum_{i=1}^n y_i.\\)\n\n\\(\\beta_0 \\sum_{i=1}^n x_i + \\beta_1 \\sum_{i=1}^n x_i^2 = \\sum_{i=1}^n x_i y_i.\\)\n\nResolviendo para \\(\\beta_0\\) y \\(\\beta_1\\):\n\nDe la primera ecuación, despejamos \\(\\beta_0\\):\n\\[\n\\beta_0 = \\frac{\\sum_{i=1}^n y_i - \\beta_1 \\sum_{i=1}^n x_i}{n}. \\tag{3}\n\\]\nSustituimos \\(\\beta_0\\) en la segunda ecuación:\n\\[\n\\frac{\\sum_{i=1}^n y_i - \\beta_1 \\sum_{i=1}^n x_i}{n} \\sum_{i=1}^n x_i + \\beta_1 \\sum_{i=1}^n x_i^2 = \\sum_{i=1}^n x_i y_i.\n\\]\nSimplificando:\n\\[\n\\beta_1 \\left(\\sum_{i=1}^n x_i^2 - \\frac{(\\sum_{i=1}^n x_i)^2}{n}\\right) = \\sum_{i=1}^n x_i y_i - \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n}.\n\\]\nExpresamos \\(\\beta_1\\):\n\\[\n\\beta_1 = \\frac{\\sum_{i=1}^n x_i y_i - \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n}}{\\sum_{i=1}^n x_i^2 - \\frac{(\\sum_{i=1}^n x_i)^2}{n}}.\n\\] Esta es la fórmula para \\(\\beta_1\\), que puede reescribirse como:\n\\[\n\\beta_1 = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)},\n\\] donde \\(\\text{Cov}(x, y)\\) y \\(\\text{Var}(x)\\) son la covarianza y la varianza muestral de \\(x\\) y \\(y\\).\nFinalmente, sustituimos \\(\\beta_1\\) en la ecuación (3) para obtener \\(\\beta_0\\):\n\\[\n\\beta_0 = \\bar{y} - \\beta_1 \\bar{x},\n\\] donde \\(\\bar{x}\\) y \\(\\bar{y}\\) son las medias de \\(x\\) y \\(y\\).\n\n\n\n\nBajo los supuestos del modelo, el Teorema de Gauss-Markov demuestra que estos estimadores son los Mejores Estimadores Lineales Insesgados (MELI / BLUE).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#inferencia-y-bondad-de-ajuste",
    "href": "tema1.html#inferencia-y-bondad-de-ajuste",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.4 Inferencia y bondad de ajuste",
    "text": "2.4 Inferencia y bondad de ajuste\nUna vez hemos estimado los parámetros del modelo, nuestro trabajo apenas ha comenzado. Ahora debemos pasar de la descripción a la inferencia. Necesitamos un conjunto de herramientas que nos permitan responder a preguntas cruciales: ¿Son nuestros coeficientes estimados, \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\), meras casualidades de nuestra muestra o reflejan una relación real en la población? ¿Qué tan bueno es nuestro modelo para explicar la variabilidad de la variable respuesta? Esta sección se dedica a responder estas preguntas.\n\n2.4.1 Propiedades de los estimadores de MCO\nAntes de realizar inferencias, es fundamental entender las propiedades teóricas de los estimadores que hemos calculado.\n\nInsesgadez: Los estimadores de MCO son insesgados. Esto significa que si pudiéramos repetir nuestro muestreo muchísimas veces y calcular los estimadores en cada muestra, el promedio de todas nuestras estimaciones de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) convergería a los verdaderos valores poblacionales \\(\\beta_0\\) y \\(\\beta_1\\). Matemáticamente: \\[\n  E[\\hat{\\beta}_0] = \\beta_0 \\quad \\text{y} \\quad E[\\hat{\\beta}_1] = \\beta_1\n  \\]\nVarianza de los estimadores: Las fórmulas para la varianza de nuestros estimadores cuantifican su precisión. Una varianza pequeña implica que el estimador es más estable a través de diferentes muestras. \\[\n  Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{\\sigma^2}{S_{xx}}\n  \\] \\[\n  Var(\\hat{\\beta}_0) = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right]\n  \\] Donde \\(\\sigma^2\\) es la varianza (desconocida) del término de error \\(\\varepsilon\\).\nTeorema de Gauss-Markov: Este es uno de los resultados más importantes de la teoría de la regresión. Establece que, bajo los supuestos de linealidad, independencia y homocedasticidad (no se requiere normalidad), los estimadores de MCO son los Mejores Estimadores Lineales Insesgados (MELI, o BLUE en inglés). Esto significa que, de entre toda la clase de estimadores que son lineales e insesgados, los de MCO son los que tienen la menor varianza posible.\n\n\n\n\n\n\n\nPropiedades adicionales para las predicciones y para los residuos\n\n\n\n\n\n\nLa suma de los residuos es cero: \\[\n  \\sum_{i=1}^n e_i=\\sum_{i=1}^n(y_i-\\hat{y_i})=0\n  \\]\nLa suma de los valores observados es igual a la suma de los valores ajustados: \\[\n  \\sum_{i=1}^n y_i=\\sum_{i=1}^n \\hat{y_i}\n  \\]\nLa suma de los residuos ponderados por los regresores es cero: \\[\n  \\sum_{i=1}^n x_ie_i=0\n  \\]\nLa suma de los residuos ponderados por las predicciones es cero: \\[\n  \\sum_{i=1}^n \\hat{y_i}e_i=0\n  \\]\nLa recta de regresión contiene el punto \\((\\bar{x},\\bar{y})\\):\\[\\hat{\\beta_0} + \\hat{\\beta_1}\\bar{x} = \\bar{y}\\]\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nPara los datos de calificaciones y tiempo de estudio, estos son los estimadores de los parámetros del modelo de regresión:\n\n# 1. Ajustamos el modelo lineal\nmodelo_estudio &lt;- lm(Calificaciones ~ Tiempo_Estudio, data = datos)\n\n# 2. Obtenemos el resumen completo del modelo\nsummary(modelo_estudio)\n\n\nCall:\nlm(formula = Calificaciones ~ Tiempo_Estudio, data = datos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11465 -0.30262 -0.00942  0.29509  1.10533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.00118    0.11977   41.76   &lt;2e-16 ***\nTiempo_Estudio  0.09875    0.00488   20.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4842 on 98 degrees of freedom\nMultiple R-squared:  0.8069,    Adjusted R-squared:  0.8049 \nF-statistic: 409.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n2.4.2 Estimación de la varianza del error\nLas fórmulas de la varianza de los estimadores dependen de \\(\\sigma^2\\), la varianza del error poblacional, que es desconocida. Por lo tanto, necesitamos estimarla a partir de nuestros datos. Un estimador insesgado de \\(\\sigma^2\\) es la Media Cuadrática del Error (MSE):\n\\[\n\\hat{\\sigma}^2 = \\text{MSE} = \\frac{\\text{SSE}}{n-2} = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n-2}\n\\]\nDividimos por \\(n-2\\), los grados de libertad del error, porque hemos “gastado” dos grados de libertad de nuestros datos para estimar los dos parámetros, \\(\\beta_0\\) y \\(\\beta_1\\). La raíz cuadrada de la MSE, \\(\\hat{\\sigma}\\), se conoce como el error estándar de los residuos y es una medida de la dispersión promedio de los puntos alrededor de la recta de regresión.\n\n2.4.2.1 El error estándar de los residuos y el RMSE\nLa raíz cuadrada de la MSE, \\(\\hat{\\sigma}\\), se conoce formalmente como el error estándar de los residuos (Residual Standard Error). Este valor es nuestra estimación de la desviación estándar del error poblacional, \\(\\sigma\\), y es una medida de la dispersión promedio de los puntos alrededor de la recta de regresión.\n\\[\n\\hat{\\sigma} = \\sqrt{\\text{MSE}}\n\\]\nEn el campo del modelado predictivo y el machine learning, esta misma cantidad se conoce como la Raíz del Error Cuadrático Medio o RMSE (Root Mean Squared Error). Aunque la fórmula es idéntica, la interpretación del RMSE se centra en la evaluación del rendimiento predictivo del modelo. El RMSE nos dice, en promedio, cuál es la magnitud del error de predicción de nuestro modelo, y tiene la ventaja de estar en las mismas unidades que la variable respuesta \\(Y\\). Por ejemplo, si estamos prediciendo precios de viviendas en euros, un RMSE de 5000 significa que nuestras predicciones se desvían, en promedio, unos 5000 € de los precios reales.\n\n\n\n2.4.3 Análisis de la Varianza (ANOVA) para la significancia de la regresión\nUna vez hemos estimado los coeficientes, necesitamos una prueba formal para determinar si el modelo en su conjunto es útil. Es decir, ¿la variable predictora \\(X\\) explica una porción de la variabilidad de la variable respuesta \\(Y\\) que sea estadísticamente significativa, o la relación que observamos podría deberse simplemente al azar? El Análisis de la Varianza (ANOVA) nos proporciona la herramienta para responder a esta pregunta a través del contraste F de significancia global.\nLas hipótesis de este contraste son:\n\n\\(H_0: \\beta_1 = 0\\): La hipótesis nula postula que no existe una relación lineal entre \\(X\\) e \\(Y\\). El modelo no tiene poder explicativo y no es mejor que usar simplemente la media, \\(\\bar{y}\\), como predicción para cualquier valor de \\(x\\).\n\\(H_1: \\beta_1 \\neq 0\\): La hipótesis alternativa sostiene que sí existe una relación lineal significativa.\n\n\n\n\n\n\n\nRepaso\n\n\n\nEs conveniente repasar el tema de Análisis de la Varianza estudiado en la asignatura de Inferencia, ya que los conceptos son directamente aplicables aquí.\n\n\nLa idea fundamental del ANOVA es comparar la variabilidad que nuestro modelo explica con la variabilidad que no puede explicar (el error residual). Para ello, se descompone la variabilidad total de nuestras observaciones (\\(y_i\\)) en dos partes ortogonales.\n\nLa Suma Total de Cuadrados (SST) mide la variabilidad total de los datos alrededor de su media. Es nuestra referencia base de la dispersión total que hay que explicar. \\[\n\\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n\\]\nEsta variabilidad se descompone en:\n\nSuma de Cuadrados de la Regresión (SSR): Mide la parte de la variabilidad total que es explicada por nuestro modelo. Cuantifica cuánto se desvían las predicciones del modelo (\\(\\hat{y}_i\\)) de la media general (\\(\\bar{y}\\)). \\[\n  \\text{SSR} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\n  \\]\nSuma de Cuadrados del Error (SSE): Mide la variabilidad residual, es decir, la parte que el modelo no puede capturar. Cuantifica la dispersión de los puntos reales (\\(y_i\\)) alrededor de la recta de regresión (\\(\\hat{y}_i\\)). \\[\n  \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n  \\]\n\n\nLa descomposición fundamental de la varianza es, por tanto: \\(\\text{SST} = \\text{SSR} + \\text{SSE}\\).\nPara poder comparar estas sumas de cuadrados de forma justa, las estandarizamos dividiéndolas por sus respectivos grados de libertad, obteniendo así las Medias Cuadráticas (MS):\n\\[\n\\text{MSR} = \\frac{\\text{SSR}}{1} \\quad \\quad \\quad \\text{MSE} = \\frac{\\text{SSE}}{n-2}\n\\]\nFinalmente, el estadístico F se construye como el cociente entre la variabilidad explicada por el modelo y la variabilidad no explicada:\n\\[\nF = \\frac{\\text{MSR}}{\\text{MSE}}\n\\]\nIntuitivamente, el estadístico F actúa como una ratio de señal a ruido. La MSR (la “señal”) representa la variabilidad que nuestro modelo captura sistemáticamente, mientras que la MSE (el “ruido”) representa la variabilidad aleatoria o residual. Un valor de F grande nos dice que la señal es mucho más fuerte que el ruido, lo que apoya la hipótesis de que la relación que hemos modelado es real y no fruto del azar.\nToda esta información se organiza de forma estándar en la tabla ANOVA:\n\n\n\nFuente\n\\(df\\)\n\\(SS\\)\n\\(MS = SS/df\\)\nEstadístico \\(F\\)\n\n\n\n\nRegresión\n1\n\\(SSR\\)\n\\(MSR\\)\n\\(F = MSR/MSE\\)\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE\\)\n\n\n\nTotal\n\\(n-1\\)\n\\(SST\\)\n\n\n\n\n\nBajo la hipótesis nula (\\(H_0: \\beta_1 = 0\\)), el estadístico \\(F\\) sigue una distribución \\(F\\) con 1 y \\(n-2\\) grados de libertad. Si el p-valor asociado a nuestro estadístico F es suficientemente pequeño (\\(p &lt; \\alpha\\)), rechazamos \\(H_0\\) y concluimos que nuestro modelo tiene un poder explicativo estadísticamente significativo.\n\n\n2.4.4 Bondad del ajuste: coeficiente de determinación\nEl coeficiente de determinación (\\(R^2\\)) es una medida clave que cuantifica qué proporción de la variabilidad total observada en la muestra (\\(y_i\\)) es explicada por la relación lineal con \\(X\\) a través del modelo. Su fórmula se deriva de la descomposición de la varianza:\n\\[\nR^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n\\]\nDonde las sumas de cuadrados se calculan a partir de los datos muestrales:\n\n\\(\\text{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2\\): Suma Total de Cuadrados, mide la variabilidad total de las observaciones.\n\\(\\text{SSR} = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\\): Suma de Cuadrados de la Regresión, mide la variabilidad explicada por el modelo.\n\\(\\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\): Suma de Cuadrados del Error, mide la variabilidad no explicada (residual).\n\nUn \\(R^2\\) cercano a 1 indica que el modelo ajusta bien los datos, mientras que un \\(R^2\\) cercano a 0 indica un ajuste pobre.\n\n\n\n\n\n\nRelación entre R² y el coeficiente de correlación\n\n\n\n\n\nEn el caso específico del modelo de regresión lineal simple, existe una relación directa y simple: el coeficiente de determinación \\(R^2\\) es literalmente el cuadrado del coeficiente de correlación de Pearson (\\(r\\)) entre \\(X\\) e \\(Y\\).\n\\[ R^2 = (r_{xy})^2 \\]\nEsto refuerza la idea de que ambos miden la fuerza de la asociación lineal, aunque \\(R^2\\) lo hace desde la perspectiva de la varianza explicada por el modelo.\n\n\n\n\n\n\n\n\n\nInterpretación de R²\n\n\n\nEl coeficiente de determinación, \\(R^2\\), es una métrica muy popular, pero su interpretación requiere cautela. Un valor alto no garantiza un buen modelo, y un valor bajo no siempre implica un modelo inútil. Es fundamental tener en cuenta las siguientes observaciones:\n\n\\(R^2\\) no mide la linealidad de la relación. Un modelo puede tener un \\(R^2\\) muy alto incluso si la relación subyacente entre las variables \\(X\\) e \\(Y\\) no es lineal. Por ello, un \\(R^2\\) elevado nunca debe sustituir a un análisis gráfico de los residuos para verificar el supuesto de linealidad.\n\\(R^2\\) es sensible al rango de la variable predictora \\(X\\). Si el modelo de regresión es adecuado, la magnitud de \\(R^2\\) aumentará si aumenta la dispersión de las observaciones \\(x_i\\) (es decir, si \\(S_{xx}\\) crece). Esto se debe a que un mayor rango en \\(X\\) tiende a aumentar la Suma Total de Cuadrados (SST), lo que puede inflar el valor de \\(R^2\\) sin que la precisión del modelo (medida por la MSE) haya mejorado.\nUn rango restringido en \\(X\\) puede producir un \\(R^2\\) artificialmente bajo. Como consecuencia del punto anterior, si los datos se han recogido en un rango muy estrecho de la variable \\(X\\), el \\(R^2\\) puede ser muy pequeño, aunque exista una relación fuerte y significativa entre las variables. Esto podría llevar a la conclusión errónea de que el predictor no es útil.\n\n\n\n\n\n\n\n\n\nPara pensar\n\n\n\nEn regresión simple, \\(R^2\\) mide la proporción de variabilidad de \\(Y\\) explicada por el modelo con una sola \\(X\\).\nPregunta: si, además de esa \\(X\\), pudieras añadir una segunda variable \\(Z\\) que en realidad no tiene relación con \\(Y\\) (por ejemplo, puro ruido), ¿crees que el \\(R^2\\) del modelo con \\(X\\) y \\(Z\\) podría ser menor que el del modelo con solo \\(X\\)? Puedes hacer simulaciones si te ayuda a pensar.\n\n\n\n\n2.4.5 Inferencia sobre los coeficientes\nAdemás de la prueba F global, podemos realizar inferencias sobre cada parámetro individualmente. Para ello, necesitamos el supuesto de normalidad de los errores.\n\n2.4.5.1 Distribución de los estimadores\nBajo el supuesto de normalidad, se puede demostrar que los estimadores también siguen una distribución Normal: \\[\n\\hat{\\beta}_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{S_{xx}}\\right) \\quad \\quad \\quad \\hat{\\beta}_0 \\sim N\\left(\\beta_0, \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right]\\right)\n\\] Al estandarizar y reemplazar la desconocida \\(\\sigma^2\\) por su estimador \\(\\hat{\\sigma}^2 = \\text{MSE}\\), obtenemos un estadístico que sigue una distribución t-Student con \\(n-2\\) grados de libertad: \\[\nt = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\text{SE}(\\hat{\\beta}_1)} \\sim t_{n-2}\\] donde \\(\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\frac{\\text{MSE}}{S_{xx}}}\\) es el error estándar del estimador \\(\\hat{\\beta}_1\\).\n\n\n2.4.5.2 Contraste de hipótesis para la pendiente\nEl contraste más común es el de la significancia de la pendiente: \\[ H_0: \\beta_1 = 0 \\text{ vs } H_1: \\beta_1 \\neq 0 \\]\nBajo \\(H_0\\), el estadístico de contraste es: \\[\nt_0 = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}\\]\nRechazamos \\(H_0\\) si \\(|t_0| &gt; t_{\\alpha/2, n-2}\\) o, equivalentemente, si el p-valor asociado es menor que \\(\\alpha\\).\n\n\n\n\n\n\nRelación entre el contraste F y el contraste t\n\n\n\nEn el contexto de la regresión lineal simple (y solo en este caso), el contraste F para la significancia global del modelo es matemáticamente equivalente al contraste t para la significancia del coeficiente \\(\\beta_1\\). Se puede demostrar que \\(F = t^2\\), y el p-valor de ambos contrastes será idéntico.\n\n\n\n\n2.4.5.3 Intervalo de confianza para la pendiente\nA partir de la distribución t, podemos construir un intervalo de confianza al \\(100(1-\\alpha)\\%\\) para el verdadero valor de la pendiente \\(\\beta_1\\): \\[\n\\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\cdot \\text{SE}(\\hat{\\beta}_1)\n\\] Este intervalo nos da un rango de valores plausibles para el efecto de \\(X\\) sobre \\(Y\\). Si el intervalo no contiene el cero, es equivalente a rechazar la hipótesis nula \\(H_0: \\beta_1 = 0\\).\n\n\n\n\n\n\nPara recordar\n\n\n\nEn los programas estadísticos se suele proporcionar el p-valor del contraste. Puedes repasar el significado de p-valor proporcionado en la asignatura de Inferencia.\n\n\n\n\n\n\n\n\nEjemplo: Interpretación del summary\n\n\n\n\n\nLa función summary() en R nos proporciona toda esta información.\n\nsummary(modelo_estudio)\n\n\nCall:\nlm(formula = Calificaciones ~ Tiempo_Estudio, data = datos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11465 -0.30262 -0.00942  0.29509  1.10533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.00118    0.11977   41.76   &lt;2e-16 ***\nTiempo_Estudio  0.09875    0.00488   20.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4842 on 98 degrees of freedom\nMultiple R-squared:  0.8069,    Adjusted R-squared:  0.8049 \nF-statistic: 409.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretación:\n\nCoefficients: El p-valor para Tiempo_Estudio (&lt;0.001) es muy pequeño, por lo que rechazamos \\(H_0\\) y concluimos que la variable es un predictor significativo.\nR-squared: El valor de \\(R^2\\) (0.81) nos indica que el 81% de la variabilidad en las calificaciones es explicada por el tiempo de estudio.\nF-statistic: El p-valor del estadístico F es \\(&lt;0.001\\), esto confirma que el modelo en su conjunto es estadísticamente significativo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#predicción-de-nuevas-observaciones",
    "href": "tema1.html#predicción-de-nuevas-observaciones",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.5 Predicción de nuevas observaciones",
    "text": "2.5 Predicción de nuevas observaciones\nUna vez que hemos ajustado y validado un modelo de regresión, uno de sus propósitos más importantes es utilizarlo para hacer predicciones. Sin embargo, es fundamental distinguir entre dos tipos de predicción:\n\nEstimar la respuesta media para un valor dado de \\(X\\). Por ejemplo: “¿Cuál es la calificación promedio que esperamos para todos los estudiantes que estudian 25 horas semanales?”.\nPredecir una respuesta individual para un valor dado de \\(X\\). Por ejemplo: “Si un estudiante concreto estudia 25 horas semanales, ¿entre qué valores esperamos que se encuentre su calificación?”.\n\nEstos dos objetivos, aunque parecidos, responden a preguntas distintas y manejan diferentes fuentes de incertidumbre, lo que da lugar a dos tipos de intervalos.\n\n2.5.1 Intervalo de confianza para la respuesta media\nEste intervalo estima el valor esperado de \\(Y\\) para un valor concreto del regresor, \\(x_0\\). Su objetivo es acotar dónde se encuentra la línea de regresión poblacional verdadera para ese punto \\(x_0\\). La estimación puntual es \\(\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0\\).\nEl intervalo de confianza al \\(100(1-\\alpha)\\%\\) para la respuesta media \\(E[Y|X=x_0]\\) viene dado por:\n\\[\n\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{\\text{MSE} \\left( \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}} \\right)}\n\\]\nLa anchura de este intervalo depende de dos fuentes de error: la incertidumbre en la estimación de la recta y la distancia del punto \\(x_0\\) a la media \\(\\bar{x}\\). El intervalo es más estrecho cerca del centro de los datos y más ancho en los extremos.\n\n\n2.5.2 Intervalo de predicción para una respuesta individual\nEste intervalo es el que debemos usar cuando queremos predecir el valor para una única observación futura, no para la media. Este intervalo debe tener en cuenta dos fuentes de variabilidad:\n\nLa incertidumbre sobre la localización de la verdadera recta de regresión (la misma que en el intervalo de confianza).\nLa variabilidad inherente de una observación individual alrededor de la recta de regresión (el error aleatorio \\(\\varepsilon_i\\), cuya varianza estimamos con la MSE).\n\nPor esta razón, el intervalo de predicción siempre será más ancho que el intervalo de confianza para la respuesta media. El intervalo de predicción al \\(100(1-\\alpha)\\%\\) para una observación futura \\(y_0\\) en el punto \\(x_0\\) es:\n\\[\n\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{\\text{MSE} \\left( 1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}} \\right)}\n\\]\nLa única diferencia matemática es el “+1” dentro de la raíz cuadrada, que representa la varianza \\(\\sigma^2\\) del error de una sola observación.\n\n2.5.2.1 Predicción para la media de m observaciones futuras\nSi se desea un intervalo de predicción para la media de m futuras observaciones en un valor \\(x_0\\), la fórmula se modifica ligeramente. Este intervalo será más estrecho que el de una sola observación pero más ancho que el de la respuesta media:\n\\[\n\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{\\text{MSE} \\left( \\frac{1}{m} + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}} \\right)}\n\\]\n\n\n\n\n\n\nEjemplo práctico: Predicción de calificaciones\n\n\n\n\n\nVamos a calcular y visualizar los intervalos para nuestro modelo de estudio. Usaremos la función predict() de R, que calcula estos intervalos de forma automática.\n\n# 1. Crear una secuencia de nuevos valores de X para predecir\nnuevos_datos &lt;- data.frame(\n  Tiempo_Estudio = seq(min(datos$Tiempo_Estudio), max(datos$Tiempo_Estudio), length.out = 100)\n)\n\n# 2. Calcular el intervalo de confianza para la RESPUESTA MEDIA\nconf_interval &lt;- predict(\n  modelo_estudio, \n  newdata = nuevos_datos, \n  interval = \"confidence\", \n  level = 0.95\n)\n\n# 3. Calcular el intervalo de predicción para una OBSERVACIÓN INDIVIDUAL\npred_interval &lt;- predict(\n  modelo_estudio, \n  newdata = nuevos_datos, \n  interval = \"prediction\", \n  level = 0.95\n)\n\n# 4. Unir todo para graficar con ggplot2\nplot_data &lt;- cbind(nuevos_datos, as.data.frame(conf_interval), pred_pred = as.data.frame(pred_interval))\ncolnames(plot_data) &lt;- c(\"Tiempo_Estudio\", \"fit_conf\", \"lwr_conf\", \"upr_conf\", \"fit_pred\", \"lwr_pred\", \"upr_pred\")\n\n# 5. Visualización\nggplot() +\n  # Capa 1: Puntos originales del dataframe 'datos'\n  geom_point(data = datos, aes(x = Tiempo_Estudio, y = Calificaciones), color = \"#0072B2\", alpha = 0.7) +\n  \n  # Capa 2: Línea de regresión del dataframe 'plot_data'\n  geom_line(data = plot_data, aes(x = Tiempo_Estudio, y = fit_conf), color = \"black\", linewidth = 1) +\n  \n  # Capa 3: Banda de predicción (roja) del dataframe 'plot_data'\n  geom_ribbon(data = plot_data, aes(x = Tiempo_Estudio, ymin = lwr_pred, ymax = upr_pred), fill = \"red\", alpha = 0.2) +\n  \n  # Capa 4: Banda de confianza (azul) del dataframe 'plot_data'\n  geom_ribbon(data = plot_data, aes(x = Tiempo_Estudio, ymin = lwr_conf, ymax = upr_conf), fill = \"blue\", alpha = 0.3) +\n  \n  # Etiquetas y tema\n  labs(\n    title = \"Intervalos de Confianza y Predicción\",\n    x = \"Tiempo de Estudio (horas/semana)\",\n    y = \"Calificaciones (promedio)\",\n    caption = \"La banda azul (más estrecha) es el IC del 95% para la media.\\nLa banda roja (más ancha) es el IP del 95% para una nueva observación.\"\n  ) +\n  theme_classic(base_size = 14)\n\n\n\n\n\n\n\nFigura 2.2: Comparación visual del intervalo de confianza (azul, más estrecho) y el intervalo de predicción (rojo, más ancho).\n\n\n\n\n\nEl gráfico muestra claramente que la incertidumbre al predecir una calificación individual es mucho mayor que la incertidumbre al estimar la calificación promedio. Ambas bandas se ensanchan al alejarse del centro de los datos.\nSi quisiéramos una predicción para un estudiante que estudia 25 horas:\n\ndato_nuevo &lt;- data.frame(Tiempo_Estudio = 25)\n\n# Guardamos la predicción para la media en un objeto\npred_media &lt;- predict(modelo_estudio, newdata = dato_nuevo, interval = \"confidence\")\n\n# Guardamos la predicción para un individuo en un objeto\npred_indiv &lt;- predict(modelo_estudio, newdata = dato_nuevo, interval = \"prediction\")\n\nInterpretación:\n\nCon un 95% de confianza, la calificación promedio de los estudiantes que estudian 25 horas está entre 7.37 y 7.57.\nCon un 95% de confianza, la calificación de un estudiante concreto que estudia 25 horas estará entre 6.5 y 8.44.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#diagnóstico-del-modelo",
    "href": "tema1.html#diagnóstico-del-modelo",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.6 Diagnóstico del Modelo",
    "text": "2.6 Diagnóstico del Modelo\nUna vez que hemos ajustado un modelo y evaluado su significancia, el trabajo no ha terminado. Un paso crucial, a menudo subestimado, es el diagnóstico del modelo (Fox y Weisberg 2018; Harrell 2015). Este proceso consiste en verificar si se cumplen los supuestos del modelo de regresión lineal clásico. La fiabilidad de nuestras inferencias (los p-valores de los contrastes t y F, y los intervalos de confianza) depende directamente de la validez de estos supuestos.\nEl diagnóstico se realiza principalmente a través del análisis de los residuos del modelo (\\(e_i = y_i - \\hat{y}_i\\)). Los residuos son nuestra mejor aproximación empírica de los errores teóricos no observables (\\(\\varepsilon_i\\)). A continuación, se detalla cómo verificar cada uno de los supuestos clave.\n\n2.6.1 Linealidad\nEste supuesto establece que la relación entre la variable predictora \\(X\\) y el valor esperado de la variable respuesta \\(Y\\) es, en promedio, una línea recta: \\(E[Y | X] = \\beta_0 + \\beta_1 X\\).\nMétodos de diagnóstico:\n\nDiagnóstico visual: El gráfico de residuos (\\(e_i\\)) frente a los valores ajustados por el modelo (\\(\\hat{y}_i\\)) es la herramienta fundamental. La lógica es sencilla pero potente: si el modelo lineal es adecuado, los errores que comete (los residuos) deberían ser completamente aleatorios, sin guardar relación alguna con la magnitud de las predicciones.\nTest de Ramsey RESET: Este test estadístico detecta violaciones de la forma funcional mediante la inclusión de términos no lineales (\\(\\hat{y}^2, \\hat{y}^3, ...\\)) en el modelo.\n\nH₀: La forma funcional es correcta (lineal)\n\nH₁: La forma funcional es incorrecta (no lineal)\n\nEstadístico: Sigue una distribución F bajo H₀\n\nInterpretación: p-valor bajo indica violación de linealidad\n\n\nEn un escenario ideal, el gráfico debería parecer una nube de puntos distribuida horizontalmente y sin estructura aparente, centrada en la línea del cero. Esto nos indica que los errores son, en promedio, nulos para todos los niveles de predicción, cumpliendo así el supuesto de linealidad. La línea roja que R superpone en este gráfico, que suaviza la tendencia de los puntos, debería ser prácticamente plana y pegada al cero, confirmando la ausencia de patrones.\n\n\n\n\n\n\nEjemplo de un modelo válido\n\n\n\n\n\nPara nuestro modelo_estudio, podemos generar específicamente el primer gráfico de diagnóstico, que es el de Residuos vs. Valores Ajustados.\n\n# Crear un dataframe con los datos para ggplot2\nlibrary(ggplot2)\nlibrary(broom)\n\n# Extraer residuos y valores ajustados\ndatos_diagnostico &lt;- data.frame(\n  residuos = residuals(modelo_estudio),\n  valores_ajustados = fitted(modelo_estudio)\n)\n\n# Gráfico de Residuos vs. Valores Ajustados con ggplot2\nggplot(datos_diagnostico, aes(x = valores_ajustados, y = residuos)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linewidth = 0.8, formula = y ~ x) +\n  labs(\n    title = \"Residuos vs. Valores Ajustados\",\n    x = \"Valores Ajustados\",\n    y = \"Residuos\"\n  ) +\n  theme_classic(base_size = 12)\n\n\n\n\n\n\n\nFigura 2.3: Gráfico de Residuos vs. Valores Ajustados para el modelo de estudio. No se observan patrones.\n\n\n\n\n\nComo se puede observar, los puntos se distribuyen de forma aleatoria alrededor de la línea horizontal en cero. La línea roja, que suaviza la tendencia de los residuos, es prácticamente plana. Esto es un claro indicativo de que el supuesto de linealidad se cumple en nuestro modelo.\n\n\n\nPor el contrario, la aparición de un patrón sistemático en los residuos es la señal de alarma de que algo anda mal. En lo que respecta al supuesto de linealidad, la evidencia más clara de una violación es una tendencia curvilínea (como una “U” o una parábola). Este patrón nos dice que el modelo es estructuralmente incapaz de capturar la forma de los datos y, por lo tanto, comete errores predecibles. Por ejemplo, puede subestimar la respuesta en los extremos (generando residuos positivos) y sobreestimarla en el centro (residuos negativos), lo que invalida el modelo lineal.\n\n\n\n\n\n\nContraejemplo: Violación del supuesto de linealidad\n\n\n\n\n\nAhora, vamos a simular a propósito unos datos que siguen una relación cuadrática (curva) y ajustaremos incorrectamente un modelo lineal para ver cómo se manifiesta el problema en el gráfico de diagnóstico.\n\n# 1. Simulación de datos no lineales\nset.seed(42) # Nueva semilla para este ejemplo\nx_no_lineal &lt;- runif(100, 0, 10)\n# La relación verdadera es cuadrática (y = 10 - (x-5)^2) más un error\ny_no_lineal &lt;- 10 - (x_no_lineal - 5)^2 + rnorm(100, 0, 4)\ndatos_no_lineal &lt;- data.frame(x = x_no_lineal, y = y_no_lineal)\n\n# 2. Ajuste de un modelo lineal (incorrecto)\nmodelo_no_lineal &lt;- lm(y ~ x, data = datos_no_lineal)\n\n# 3. Gráfico de Residuos vs. Valores Ajustados con ggplot2\ndatos_diag_no_lineal &lt;- data.frame(\n  residuos = residuals(modelo_no_lineal),\n  valores_ajustados = fitted(modelo_no_lineal)\n)\n\nggplot(datos_diag_no_lineal, aes(x = valores_ajustados, y = residuos)) +\n  geom_point(color = \"#D55E00\", alpha = 0.7) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linewidth = 0.8, formula = y ~ x) +\n  labs(\n    title = \"Residuos vs. Valores Ajustados (Violación de Linealidad)\",\n    x = \"Valores Ajustados\",\n    y = \"Residuos\"\n  ) +\n  theme_classic(base_size = 12)\n\n\n\n\n\n\n\nFigura 2.4: Patrón curvo evidente en los residuos, violando el supuesto de linealidad.\n\n\n\n\n\nEl gráfico de diagnóstico es inequívoco. A diferencia del ejemplo anterior, donde los puntos formaban una nube aleatoria, aquí los residuos dibujan un patrón parabólico perfecto (una “U” invertida). La línea roja de tendencia, en lugar de ser plana, sigue fielmente esta curva.\nTest de Ramsey RESET para Linealidad\nAdemás del diagnóstico visual, podemos usar el test de Ramsey RESET (Regression Equation Specification Error Test) para detectar violaciones de linealidad:\n\nsuppressPackageStartupMessages(library(lmtest))\n# Test RESET para el modelo correcto (horas de estudio)\nreset_resultado &lt;- resettest(modelo_estudio, power = 2:3, type = \"fitted\")\nprint(reset_resultado)\n\n\n    RESET test\n\ndata:  modelo_estudio\nRESET = 1.0513, df1 = 2, df2 = 96, p-value = 0.3535\n\n# Test RESET para el modelo incorrecto (no lineal)\nreset_no_lineal &lt;- resettest(modelo_no_lineal, power = 2:3, type = \"fitted\")\nprint(reset_no_lineal)\n\n\n    RESET test\n\ndata:  modelo_no_lineal\nRESET = 231.9, df1 = 2, df2 = 96, p-value &lt; 2.2e-16\n\n\nEl test de Ramsey RESET confirma nuestro diagnóstico visual:\n\nPara el modelo correcto: p-valor = 0.3535 → No rechazamos H₀ (forma funcional correcta)\nPara el modelo no lineal: p-valor = 0 → Rechazamos H₀ (forma funcional incorrecta)\n\n\n\n\n\n\n2.6.2 Homocedasticidad\nEl supuesto de homocedasticidad establece que la varianza de los errores del modelo debe ser constante para todos los niveles de la variable predictora. Es decir, la dispersión de los datos alrededor de la línea de regresión es la misma en todo su recorrido (\\(Var(\\varepsilon_i | X_i) = \\sigma^2\\)). La violación de este supuesto se conoce como heterocedasticidad, y es un problema común en el modelado.\nMétodos de diagnóstico:\n\nDiagnóstico visual: El gráfico Scale-Location muestra la raíz cuadrada de los residuos estandarizados absolutos frente a los valores ajustados. Una línea horizontal indica homocedasticidad.\nTest de Breusch-Pagan: Test clásico para detectar heterocedasticidad.\n\nH₀: Homocedasticidad (varianza constante)\nH₁: Heterocedasticidad\nEstadístico: LM ~ χ² bajo H₀\n\nTest de White: Versión más robusta que no asume una forma específica de heterocedasticidad.\n\nH₀: Homocedasticidad\nH₁: Heterocedasticidad (forma general)\nVentaja: No requiere especificar la forma funcional de la heterocedasticidad\n\n\n¿Por qué es tan importante? Si un modelo es heteroscedástico, los errores estándar de los coeficientes (\\(\\beta_0, \\beta_1\\)) estarán calculados de forma incorrecta. Como consecuencia, los intervalos de confianza y los contrastes de hipótesis (p-valores) no serán fiables, pudiendo llevarnos a conclusiones erróneas sobre la significancia de nuestras variables.\n\n\n\n\n\n\nSobre los residuos estandarizados\n\n\n\nLos residuos simples (\\(e_i = y_i - \\hat{y}_i\\)) no son directamente comparables entre sí porque tienen diferentes varianzas dependiendo de su apalancamiento (leverage). Por eso, en los gráficos de diagnóstico se utilizan residuos estandarizados o, mejor aún, residuos estudentizados, que ponen todos los residuos en una escala común. Esto facilita la identificación de patrones y valores atípicos. La explicación detallada de estos conceptos se encuentra en la sección de identificación de observaciones influyentes.\n\n\nLa heteroscedasticidad se detecta principalmente buscando patrones en la dispersión de los residuos.\n\nGráfico de Residuos vs. Valores Ajustados: Como en la prueba de linealidad, este gráfico es nuestra primera herramienta. Aquí no buscamos patrones en la media de los residuos (que debe ser cero), sino en su dispersión. La señal de alarma inequívoca de heteroscedasticidad es una forma de embudo o megáfono, donde la dispersión de los residuos aumenta o disminuye a medida que cambian los valores ajustados.\nGráfico Scale-Location: Este gráfico está diseñado específicamente para detectar heteroscedasticidad. Muestra la raíz cuadrada de los residuos estandarizados en el eje Y (sqrt(|Standardized residuals|)) frente a los valores ajustados en el eje X. Al usar la raíz cuadrada, se suaviza la distribución de los residuos, haciendo los patrones de varianza más fáciles de ver. Si la varianza es constante (homocedasticidad), deberíamos ver una nube de puntos aleatoria con una línea de tendencia roja aproximadamente plana. Una pendiente en esta línea roja indica que la varianza cambia con el nivel de la respuesta.\nPrueba de Breusch-Pagan: Es el contraste de hipótesis formal. Su lógica es ingeniosa: realiza una regresión auxiliar donde intenta predecir los residuos al cuadrado a partir de las variables predictoras originales. Si las variables predictoras ayudan a explicar la magnitud de los residuos al cuadrado, significa que la varianza del error depende de los predictores, y por tanto, hay heteroscedasticidad.\n\nHipótesis Nula (\\(H_0\\)): El modelo es homocedástico.\nDecisión: Un p-valor pequeño (p. ej., &lt; 0.05) es evidencia en contra de la homocedasticidad.\n\n\n\n\n\n\n\n\nEjemplo de un modelo válido\n\n\n\n\n\nAnalicemos nuestro modelo_estudio. Nos centraremos en el gráfico Scale-Location (which = 3) y en la prueba de Breusch-Pagan.\n\n# Crear datos para el gráfico Scale-Location\ndatos_scale_loc &lt;- data.frame(\n  valores_ajustados = fitted(modelo_estudio),\n  residuos_std_sqrt = sqrt(abs(rstandard(modelo_estudio)))\n)\n\n# Gráfico Scale-Location con ggplot2\nggplot(datos_scale_loc, aes(x = valores_ajustados, y = residuos_std_sqrt)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linewidth = 0.8, formula = y ~ x) +\n  labs(\n    title = \"Scale-Location\",\n    x = \"Valores Ajustados\",\n    y = expression(sqrt(\"|Residuos Estandarizados|\"))\n  ) +\n  theme_classic(base_size = 12)\n\n# Prueba de Breusch-Pagan\nsuppressPackageStartupMessages(library(lmtest))\nbp_resultado &lt;- bptest(modelo_estudio)\nprint(bp_resultado)\n\n\n    studentized Breusch-Pagan test\n\ndata:  modelo_estudio\nBP = 0.019638, df = 1, p-value = 0.8886\n\n# Prueba de White (versión robusta)\nwhite_resultado &lt;- bptest(modelo_estudio, ~ fitted(modelo_estudio) + I(fitted(modelo_estudio)^2))\nprint(white_resultado)\n\n\n    studentized Breusch-Pagan test\n\ndata:  modelo_estudio\nBP = 0.12238, df = 2, p-value = 0.9406\n\n\n\n\n\n\n\n\nFigura 2.5: Gráfico Scale-Location para el modelo de estudio. La línea de tendencia es casi plana.\n\n\n\n\n\nEl diagnóstico es positivo. En el gráfico Scale-Location, la línea roja es casi horizontal, lo que indica que la varianza de los residuos es estable a lo largo de los valores ajustados. Esto se confirma con ambas pruebas:\n\nBreusch-Pagan: p-valor = 0.8886 → No rechazamos H₀ (homocedasticidad)\nWhite: p-valor = 0.9406 → No rechazamos H₀ (varianza constante) Nuestro modelo cumple el supuesto.\n\n\n\n\n\n\n\n\n\n\nContraejemplo: Violación del supuesto de homocedasticidad\n\n\n\n\n\nAhora, simularemos datos donde el error aumenta a medida que x crece, un caso clásico de heteroscedasticidad.\n\n# 1. Simulación de datos heteroscedásticos\nset.seed(101)\nx_hetero &lt;- 1:100\ny_hetero &lt;- 10 + 2 * x_hetero + rnorm(100, mean = 0, sd = 0.4 * x_hetero)\ndatos_hetero &lt;- data.frame(x = x_hetero, y = y_hetero)\nmodelo_hetero &lt;- lm(y ~ x, data = datos_hetero)\n\n# 2. Preparar datos para los gráficos\ndatos_diag_hetero &lt;- data.frame(\n  residuos = residuals(modelo_hetero),\n  valores_ajustados = fitted(modelo_hetero),\n  residuos_std_sqrt = sqrt(abs(rstandard(modelo_hetero)))\n)\n\n# 3. Gráfico de Residuos vs. Valores Ajustados\np1 &lt;- ggplot(datos_diag_hetero, aes(x = valores_ajustados, y = residuos)) +\n  geom_point(color = \"#D55E00\", alpha = 0.7) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linewidth = 0.8, formula = y ~ x) +\n  labs(\n    title = \"Residuos vs. Valores Ajustados\",\n    x = \"Valores Ajustados\",\n    y = \"Residuos\"\n  ) +\n  theme_classic(base_size = 10)\n\n# 4. Gráfico Scale-Location\np2 &lt;- ggplot(datos_diag_hetero, aes(x = valores_ajustados, y = residuos_std_sqrt)) +\n  geom_point(color = \"#D55E00\", alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linewidth = 0.8, formula = y ~ x) +\n  labs(\n    title = \"Scale-Location\",\n    x = \"Valores Ajustados\",\n    y = expression(sqrt(\"|Residuos Estandarizados|\"))\n  ) +\n  theme_classic(base_size = 10)\n\n# 5. Mostrar ambos gráficos lado a lado\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n# 6. Prueba de Breusch-Pagan\nsuppressPackageStartupMessages(library(lmtest))\ntest_values &lt;- bptest(modelo_hetero)\n\n\n\n\n\n\n\nFigura 2.6: Diagnóstico de heteroscedasticidad. Izquierda: Gráfico de Residuos vs. Ajustados (patrón de embudo). Derecha: Gráfico Scale-Location (tendencia ascendente).\n\n\n\n\n\nLos resultados son un libro de texto sobre la heteroscedasticidad.\n\nEl gráfico de Residuos vs. Valores Ajustados (izquierda) tiene una forma de embudo inconfundible: la dispersión de los puntos aumenta drásticamente de izquierda a derecha.\nEl gráfico Scale-Location (derecha) confirma el problema, mostrando una línea roja con una clara pendiente ascendente.\nLa prueba de Breusch-Pagan arroja un p-valor 7.43e-07, dándonos una fuerte evidencia estadística para rechazar la hipótesis nula de homocedasticidad.\n\nEste modelo viola claramente el supuesto, y las inferencias basadas en él (como el p-valor del coeficiente de x) no serían fiables.\n\n\n\n\n\n2.6.3 Normalidad de los residuos\nEste supuesto postula que los residuos del modelo (\\(\\varepsilon_i\\)) siguen una distribución normal: \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). Es especialmente importante para la validez de los intervalos de confianza y los contrastes de hipótesis cuando el tamaño de la muestra es pequeño.\nMétodos de diagnóstico:\n\nDiagnóstico visual:\n\nGráfico Q-Q: Los puntos deben seguir la línea diagonal si hay normalidad\nHistograma: Debe mostrar forma campaniforme y simétrica\n\nTest de Shapiro-Wilk: Test más potente para muestras pequeñas y medianas (n &lt; 50).\n\nH₀: Los residuos siguen distribución normal\nH₁: Los residuos no siguen distribución normal\nLimitación: Muy sensible en muestras grandes\n\nTest de Jarque-Bera: Basado en medidas de asimetría y curtosis.\n\nH₀: Los residuos son normales (asimetría = 0, curtosis = 3)\nH₁: Los residuos no son normales\nVentaja: Menos sensible al tamaño muestral que Shapiro-Wilk\n\n\nPara evaluar la normalidad disponemos de estas herramientas visuales y analíticas:\n\nGráfico Normal Q-Q (Normal Q-Q Plot): Compara los cuantiles de los residuos estandarizados con los cuantiles de una distribución normal teórica. Los puntos deben caer muy cerca de la línea diagonal de 45 grados.\nHistograma de los Residuos: Un simple histograma de los residuos debe mostrar una forma aproximada de campana de Gauss.\nPrueba de Shapiro-Wilk: Es uno de los contrastes más potentes para la normalidad.\n\nHipótesis Nula (\\(H_0\\)): Los residuos provienen de una distribución normal.\nDecisión: Un p-valor pequeño (&lt; 0.05) sugiere rechazar \\(H_0\\).\n\n\n\n\n\n\n\n\nEjemplo de normalidad válida\n\n\n\n\n\nPara nuestro modelo_estudio, examinamos la normalidad mediante el gráfico Q-Q y la prueba de Shapiro-Wilk.\n\n# Crear datos para los gráficos\nresiduos &lt;- residuals(modelo_estudio)\n\n# 1. Gráfico Q-Q con ggplot2\ndatos_qq &lt;- data.frame(residuos = residuos)\n\np1 &lt;- ggplot(datos_qq, aes(sample = residuos)) +\n  geom_qq(color = \"#0072B2\", alpha = 0.7) +\n  geom_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Normal Q-Q Plot\",\n    x = \"Cuantiles Teóricos\",\n    y = \"Cuantiles de la Muestra\"\n  ) +\n  theme_classic(base_size = 10)\n\n# 2. Histograma con ggplot2\ndatos_hist &lt;- data.frame(residuos = residuos)\n\np2 &lt;- ggplot(datos_hist, aes(x = residuos)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 15, fill = \"lightblue\", \n                 color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(residuos), sd = sd(residuos)),\n                color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Histograma de Residuos\",\n    x = \"Residuos\",\n    y = \"Densidad\"\n  ) +\n  theme_classic(base_size = 10)\n\n# 3. Mostrar ambos gráficos lado a lado\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n# Prueba de Shapiro-Wilk\nsuppressPackageStartupMessages(library(lmtest))\nshapiro_resultado &lt;- shapiro.test(residuals(modelo_estudio))\nprint(shapiro_resultado)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(modelo_estudio)\nW = 0.99008, p-value = 0.671\n\n# Prueba de Jarque-Bera\nsuppressPackageStartupMessages(library(tseries))\njb_resultado &lt;- jarque.bera.test(residuals(modelo_estudio))\nprint(jb_resultado)\n\n\n    Jarque Bera Test\n\ndata:  residuals(modelo_estudio)\nX-squared = 0.68515, df = 2, p-value = 0.7099\n\n\n\n\n\n\n\n\nFigura 2.7: Diagnóstico de normalidad del modelo de estudio. Izquierda: Q-Q Plot. Derecha: Histograma de residuos.\n\n\n\n\n\nEl diagnóstico es excelente. En el gráfico Q-Q, los puntos se alinean muy bien con la línea diagonal, indicando normalidad. El histograma muestra una distribución aproximadamente simétrica que se ajusta bien a la curva normal teórica (línea roja). Ambas pruebas estadísticas confirman la normalidad:\n\nShapiro-Wilk: p-valor = 0.671 → No rechazamos H₀ (normalidad)\nJarque-Bera: p-valor = 0.7099 → No rechazamos H₀ (normalidad)\n\n\n\n\n\n\n\n\n\n\nContraejemplo: Violación del supuesto de normalidad\n\n\n\n\n\nAhora simularemos datos donde los residuos siguen una distribución asimétrica (distribución exponencial) para mostrar una violación clara del supuesto de normalidad.\n\n# 1. Simulación de datos con errores no normales (exponenciales)\nset.seed(456)\nx_no_normal &lt;- 1:100\n# Errores exponenciales (muy asimétricos) centrados en 0\nerrores_exp &lt;- rexp(100, rate = 1) - 1  # Restamos 1 para centrar en 0\ny_no_normal &lt;- 5 + 2 * x_no_normal + errores_exp * 10\ndatos_no_normal &lt;- data.frame(x = x_no_normal, y = y_no_normal)\nmodelo_no_normal &lt;- lm(y ~ x, data = datos_no_normal)\n\n# 2. Crear datos para los gráficos\nresiduos_no_normal &lt;- residuals(modelo_no_normal)\n\n# 3. Gráfico Q-Q con ggplot2\ndatos_qq_mal &lt;- data.frame(residuos = residuos_no_normal)\n\np1_mal &lt;- ggplot(datos_qq_mal, aes(sample = residuos)) +\n  geom_qq(color = \"#D55E00\", alpha = 0.7) +\n  geom_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Normal Q-Q Plot (Violación)\",\n    x = \"Cuantiles Teóricos\",\n    y = \"Cuantiles de la Muestra\"\n  ) +\n  theme_classic(base_size = 10)\n\n# 4. Histograma con ggplot2\ndatos_hist_mal &lt;- data.frame(residuos = residuos_no_normal)\n\np2_mal &lt;- ggplot(datos_hist_mal, aes(x = residuos)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 15, fill = \"lightcoral\", \n                 color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(residuos_no_normal), sd = sd(residuos_no_normal)),\n                color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Histograma de Residuos (Violación)\",\n    x = \"Residuos\",\n    y = \"Densidad\"\n  ) +\n  theme_classic(base_size = 10)\n\n# 5. Mostrar ambos gráficos lado a lado\ngrid.arrange(p1_mal, p2_mal, ncol = 2)\n\n# Prueba de Shapiro-Wilk\nshapiro_resultado &lt;- shapiro.test(residuals(modelo_no_normal))\n\n\n\n\n\n\n\nFigura 2.8: Violación del supuesto de normalidad. Izquierda: Q-Q Plot con clara desviación. Derecha: Histograma asimétricamente distribuido.\n\n\n\n\n\nLa violación es evidente. En el gráfico Q-Q, los puntos se desvían sistemáticamente de la línea diagonal, especialmente en los extremos, formando una curva característica de distribuciones asimétricas. El histograma muestra una clara asimetría hacia la derecha que no se ajusta a la curva normal teórica (línea azul). La prueba de Shapiro-Wilk arroja un p-valor muy pequeño (1.78e-10), rechazando fuertemente la hipótesis nula de normalidad.\n\n\n\n\n\n2.6.4 Independencia de los residuos\nEste supuesto afirma que el error de una observación no está correlacionado con el de ninguna otra: \\(Cov(\\varepsilon_i, \\varepsilon_j) = 0\\) para \\(i \\neq j\\). La violación, conocida como autocorrelación, es común en datos de series temporales.\nMétodos de diagnóstico:\n\nDiagnóstico visual: Gráfico de residuos vs orden de observación. No debe mostrar patrones temporales o secuenciales.\nTest de Durbin-Watson: Test clásico para autocorrelación de primer orden.\n\nH₀: No hay autocorrelación (\\(\\rho = 0\\))\nH₁: Hay autocorrelación de orden 1\nEstadístico: \\(DW = \\frac{\\sum_{i=2}^{n}(e_i - e_{i-1})^2}{\\sum_{i=1}^{n}e_i^2}\\)\nInterpretación: Valores cerca de 2 → no autocorrelación; cerca de 0 → autocorrelación positiva; cerca de 4 → autocorrelación negativa\n\nTest de Breusch-Godfrey (LM): Generalización del Durbin-Watson para órdenes superiores y regresores retardados.\n\nH₀: No hay autocorrelación serial hasta el orden especificado\nH₁: Hay autocorrelación serial\nVentaja: Más general y potente que Durbin-Watson\n\n\nEl estadístico de Durbin-Watson varía entre 0 y 4. Un valor cercano a 2 sugiere no autocorrelación. Valores cercanos a 0 indican autocorrelación positiva, y cercanos a 4, autocorrelación negativa.\n\n\n\n\n\n\nEjemplo de independencia válida\n\n\n\n\n\nPara nuestro modelo_estudio, evaluamos la independencia mediante el gráfico de residuos vs orden y la prueba de Durbin-Watson.\n\n# Gráfico de residuos vs orden de observación con ggplot2\ndatos_orden &lt;- data.frame(\n  orden = 1:length(residuals(modelo_estudio)),\n  residuos = residuals(modelo_estudio)\n)\n\nggplot(datos_orden, aes(x = orden, y = residuos)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  geom_line(color = \"#0072B2\", alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Residuos vs Orden de Observación\",\n    x = \"Orden de observación\",\n    y = \"Residuos\"\n  ) +\n  theme_classic(base_size = 12)\n\n# Prueba de Durbin-Watson\nsuppressPackageStartupMessages(library(lmtest))\ndw_resultado_valido &lt;- dwtest(modelo_estudio)\nprint(dw_resultado_valido)\n\n\n    Durbin-Watson test\n\ndata:  modelo_estudio\nDW = 2.0565, p-value = 0.6104\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Prueba de Breusch-Godfrey (más general)\nbg_resultado &lt;- bgtest(modelo_estudio, order = 2)\nprint(bg_resultado)\n\n\n    Breusch-Godfrey test for serial correlation of order up to 2\n\ndata:  modelo_estudio\nLM test = 0.14002, df = 2, p-value = 0.9324\n\n\n\n\n\n\n\n\nFigura 2.9: Diagnóstico de independencia del modelo de estudio. Los residuos no muestran patrones temporales.\n\n\n\n\n\nEl diagnóstico es satisfactorio. El gráfico de residuos vs orden no muestra ningún patrón sistemático o tendencia temporal. Ambas pruebas estadísticas confirman la independencia:\n\nDurbin-Watson: DW =2.056, p-valor = 0.6104 → No hay autocorrelación de orden 1\nBreusch-Godfrey: LM =0.14, p-valor = 0.9324 → No hay autocorrelación de orden 2 fluctúan aleatoriamente alrededor del cero.\n\nLa prueba de Durbin-Watson arroja un estadístico de 2.056 (cercano a 2) y un p-valor de 0.61, confirmando que no hay evidencia de autocorrelación. El supuesto de independencia se cumple.\n\n\n\n\n\n\n\n\n\nContraejemplo: Violación del supuesto de independencia\n\n\n\n\n\nSimularemos datos con autocorrelación positiva, donde cada residuo está correlacionado con el anterior, violando el supuesto de independencia.\n\n# 1. Simulación de datos con autocorrelación\nset.seed(789)\nn &lt;- 100\nx_autocorr &lt;- 1:n\n# Generamos errores autocorrelacionados (AR1 con phi = 0.7)\nerrores_autocorr &lt;- numeric(n)\nerrores_autocorr[1] &lt;- rnorm(1)\nfor(i in 2:n) {\n  errores_autocorr[i] &lt;- 0.7 * errores_autocorr[i-1] + rnorm(1, sd = 0.5)\n}\n\ny_autocorr &lt;- 10 + 1.5 * x_autocorr + errores_autocorr * 3\ndatos_autocorr &lt;- data.frame(x = x_autocorr, y = y_autocorr)\nmodelo_autocorr &lt;- lm(y ~ x, data = datos_autocorr)\n\n# 2. Gráfico de residuos vs orden con ggplot2\ndatos_orden_autocorr &lt;- data.frame(\n  orden = 1:length(residuals(modelo_autocorr)),\n  residuos = residuals(modelo_autocorr)\n)\n\nggplot(datos_orden_autocorr, aes(x = orden, y = residuos)) +\n  geom_point(color = \"#D55E00\", alpha = 0.7) +\n  geom_line(color = \"#D55E00\", alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = \"Residuos vs Orden de Observación (Violación)\",\n    x = \"Orden de observación\",\n    y = \"Residuos\"\n  ) +\n  theme_classic(base_size = 12)\n\n# 3. Prueba de Durbin-Watson\ndw_resultado &lt;- dwtest(modelo_autocorr)\n\n\n\n\n\n\n\nFigura 2.10: Violación del supuesto de independencia. Los residuos muestran un patrón de autocorrelación positiva.\n\n\n\n\n\nLa violación es clara. El gráfico de residuos vs orden muestra un patrón ondulante característico: los residuos tienden a mantenerse del mismo signo durante varias observaciones consecutivas (rachas de valores positivos seguidas de rachas de valores negativos). Esto indica autocorrelación positiva. La prueba de Durbin-Watson confirma esto con un estadístico muy por debajo de 2 (DW = 0.74) y un p-valor muy pequeño (5.01e-11), rechazando fuertemente la hipótesis nula de independencia.\n\n\n\n\n\n2.6.5 Media nula de los residuos\nUn requisito fundamental del modelo es que la media de los residuos debe ser exactamente cero: \\(E[e_i] = 0\\). Esta propiedad se deriva matemáticamente del método de mínimos cuadrados y su verificación sirve como una comprobación de que nuestros cálculos son correctos.\n\n\n2.6.6 Identificación de observaciones influyentes y atípicas\nAlgunos puntos pueden tener una influencia desproporcionada en el modelo. Es crucial identificarlos usando diferentes métricas que evalúan aspectos complementarios de la influencia (Kutner et al. 2005; Fox y Weisberg 2018). Las métricas desarrolladas por Cook, Belsley, Kuh y Welsch proporcionan herramientas robustas para este diagnóstico.\n\n\n\n\n\n\nFundamento teórico: de los residuos simples a los estudentizados\n\n\n\nAntes de analizar las métricas de influencia, debemos entender por qué no todos los residuos simples (\\(e_i = y_i - \\hat{y}_i\\)) son comparables entre sí. El problema fundamental es que no tienen la misma varianza, incluso bajo homocedasticidad.\nLa varianza teórica del residuo \\(e_i\\) depende del apalancamiento (leverage) de la observación: \\[\n\\text{Var}(e_i) = \\sigma^2(1 - h_{ii})\n\\]\ndonde el apalancamiento \\(h_{ii}\\) se define como: \\[\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j - \\bar{x})^2}\n\\]\nLas observaciones con valores de \\(X\\) más alejados de la media tendrán mayor apalancamiento y, paradójicamente, residuos con menor varianza. Por esto, un residuo pequeño en una observación de alto leverage puede ser más preocupante que un residuo grande en el centro de los datos.\nLos residuos estandarizados solucionan parcialmente este problema: \\[\nr_i^* = \\frac{e_i}{\\sqrt{\\text{MSE}(1 - h_{ii})}}\n\\]\nPero los residuos estudentizados van un paso más allá, eliminando el sesgo de autoinfluencia: \\[\nr_i = \\frac{e_i}{\\sqrt{\\text{MSE}_{(-i)}(1 - h_{ii})}}\n\\]\ndonde \\(\\text{MSE}_{(-i)}\\) excluye la observación \\(i\\) del ajuste. Esto evita que un outlier “contamine” su propia evaluación y proporciona una distribución teórica exacta (t de Student con \\(n-k-2\\) grados de libertad).\n¿Por qué son superiores los residuos estudentizados? Por tres razones clave: (1) eliminan el sesgo de autoinfluencia al excluir cada observación de su propia evaluación, (2) evitan la contaminación que un outlier produce en la MSE global, y (3) siguen una distribución conocida (t de Student) que permite umbrales estadísticamente precisos. En la práctica: \\(|r_i| &gt; 2\\) indica posibles outliers (≈5% en normalidad) y \\(|r_i| &gt; 3\\) outliers muy probables (&lt;1%).\n\n\nLas métricas fundamentales de influencia para identificar observaciones problemáticas son:\n\nApalancamiento (Leverage, \\(h_{ii}\\)): Mide cuán atípico es el valor de la variable predictora \\(X_i\\) de una observación. Un apalancamiento alto significa que el punto tiene el potencial de ser muy influyente. En regresión simple, se calcula como: \\[ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j - \\bar{x})^2} \\] Una regla común es considerar un apalancamiento alto si \\(h_{ii} &gt; \\frac{2(k+1)}{n}\\), donde \\(k\\) es el número de predictores (1 en regresión simple).\nDistancia de Cook (\\(D_i\\)): Mide la influencia global de una observación, combinando su apalancamiento y su residuo. Representa cuánto cambian los coeficientes del modelo si la i-ésima observación es eliminada. \\[ D_i = \\frac{r_i^2}{k+1} \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Se considera que un punto es influyente si su distancia de Cook es grande, por ejemplo, si \\(D_i &gt; 1\\) o \\(D_i &gt; 4/(n-k-1)\\).\nDFFITS: Mide cuánto cambia la predicción \\(\\hat{y}_i\\) cuando se elimina la i-ésima observación. Es una medida estandarizada que combina el residuo estudentizado y el apalancamiento. \\[ \\text{DFFITS}_i = r_i \\sqrt{\\frac{h_{ii}}{1-h_{ii}}} \\] Un punto se considera influyente si \\(|\\text{DFFITS}_i| &gt; 2\\sqrt{(k+1)/n}\\), donde \\(k\\) es el número de predictores.\n\n\n\n\n\n\n\nEjemplo: Cálculo y análisis de DFFITS\n\n\n\n\n\nDFFITS es especialmente útil para evaluar cómo cada observación afecta a su propia predicción. Analicemos esta medida con nuestro modelo_estudio.\n\n# Calcular DFFITS y sus componentes\ndffits_vals &lt;- dffits(modelo_estudio)\nresiduos_stud &lt;- rstudent(modelo_estudio)  # Residuos estudentizados\nleverage_vals &lt;- hatvalues(modelo_estudio)\n\n# Crear dataframe para análisis\ndatos_dffits &lt;- data.frame(\n  observacion = 1:length(dffits_vals),\n  dffits = dffits_vals,\n  residuo_stud = residuos_stud,\n  leverage = leverage_vals\n)\n\n# Umbral de DFFITS\nn &lt;- nrow(datos)\nk &lt;- 1  # número de predictores\ndffits_threshold &lt;- 2 * sqrt((k + 1) / n)\n\n# Gráfico de DFFITS\nggplot(datos_dffits, aes(x = observacion, y = dffits)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  geom_hline(yintercept = 0, color = \"gray\", linetype = \"dashed\") +\n  geom_hline(yintercept = c(-dffits_threshold, dffits_threshold), \n             color = \"red\", linetype = \"dashed\", alpha = 0.7) +\n  labs(\n    title = \"DFFITS por Observación\",\n    x = \"Número de Observación\",\n    y = \"DFFITS\",\n    caption = paste(\"Líneas rojas: umbrales ±\", round(dffits_threshold, 3))\n  ) +\n  theme_classic(base_size = 12)\n\n# Análisis cuantitativo\ninfluential_dffits &lt;- which(abs(dffits_vals) &gt; dffits_threshold)\ntop_indices &lt;- order(abs(dffits_vals), decreasing = TRUE)[1:5]\n\n\n\n\n\n\n\nFigura 2.11: Análisis de DFFITS para identificar observaciones que afectan significativamente a sus propias predicciones.\n\n\n\n\n\nAnálisis de resultados:\nEl umbral de influencia para DFFITS es 0.283. En nuestro modelo, 7 observaciones superan este umbral: las observaciones 6, 20, 22, 47, 85, 87, 89, lo que las clasifica como influyentes según este criterio.\nLas cinco observaciones con mayor |DFFITS| son las observaciones 20, 85, 89, 47, 6, con valores de 0.446, -0.38, 0.364, 0.325, 0.312 respectivamente. Lo más notable es que todas estas cinco observaciones (20, 85, 89, 47, 6) superan el umbral de DFFITS, confirmando su carácter influyente.\nInterpretación clave: La observación 20 es el caso más destacado: tiene un DFFITS de 0.446, superando el umbral de 0.283. Esta combinación de residuo y apalancamiento resulta en un DFFITS significativo que indica cambios sustanciales en su predicción.\nConclusión práctica: Tenemos 7 observaciones influyentes según DFFITS (6, 20, 22, 47, 85, 87, 89) que merecen investigación adicional. Estas observaciones cambian significativamente sus propias predicciones cuando son eliminadas del modelo, sugiriendo que podrían representar casos especiales o errores de medición que deberían ser examinados más detalladamente.\n\n\n\nEl gráfico Residuals vs. Leverage es la herramienta visual más importante para el diagnóstico de influencia, ya que combina en un solo gráfico el apalancamiento (eje X) y los residuos estudentizados (eje Y), permitiendo identificar simultáneamente observaciones con alto leverage y outliers. Además, incluye curvas que delimitan regiones de alta Distancia de Cook, facilitando la identificación visual de los puntos más problemáticos.\n\n\n\n\n\n\nEjemplo: Gráfico Residuals vs. Leverage\n\n\n\n\n\nVamos a analizar el gráfico más importante para el diagnóstico de influencia usando nuestro modelo_estudio.\n\n# Crear datos para el gráfico Residuals vs. Leverage\nleverage_vals &lt;- hatvalues(modelo_estudio)\nresiduos_stud &lt;- rstudent(modelo_estudio)  # Residuos estudentizados\ncook_dist &lt;- cooks.distance(modelo_estudio)\n\ndatos_leverage &lt;- data.frame(\n  leverage = leverage_vals,\n  residuos_stud = residuos_stud,\n  cook = cook_dist,\n  observacion = 1:length(leverage_vals)\n)\n\n# Calcular umbrales\nn &lt;- nrow(datos)\nk &lt;- 1\nleverage_threshold &lt;- 2 * (k + 1) / n\ncook_threshold &lt;- 4 / (n - k - 1)\n\n# Función para crear curvas de Cook\ncook_curve &lt;- function(leverage, cook_value, k) {\n  sqrt(cook_value * (k + 1) * (1 - leverage) / leverage)\n}\n\n# Crear curvas de Cook para diferentes valores\nlev_seq &lt;- seq(0.001, max(leverage_vals) * 1.1, length.out = 100)\ncook_05 &lt;- data.frame(\n  leverage = lev_seq,\n  pos = cook_curve(lev_seq, 0.5, k),\n  neg = -cook_curve(lev_seq, 0.5, k)\n)\ncook_1 &lt;- data.frame(\n  leverage = lev_seq,\n  pos = cook_curve(lev_seq, 1, k),\n  neg = -cook_curve(lev_seq, 1, k)\n)\n\n# Gráfico Residuals vs. Leverage con ggplot2\nggplot(datos_leverage, aes(x = leverage, y = residuos_stud)) +\n  # Curvas de Cook\n  geom_line(data = cook_05, aes(x = leverage, y = pos), \n            color = \"red\", linetype = \"dashed\", alpha = 0.6, inherit.aes = FALSE) +\n  geom_line(data = cook_05, aes(x = leverage, y = neg), \n            color = \"red\", linetype = \"dashed\", alpha = 0.6, inherit.aes = FALSE) +\n  geom_line(data = cook_1, aes(x = leverage, y = pos), \n            color = \"red\", linetype = \"solid\", alpha = 0.8, inherit.aes = FALSE) +\n  geom_line(data = cook_1, aes(x = leverage, y = neg), \n            color = \"red\", linetype = \"solid\", alpha = 0.8, inherit.aes = FALSE) +\n  # Puntos de datos\n  geom_point(color = \"#0072B2\", alpha = 0.7, size = 2) +\n  # Líneas de referencia\n  geom_hline(yintercept = 0, color = \"gray\", linetype = \"dashed\") +\n  geom_hline(yintercept = c(-2, 2), color = \"orange\", linetype = \"dotted\", alpha = 0.7) +\n  geom_vline(xintercept = leverage_threshold, color = \"purple\", linetype = \"dotted\", alpha = 0.7) +\n  # Etiquetas\n  labs(\n    title = \"Residuals vs. Leverage\",\n    x = \"Leverage\",\n    y = \"Residuos Estudentizados\",\n    caption = \"Curvas rojas: Cook 0.5 (discontinua) y 1.0 (continua) | Líneas naranjas: ±2 | Línea morada: umbral leverage\"\n  ) +\n  theme_classic(base_size = 12)\n\n# Identificar observaciones problemáticas\nhigh_leverage &lt;- which(leverage_vals &gt; leverage_threshold)\noutliers_stud &lt;- which(abs(residuos_stud) &gt; 2)\nhigh_cook &lt;- which(cook_dist &gt; cook_threshold)\n\n\n\n\n\n\n\nFigura 2.12: Gráfico Residuals vs. Leverage para identificar observaciones influyentes.\n\n\n\n\n\nAnálisis del gráfico:\nEl gráfico revela varios puntos importantes. Tenemos 6 outliers (residuos estudentizados &gt; 2): las observaciones 20, 22, 47, 85, 89, 99. Además, 2 observaciones superan el umbral de leverage (&gt; 0.04): las observaciones 24, 74.\nInterpretación por regiones:\n\nZona derecha (alto leverage): Las observaciones 24, 74 superan el umbral de leverage, lo que significa que tienen valores de X atípicos y alto potencial influyente\nZona izquierda superior/inferior: Los 6 outliers (20, 22, 47, 85, 89, 99) están distribuidos aquí, con leverage bajo-moderado pero residuos grandes\nEsquinas críticas: Afortunadamente vacías (alto leverage + outlier sería muy problemático)\n\nDistancia de Cook: Las curvas rojas muestran que aunque ningún punto supera Cook = 1.0 (línea continua), varios puntos se acercan a la curva de Cook = 0.5 (línea discontinua), indicando influencia moderada. Las observaciones con alto leverage están en esta zona de influencia moderada.\nConclusión práctica: El modelo presenta una situación favorable: aunque tenemos outliers (observaciones 20, 22, 47, 85, 89, 99) que son atípicos en Y, y observaciones de alto leverage (observaciones 24, 74) que son atípicos en X, crucialmente no hay solapamiento entre ambos grupos. Esto significa que no tenemos la situación más problemática (alto leverage + outlier). Aun así, ambos grupos merecen investigación.\n\n\n\n\n2.6.6.1 Interpretación práctica de las medidas de influencia\nCada medida nos proporciona información complementaria sobre diferentes aspectos de la influencia:\n\nLeverage (Apalancamiento): Identifica observaciones con valores “raros” en las variables predictoras. Alto leverage no es necesariamente problemático, pero indica potencial para ser influyente.\nDistancia de Cook: Es la medida más general de influencia. Valores altos indican que eliminar esa observación cambiaría substancialmente los coeficientes del modelo.\nDFFITS: Se enfoca específicamente en cómo cambia la predicción de cada punto cuando se elimina esa observación. Es especialmente útil para evaluar el impacto en las predicciones.\n\nEn la práctica, una observación es especialmente preocupante si es problemática según múltiples criterios a la vez.\n\n\n\n\n\n\nDiagnóstico completo del modelo de estudio\n\n\n\n\n\nA continuación, realizamos todas las verificaciones de diagnóstico para nuestro modelo_estudio:\n\n# Preparar todos los datos necesarios para los gráficos\nresiduos_completo &lt;- residuals(modelo_estudio)\nvalores_ajustados_completo &lt;- fitted(modelo_estudio)\nresiduos_std &lt;- rstandard(modelo_estudio)\nleverage_vals &lt;- hatvalues(modelo_estudio)\n\n# 1. Gráfico Residuos vs. Valores Ajustados\np1_completo &lt;- ggplot(data.frame(x = valores_ajustados_completo, y = residuos_completo), \n                      aes(x = x, y = y)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linewidth = 0.8, formula = y ~ x) +\n  labs(title = \"Residuos vs. Valores Ajustados\", x = \"Valores Ajustados\", y = \"Residuos\") +\n  theme_classic(base_size = 10)\n\n# 2. Gráfico Q-Q Normal\ndatos_qq_completo &lt;- data.frame(residuos = residuos_std)\n\np2_completo &lt;- ggplot(datos_qq_completo, aes(sample = residuos)) +\n  geom_qq(color = \"#0072B2\", alpha = 0.7) +\n  geom_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Normal Q-Q Plot\", x = \"Cuantiles Teóricos\", y = \"Cuantiles de la Muestra\") +\n  theme_classic(base_size = 10)\n\n# 3. Gráfico Scale-Location\np3_completo &lt;- ggplot(data.frame(x = valores_ajustados_completo, \n                                 y = sqrt(abs(residuos_std))), \n                      aes(x = x, y = y)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linewidth = 0.8, formula = y ~ x) +\n  labs(title = \"Scale-Location\", x = \"Valores Ajustados\", \n       y = expression(sqrt(\"|Residuos Estandarizados|\"))) +\n  theme_classic(base_size = 10)\n\n# 4. Gráfico Residuos vs. Leverage (con curvas de Cook)\np4_completo &lt;- ggplot(datos_leverage, aes(x = leverage, y = residuos_stud)) +\n  # Curvas de Cook\n  geom_line(data = cook_05, aes(x = leverage, y = pos), \n            color = \"red\", linetype = \"dashed\", alpha = 0.6, inherit.aes = FALSE) +\n  geom_line(data = cook_05, aes(x = leverage, y = neg), \n            color = \"red\", linetype = \"dashed\", alpha = 0.6, inherit.aes = FALSE) +\n  geom_line(data = cook_1, aes(x = leverage, y = pos), \n            color = \"red\", linetype = \"solid\", alpha = 0.8, inherit.aes = FALSE) +\n  geom_line(data = cook_1, aes(x = leverage, y = neg), \n            color = \"red\", linetype = \"solid\", alpha = 0.8, inherit.aes = FALSE) +\n  # Puntos de datos\n  geom_point(color = \"#0072B2\", alpha = 0.7, size = 1.5) +\n  # Líneas de referencia\n  geom_hline(yintercept = 0, color = \"gray\", linetype = \"dashed\") +\n  geom_hline(yintercept = c(-2, 2), color = \"orange\", linetype = \"dotted\", alpha = 0.7) +\n  geom_vline(xintercept = leverage_threshold, color = \"purple\", linetype = \"dotted\", alpha = 0.7) +\n  labs(title = \"Residuals vs. Leverage\", x = \"Leverage\", y = \"Residuos Estudentizados\") +\n  theme_classic(base_size = 10)\n\n# 5. Histograma de residuos\np5_completo &lt;- ggplot(data.frame(residuos = residuos_completo), aes(x = residuos)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 15, fill = \"lightblue\", \n                 color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(residuos_completo), sd = sd(residuos_completo)),\n                color = \"red\", linewidth = 1) +\n  labs(title = \"Histograma de Residuos\", x = \"Residuos\", y = \"Densidad\") +\n  theme_classic(base_size = 10)\n\n# 6. Residuos vs. Orden\np6_completo &lt;- ggplot(data.frame(orden = 1:length(residuos_completo), \n                                 residuos = residuos_completo), \n                      aes(x = orden, y = residuos)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  geom_line(color = \"#0072B2\", alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residuos vs. Orden\", x = \"Orden de observación\", y = \"Residuos\") +\n  theme_classic(base_size = 10)\n\n# Mostrar todos los gráficos en una rejilla 2x3\nlibrary(gridExtra)\ngrid.arrange(p1_completo, p2_completo, p3_completo, \n             p4_completo, p5_completo, p6_completo, ncol = 3)\n\n\n\n\n\n\n\nFigura 2.13: Gráficos de diagnóstico completo del modelo de regresión.\n\n\n\n\n\nConclusión del diagnóstico:\nNuestro modelo de estudio pasa exitosamente todas las verificaciones:\n✅ Linealidad: Los gráficos de residuos vs valores ajustados no muestran patrones sistemáticos, confirmando que la relación lineal es apropiada.\n✅ Homocedasticidad: La prueba de Breusch-Pagan arroja un p-valor de 0.8886, que es mayor a 0.05, por lo que no hay evidencia de heterocedasticidad. La varianza de los errores es constante.\n✅ Normalidad: La prueba de Shapiro-Wilk presenta un p-valor de 0.671, superior a 0.05, confirmando que los residuos siguen una distribución normal. Esto se corrobora visualmente en el Q-Q plot.\n✅ Independencia: El estadístico de Durbin-Watson es 2.056 con un p-valor de 0.61, indicando ausencia de autocorrelación en los residuos.\n✅ Media nula: La media de los residuos es 0, prácticamente cero como se esperaría en un modelo bien especificado.\n\n\nSe identificaron las siguientes observaciones que requieren atención:\n• Alto apalancamiento (&gt; 0.04 ): 24, 74\n• Influyentes según Cook (&gt; 0.041 ): 6, 20, 47, 85, 87, 89\n• Influyentes según DFFITS (&gt; 0.283 ): 6, 20, 22, 47, 85, 87, 89\n• Posibles outliers (|residuo std| &gt; 2): 20, 22, 47, 85, 89, 99\n\nEstas observaciones deberían ser investigadas más detalladamente antes de proceder con las inferencias finales.\n\n\nEsto confirma que nuestras inferencias estadísticas (p-valores, intervalos de confianza) son válidas y confiables (James et al. 2021; Harrell 2015).\n\n\n\n\n\n\n\nDraper, NR. 1998. Applied regression analysis. McGraw-Hill. Inc.\n\n\nFox, John, y Sanford Weisberg. 2018. An R companion to applied regression. Sage publications.\n\n\nGalton, Francis. 1886. «Regression towards mediocrity in hereditary stature». The Journal of the Anthropological Institute of Great Britain and Ireland 15: 246-63.\n\n\nHarrell, Frank E., Jr. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. Second. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2021. An Introduction to Statistical Learning with Applications in R. Second. Springer.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.\n\n\nWeisberg, S. 2005. «Applied linear regression». Wiley.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema2.html",
    "href": "tema2.html",
    "title": "3  El modelo de regresión lineal múltiple",
    "section": "",
    "text": "3.1 Formulación teórica del modelo\nEl modelo de regresión lineal múltiple constituye la extensión natural y más potente del modelo simple que estudiamos en el capítulo anterior. Mientras que la regresión simple nos permitía examinar la relación entre una variable respuesta y un único predictor, la regresión múltiple nos capacita para modelar simultáneamente el efecto de múltiples variables predictoras, una situación mucho más realista en la mayoría de aplicaciones prácticas (Kutner et al. 2005; James et al. 2021; Fox y Weisberg 2018).\nEn este capítulo profundizaremos en los aspectos únicos de la regresión múltiple que no están presentes en el caso simple: la interpretación de coeficientes en presencia de otros predictores, el diagnóstico específico del modelo múltiple, y el problema crucial de la multicolinealidad. Estos conceptos son fundamentales para desarrollar modelos predictivos robustos y interpretables (Harrell 2015; Draper 1998).\nEl paso de la regresión simple a la múltiple es más que una simple adición de términos; es un salto conceptual. Nos permite construir modelos que reflejan mejor la complejidad del mundo real, donde los resultados raramente dependen de una única causa. Al controlar simultáneamente por varios factores, podemos aislar con mayor precisión el efecto de una variable de interés, reduciendo el riesgo de llegar a conclusiones sesgadas por variables omitidas.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>El modelo de regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "tema2.html#formulación-teórica-del-modelo",
    "href": "tema2.html#formulación-teórica-del-modelo",
    "title": "3  El modelo de regresión lineal múltiple",
    "section": "",
    "text": "3.1.1 El modelo poblacional\nPara \\(n\\) observaciones y \\(p\\) variables predictoras, el modelo poblacional postula que la relación verdadera entre la variable respuesta \\(Y\\) y los predictores \\(X_1, X_2, \\ldots, X_p\\) sigue una relación lineal:\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i, \\quad i = 1,\\dots,n\\]\nDonde \\(Y_i\\) es la \\(i\\)-ésima variable respuesta aleatoria, \\(X_{ij}\\) es la \\(i\\)-ésima variable predictora aleatoria del \\(j\\)-ésimo predictor, y \\(\\varepsilon_i\\) es el término de error aleatorio. Los parámetros \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) son los coeficientes poblacionales verdaderos pero desconocidos.\n\n\n3.1.2 El modelo muestral\nEn la práctica, trabajamos con datos observados y estimamos el modelo usando la muestra disponible:\n\\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + \\cdots + \\hat{\\beta}_p x_{ip}, \\quad i = 1,\\dots,n\\]\nDonde \\(\\hat{y}_i\\) es la \\(i\\)-ésima predicción, \\(x_{ij}\\) es la \\(i\\)-ésima observación del \\(j\\)-ésimo predictor, y \\(\\hat{\\beta}_j\\) son los coeficientes estimados. El coeficiente \\(\\hat{\\beta}_j\\) representa el cambio estimado en la media de \\(Y\\) ante un cambio de una unidad en el predictor \\(X_j\\), manteniendo constantes todas las demás variables predictoras del modelo. Este principio, conocido como ceteris paribus (del latín, “lo demás constante”), es la piedra angular de la interpretación en regresión múltiple.\n\n\n3.1.3 Notación matricial\nLa notación matricial es fundamental para el desarrollo teórico y computacional. Nos permite expresar el sistema de \\(n\\) ecuaciones de forma compacta y elegante.\nModelo poblacional: \\[\\mathbf{Y} = \\tilde{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\]\ndonde:\n\\[\\mathbf{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}, \\quad\n\\tilde{X} = \\begin{bmatrix}\n1 & X_{11} & X_{12} & \\cdots & X_{1p} \\\\\n1 & X_{21} & X_{22} & \\cdots & X_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & X_{n1} & X_{n2} & \\cdots & X_{np}\n\\end{bmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}, \\quad\n\\boldsymbol{\\varepsilon} = \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix}\\]\nDonde \\(\\tilde{X}\\) contiene variables aleatorias (denotadas con mayúsculas \\(X_{ij}\\)).\nModelo muestral: \\[\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\]\ndonde:\n\\[\\hat{\\mathbf{y}} = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{bmatrix}, \\quad\n\\mathbf{X} = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}, \\quad\n\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\vdots \\\\ \\hat{\\beta}_p \\end{bmatrix}\\]\nDonde \\(\\mathbf{X}\\) contiene datos observados (denotados con minúsculas \\(x_{ij}\\)).\nLa matriz \\(\\mathbf{X}\\) (datos observados) y \\(\\tilde{X}\\) (variables aleatorias), ambas de dimensión \\(n \\times (p+1)\\), se denominan matriz de diseño y contienen toda la información de los predictores. La primera columna de unos corresponde al término del intercepto \\(\\beta_0\\).\n\n\n3.1.4 Supuestos del modelo lineal múltiple\nPara que nuestros estimadores tengan propiedades deseables (como ser insesgados y eficientes), el modelo debe cumplir una serie de supuestos sobre el comportamiento del término de error, conocidos como las condiciones de Gauss-Markov (Kutner et al. 2005; Weisberg 2005).\n\nLinealidad en los parámetros: El valor esperado de la respuesta es una función lineal de los parámetros \\(\\boldsymbol{\\beta}\\). El modelo \\(E[\\mathbf{Y}|\\tilde{X}] = \\tilde{X}\\boldsymbol{\\beta}\\) está bien especificado.\nExogeneidad (media del error nula): Los errores tienen una media de cero para cualquier valor de los predictores, \\(E[\\boldsymbol{\\varepsilon}|\\tilde{X}] = \\mathbf{0}\\). Esto implica que los predictores no contienen información sobre el término de error.\nHomocedasticidad e independencia: Los errores no están correlacionados entre sí y tienen una varianza constante \\(\\sigma^2\\) para cualquier valor de los predictores. En notación matricial: \\(\\text{Var}(\\boldsymbol{\\varepsilon}|\\tilde{X}) = \\sigma^2\\mathbf{I}_n\\).\nAusencia de multicolinealidad perfecta: Ningún predictor es una combinación lineal exacta de los otros. Esto asegura que la matriz \\(\\mathbf{X}\\) tiene rango completo \\((p+1)\\), lo cual es necesario para poder estimar de forma única todos los coeficientes.\nNormalidad de los errores (para inferencia): Para poder realizar contrastes de hipótesis e intervalos de confianza, se añade el supuesto de que los errores siguen una distribución Normal: \\(\\boldsymbol{\\varepsilon} \\sim N(\\mathbf{0}, \\sigma^2\\mathbf{I}_n)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>El modelo de regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "tema2.html#estimación-de-los-parámetros",
    "href": "tema2.html#estimación-de-los-parámetros",
    "title": "3  El modelo de regresión lineal múltiple",
    "section": "3.2 Estimación de los parámetros",
    "text": "3.2 Estimación de los parámetros\nUna vez definido el modelo y sus supuestos, el siguiente paso es estimar los parámetros desconocidos del vector \\(\\boldsymbol{\\beta}\\). El método más extendido es el de Mínimos Cuadrados Ordinarios.\n\n3.2.1 El principio de mínimos cuadrados y la función objetivo\nLa idea de “mejor ajuste” se traduce matemáticamente en minimizar la discrepancia entre los valores observados \\(\\mathbf{y}\\) (datos muestrales) y los valores predichos por el modelo, \\(\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\). Esta discrepancia se captura a través de los residuos, \\(\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}\\).\nMCO no minimiza simplemente los residuos (ya que residuos positivos y negativos se cancelarían), sino la Suma de los Cuadrados de los Residuos (SCR o SSR en inglés). Al elevarlos al cuadrado, nos aseguramos de que todos los errores contribuyan positivamente y, además, penalizamos más fuertemente los errores grandes.\nLa función objetivo a minimizar, \\(S(\\boldsymbol{\\beta})\\), usando datos observados es:\n\\[S(\\boldsymbol{\\beta}) = \\sum_{i=1}^n e_i^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]\n\n\n3.2.2 Derivación de las ecuaciones normales\nPara encontrar el vector \\(\\hat{\\boldsymbol{\\beta}}\\) que minimiza esta función, utilizamos cálculo diferencial. Primero, expandimos la expresión cuadrática de \\(S(\\boldsymbol{\\beta})\\):\n\\[S(\\boldsymbol{\\beta}) = (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\] \\[S(\\boldsymbol{\\beta}) = \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\]\nUn punto clave aquí es notar que \\(\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y}\\) es un escalar (una matriz \\(1 \\times 1\\)), por lo que es igual a su transpuesta: \\(\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} = \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta}\\). \\(1 \\times 1\\)). Por lo tanto, es igual a su propia transpuesta: \\((\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y})^T = \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta}\\). Esto nos permite simplificar la expresión:\n\\[S(\\boldsymbol{\\beta}) = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T(\\mathbf{X}^T\\mathbf{X})\\boldsymbol{\\beta}\\]\nAhora, derivamos esta función con respecto al vector \\(\\boldsymbol{\\beta}\\) e igualamos el resultado a un vector de ceros para encontrar el mínimo. Usando las reglas de la derivación matricial:\n\nLa derivada de \\(\\mathbf{y}^T\\mathbf{y}\\) respecto a \\(\\boldsymbol{\\beta}\\) es \\(\\mathbf{0}\\).\nLa derivada de \\(2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y}\\) respecto a \\(\\boldsymbol{\\beta}\\) es \\(2\\mathbf{X}^T\\mathbf{y}\\).\nLa derivada de la forma cuadrática \\(\\boldsymbol{\\beta}^T(\\mathbf{X}^T\\mathbf{X})\\boldsymbol{\\beta}\\) respecto a \\(\\boldsymbol{\\beta}\\) es \\(2(\\mathbf{X}^T\\mathbf{X})\\boldsymbol{\\beta}\\).\n\nAplicando estas reglas, obtenemos el gradiente de la función de pérdida:\n\\[\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2(\\mathbf{X}^T\\mathbf{X})\\boldsymbol{\\beta}\\]\nIgualando a cero y sustituyendo \\(\\boldsymbol{\\beta}\\) por el estimador \\(\\hat{\\boldsymbol{\\beta}}\\) que cumple esta condición:\n\\[-2\\mathbf{X}^T\\mathbf{y} + 2(\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\\]\nSimplificando, llegamos al célebre sistema de \\(p+1\\) ecuaciones conocido como las Ecuaciones Normales:\n\\[(\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}\\]\n\n\n3.2.3 La solución MCO y la condición de invertibilidad\nPara resolver este sistema y despejar \\(\\hat{\\boldsymbol{\\beta}}\\), necesitamos multiplicar por la inversa de la matriz \\((\\mathbf{X}^T\\mathbf{X})\\). Esta inversa existe si y solo si la matriz es invertible, lo que está directamente garantizado por el supuesto de ausencia de multicolinealidad perfecta.\nSi el rango de la matriz de diseño \\(\\mathbf{X}\\) es \\(p+1\\) (sus columnas son linealmente independientes), entonces la matriz \\(\\mathbf{X}^T\\mathbf{X}\\) (de dimensión \\((p+1) \\times (p+1)\\)) será de rango completo, simétrica y definida positiva, y por tanto, invertible.\nLa solución única para el vector de estimadores MCO es:\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\nEsta compacta y poderosa ecuación es la base de la estimación en regresión lineal y es implementada por todo el software estadístico.\nCon esto, tenemos que \\[\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\underbrace{\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T}_{\\mathbf{H}}\\hat{\\mathbf{y}} = \\mathbf{H} \\mathbf{y}\\] donde \\(\\mathbf{H}\\) es la matriz hat. Esta matriz proyecta el vector \\(\\mathbf{y}\\) sobre el espacio generado por las columnas \\(\\mathbf{X}\\). También se relaciona con los residuos del siguiente modo: \\[\\mathbf{e} = \\mathbf{y}-\\hat{\\mathbf{y}}=\\mathbf{y}-\\mathbf{H}\\mathbf{y} = (\\mathbf{I}-\\mathbf{H})\\mathbf{y}\\]\n\n\n3.2.4 Propiedades de los estimadores de MCO\nUna vez que hemos obtenido la fórmula para calcular nuestros coeficientes, \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\), la pregunta fundamental es: ¿qué tan buenos son estos estimadores? La teoría estadística nos proporciona una respuesta contundente a través de sus propiedades en el muestreo, que son la base para toda la inferencia estadística posterior.\nEl Teorema de Gauss-Markov es el resultado central. Afirma que, si se cumplen los supuestos del modelo lineal clásico (1-4), los estimadores de Mínimos Cuadrados Ordinarios son los Mejores Estimadores Lineales Insesgados (MELI o BLUE). Desglosemos esto:\n\nLineal: \\(\\hat{\\boldsymbol{\\beta}}\\) es una combinación lineal de la variable respuesta \\(\\mathbf{y}\\).\nInsesgado (Unbiased): En promedio, a lo largo de infinitas muestras, nuestro estimador acertará al verdadero valor poblacional \\(\\boldsymbol{\\beta}\\). No tiene un sesgo sistemático. La demostración formal es directa: \\[\\begin{align}\n  E[\\hat{\\boldsymbol{\\beta}}] &= E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}] \\nonumber \\\\\n  &= E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon})] \\nonumber \\\\\n  &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T E[\\boldsymbol{\\varepsilon}] \\nonumber \\\\\n  &= \\boldsymbol{I}\\boldsymbol{\\beta} + \\mathbf{0} = \\boldsymbol{\\beta} \\nonumber\n  \\end{align}\\]\nMejor (Best): “Mejor” significa que tiene la mínima varianza posible dentro de la clase de todos los estimadores lineales e insesgados. No existe otro estimador de este tipo que sea más preciso. La precisión de nuestros estimadores se captura en su matriz de varianzas-covarianzas: \\[\\begin{align}\n  \\text{Var}(\\hat{\\boldsymbol{\\beta}}) &= \\text{Var}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}] \\nonumber \\\\\n  &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\, \\text{Var}(\\mathbf{y}) \\, [(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]^T \\nonumber \\\\\n  &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\, (\\sigma^2 \\mathbf{I}_n) \\, \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\nonumber \\\\\n  &= \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\nonumber \\\\\n  &= \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\nonumber\n  \\end{align}\\] Por tanto, la matriz que define la incertidumbre de nuestro estimador es: \\[\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\] Los elementos de la diagonal de esta matriz nos dan la varianza de cada coeficiente individual, \\(Var(\\hat{\\beta}_j)\\), mientras que los elementos fuera de la diagonal nos dan la covarianza entre pares de coeficientes, \\(Cov(\\hat{\\beta}_j, \\hat{\\beta}_k)\\).\n\nFinalmente, si añadimos el supuesto de normalidad de los errores (\\(\\varepsilon_i \\sim N(0, \\sigma^2)\\)), las propiedades del estimador se completan. Dado que \\(\\hat{\\boldsymbol{\\beta}}\\) es una combinación lineal de \\(\\mathbf{y}\\) (que ahora es normal), el propio estimador seguirá una distribución normal: \\[\\hat{\\boldsymbol{\\beta}} \\sim N\\left(\\boldsymbol{\\beta}, \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\right)\\]Esto implica que cada coeficiente individual también se distribuye normalmente:\\[\\hat{\\beta}_j \\sim N\\left(\\beta_j, \\sigma^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}\\right)\\] donde \\([(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}\\) es el j-ésimo elemento de la diagonal de la matriz inversa. Este resultado es la puerta de entrada a la inferencia, permitiéndonos construir intervalos de confianza y realizar contrastes de hipótesis (como los test-t).\n\n\n3.2.5 Estimación de la varianza del error\nLa matriz de varianzas-covarianzas de \\(\\hat{\\boldsymbol{\\beta}}\\) depende de \\(\\sigma^2\\), la varianza de los errores poblacionales, que es desconocida. Por lo tanto, el siguiente paso lógico es encontrar un buen estimador para ella a partir de nuestros datos.\nEl punto de partida natural son los residuos del modelo, \\(\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}\\), que son la contraparte muestral de los errores teóricos \\(\\boldsymbol{\\varepsilon}\\). La suma de los cuadrados de los residuos (SSE) es la base de nuestro estimador:\n\\[SSE = \\mathbf{e}^T\\mathbf{e} = \\mathbf{y}^T(\\mathbf{I}_n - \\mathbf{H})^T(\\mathbf{I}_n - \\mathbf{H})\\mathbf{y} = \\mathbf{y}^T(\\mathbf{I}_n - \\mathbf{H})\\mathbf{y}\\] (usando las propiedades de simetría e idempotencia de la matriz de proyección \\(\\mathbf{H}\\)).\nPara encontrar un estimador insesgado, calculamos el valor esperado de la SSE. Utilizando el lema \\(E[\\mathbf{z}^T\\mathbf{A}\\mathbf{z}] = \\text{traza}(\\mathbf{A}\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}\\) con \\(\\mathbf{z} = \\mathbf{y}\\), \\(\\mathbf{A} = \\mathbf{I}_n - \\mathbf{H}\\), \\(\\boldsymbol{\\mu} = \\mathbf{X}\\boldsymbol{\\beta}\\) y \\(\\boldsymbol{\\Sigma} = \\sigma^2\\mathbf{I}_n\\): \\[\\begin{align}\nE[SSE] &= \\text{traza}[(\\mathbf{I}_n - \\mathbf{H})\\sigma^2\\mathbf{I}_n] + \\boldsymbol{\\beta}^T\\mathbf{X}^T(\\mathbf{I}_n - \\mathbf{H})\\mathbf{X}\\boldsymbol{\\beta} \\nonumber\n\\end{align}\\] El segundo término se anula porque \\((\\mathbf{I}_n - \\mathbf{H})\\mathbf{X} = \\mathbf{X} - \\mathbf{H}\\mathbf{X} = \\mathbf{X} - \\mathbf{X} = \\mathbf{0}\\). Nos queda: \\[\\begin{align}\nE[SSE] &= \\sigma^2 \\text{traza}(\\mathbf{I}_n - \\mathbf{H}) \\nonumber \\\\\n&= \\sigma^2 (\\text{traza}(\\mathbf{I}_n) - \\text{traza}(\\mathbf{H})) \\nonumber \\\\\n&= \\sigma^2 (n - (p + 1)) \\nonumber\n\\end{align}\\] El valor esperado de la SSE no es \\(\\sigma^2\\), sino un múltiplo de ella. Esto nos lleva directamente a un estimador insesgado para \\(\\sigma^2\\) dividiendo la SSE por sus grados de libertad, \\(n - p - 1\\): \\[\\hat{\\sigma}^2 = s^2 = \\frac{SSE}{n - p - 1} = \\frac{\\mathbf{e}^T\\mathbf{e}}{n - p - 1}\\] Intuitivamente, perdemos un grado de libertad por cada parámetro que hemos estimado en el modelo (los \\(p\\) coeficientes de las pendientes y el intercepto). La raíz cuadrada de este valor, \\(\\hat{\\sigma}\\), se conoce como el Error Estándar de la Regresión y representa la magnitud de un error de predicción típico.\nCon este estimador, podemos calcular el error estándar de cada coeficiente, que mide la incertidumbre de nuestra estimación para \\(\\beta_j\\): \\[\\text{se}(\\hat{\\beta}_j) = \\sqrt{\\hat{\\sigma}^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}}\\] Bajo normalidad, se puede demostrar además que la cantidad \\(\\frac{SSE}{\\sigma^2}\\) sigue una distribución Chi-cuadrado con \\(n-p-1\\) grados de libertad, un resultado clave para la inferencia formal.\n\n\n\n\n\n\nEjemplo: Estimación de un modelo múltiple\n\n\n\n\n\nPara ilustrar estos conceptos, usemos un ejemplo con datos de precios de viviendas. Supongamos que queremos predecir el precio de una vivienda basándonos en su superficie, número de habitaciones, antigüedad, distancia al centro y si tiene garaje.\n\n\n\nCall:\nlm(formula = precio ~ superficie + habitaciones + antiguedad + \n    distancia_centro + garaje, data = viviendas)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-38847 -11074    867   9898  38486 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      53750.97    6666.71   8.063 7.53e-14 ***\nsuperficie        1171.78      47.28  24.783  &lt; 2e-16 ***\nhabitaciones     15072.31    1303.42  11.564  &lt; 2e-16 ***\nantiguedad        -744.59      75.42  -9.872  &lt; 2e-16 ***\ndistancia_centro -2028.27     164.88 -12.302  &lt; 2e-16 ***\ngarajeSí         25829.43    2349.44  10.994  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15950 on 194 degrees of freedom\nMultiple R-squared:  0.9094,    Adjusted R-squared:  0.9071 \nF-statistic: 389.4 on 5 and 194 DF,  p-value: &lt; 2.2e-16\n\n\nEste output nos muestra:\n\nCoeficientes estimados (\\(\\hat{\\boldsymbol{\\beta}}\\)) y sus errores estándar\nEstadísticos t y p-valores para cada coeficiente\nError estándar residual (\\(\\hat{\\sigma}\\) = 15952 euros)\nR² múltiple (0.9094) - proporción de varianza explicada\nEstadístico F global para contrastar la significancia del modelo",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>El modelo de regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "tema2.html#la-interpretación-de-los-coeficientes",
    "href": "tema2.html#la-interpretación-de-los-coeficientes",
    "title": "3  El modelo de regresión lineal múltiple",
    "section": "3.3 La interpretación de los coeficientes",
    "text": "3.3 La interpretación de los coeficientes\nEstimar los coeficientes y sus errores estándar es solo la mitad del trabajo. La otra mitad, y a menudo la más importante, es interpretarlos correctamente.\nEl concepto fundamental en regresión múltiple es el de ceteris paribus (“lo demás constante”). Cada coeficiente \\(\\beta_j\\) representa el cambio esperado en \\(Y\\) por un cambio de una unidad en \\(X_j\\), manteniendo todas las demás variables predictoras del modelo fijas. Es el efecto “puro” o “aislado” de \\(X_j\\) sobre \\(Y\\), después de haber controlado por la influencia de las otras variables incluidas en el modelo. Matemáticamente, es la derivada parcial del valor esperado de \\(Y\\) con respecto a \\(X_j\\): \\[\\beta_j = \\frac{\\partial E[Y|\\tilde{X}]}{\\partial X_j}\\]\nEsta interpretación es crucialmente diferente de la que se obtiene en una regresión simple. El coeficiente de una regresión simple de \\(Y\\) sobre \\(X_j\\) captura no solo el efecto directo de \\(X_j\\), sino también los efectos indirectos de cualquier otra variable omitida que esté correlacionada tanto con \\(Y\\) como con \\(X_j\\). Por ello, el valor de \\(\\hat{\\beta}_j\\) en una regresión múltiple casi nunca es igual al de una regresión simple.\nLa forma más precisa de entender \\(\\hat{\\beta}_j\\) es a través del concepto de regresión parcial. El coeficiente \\(\\hat{\\beta}_j\\) de la regresión múltiple es idéntico al coeficiente de una regresión simple entre dos conjuntos de residuos:\n\nLos residuos de una regresión de \\(\\mathbf{y}\\) sobre todas las demás variables predictoras (excepto \\(X_j\\)).\nLos residuos de una regresión de \\(\\mathbf{x_j}\\) sobre todas las demás variables predictoras.\n\nEn otras palabras, \\(\\hat{\\beta}_j\\) mide la relación entre la parte de \\(Y\\) que no puede ser explicada por las otras variables y la parte de \\(X_j\\) que tampoco puede ser explicada por las otras variables. Es la asociación entre \\(Y\\) y \\(X_j\\) después de haber “limpiado” o “netado” la influencia de todos los demás predictores de ambas. Este concepto se visualiza en los gráficos de regresión parcial (o added-variable plots), que son una herramienta de diagnóstico fundamental.\n\n\n\n\n\n\nInterpretación práctica de los coeficientes\n\n\n\n\n\nVolviendo a nuestro ejemplo de viviendas, interpretemos cada coeficiente aplicando el principio ceteris paribus:\n\nSuperficie (1172 €/m²): Por cada metro cuadrado adicional, el precio aumenta en promedio 1172 euros, manteniendo constantes el número de habitaciones, antigüedad, distancia al centro y presencia de garaje.\nHabitaciones (15072 €): Cada habitación adicional incrementa el precio en 15072 euros en promedio, controlando por la superficie y demás variables.\nAntigüedad (-745 €/año): Por cada año de antigüedad, el precio disminuye en 745 euros en promedio, ceteris paribus.\nDistancia al centro (-2028 €/km): Cada kilómetro adicional de distancia reduce el precio en 2028 euros en promedio, manteniendo todo lo demás constante.\nGaraje (25829 €): Las viviendas con garaje cuestan 25829 euros más que las que no tienen, en promedio y controlando por las demás variables.\n\nPunto clave: Estos efectos son diferentes de los que obtendríamos con regresiones simples, ya que aquí hemos “limpiado” la influencia de las otras variables.\n\n\n\n\n\n\n\n\n\nLa perspectiva geométrica de mínimos cuadrados\n\n\n\nLa estimación por mínimos cuadrados tiene una interpretación geométrica elegante y potente que nos ayuda a comprender qué está ocurriendo.\nPodemos pensar en el vector de observaciones \\(\\mathbf{y}\\) como un punto en un espacio de \\(n\\) dimensiones. Las columnas de la matriz de diseño \\(\\mathbf{X}\\) generan un subespacio vectorial dentro de \\(\\mathbb{R}^n\\), conocido como el espacio columna de \\(\\mathbf{X}\\), denotado \\(C(\\mathbf{X})\\). Este subespacio contiene todas las posibles combinaciones lineales de nuestros predictores.\nEl método MCO encuentra el vector de valores ajustados \\(\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) que está “más cerca” de \\(\\mathbf{y}\\). Geométricamente, este punto no es otro que la proyección ortogonal del vector \\(\\mathbf{y}\\) sobre el subespacio \\(C(\\mathbf{X})\\).\nEsta proyección se realiza a través de una matriz especial llamada matriz de proyección o matriz sombrero (hat matrix), denotada por \\(\\mathbf{H}\\):\n\\[\\hat{\\mathbf{y}} = \\text{Proj}_{C(\\mathbf{X})}\\,\\mathbf{y} = \\mathbf{H}\\,\\mathbf{y}, \\qquad \\text{donde} \\quad \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\]\nEsta operación induce la descomposición ortogonal fundamental del vector de respuesta:\n\\[\\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{e}\\]\nEl hecho de que la proyección sea ortogonal implica que el vector de residuos \\(\\mathbf{e}\\) es ortogonal (perpendicular) al vector de valores ajustados \\(\\hat{\\mathbf{y}}\\) y, de hecho, a todo el subespacio \\(C(\\mathbf{X})\\). Esta ortogonalidad, \\(\\hat{\\mathbf{y}}^T\\mathbf{e}=0\\), es la base del Teorema de Pitágoras para la regresión, que permite la descomposición de la variabilidad total en una parte explicada y una no explicada.\nDe todo esto, también extraemos que \\(\\mathbf{e}\\) es perpendicular a todo el subespacio \\(C(\\mathbf{X})\\). Luego, \\(\\mathbf{X}^T\\mathbf{e}=0\\), es decir, los residuos quedan descorrelacionados con cada predictor.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>El modelo de regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "tema2.html#evaluación-del-modelo-y-descomposición-de-la-varianza",
    "href": "tema2.html#evaluación-del-modelo-y-descomposición-de-la-varianza",
    "title": "3  El modelo de regresión lineal múltiple",
    "section": "3.4 Evaluación del modelo y descomposición de la varianza",
    "text": "3.4 Evaluación del modelo y descomposición de la varianza\nUna vez estimado el modelo, el siguiente paso es evaluar su desempeño. ¿Qué tan bien se ajustan nuestras predicciones a los datos reales? Aunque ya vimos la perspectiva geométrica y el Teorema de Pitágoras en regresión simple, es importante revisitar estos conceptos porque en regresión múltiple la interpretación y el cálculo de la descomposición de varianza presenta matices adicionales que debemos entender claramente.\nEn regresión múltiple, la Descomposición de la Varianza o ANOVA (Analysis of Variance) cobra especial relevancia porque ahora tenemos múltiples variables explicativas y necesitamos evaluar el aporte conjunto de todas ellas, así como su significancia global.\nLa idea fundamental es que la variabilidad total de la variable respuesta (\\(Y\\)) puede descomponerse en dos partes: una parte que es explicada por nuestro modelo de regresión (ahora con múltiples variables) y otra parte que queda sin explicar, atribuida al error aleatorio.\nPartimos de la identidad: \\((y_i - \\bar{y}) = (\\hat{y}_i - \\bar{y}) + (y_i - \\hat{y}_i) = (\\hat{y}_i - \\bar{y}) + e_i\\).\nElevando al cuadrado y sumando para todas las observaciones (y gracias a la propiedad de ortogonalidad \\(\\hat{\\mathbf{y}}^T\\mathbf{e}=0\\), que hace que los productos cruzados se anulen), llegamos a la descomposición fundamental de las sumas de cuadrados:\n\\[\\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i=1}^n e_i^2\\]\nEsto se conoce como la ecuación de ANOVA:\n\\[SST = SSR + SSE\\]\nDonde:\n\nSST (Suma de Cuadrados Total): Es la variabilidad total de \\(Y\\). Mide la dispersión de los datos observados alrededor de su media.\nSSR (Suma de Cuadrados de la Regresión): Es la variabilidad explicada por el modelo. Mide la dispersión de los valores predichos alrededor de la media.\nSSE (Suma de Cuadrados del Error): Es la variabilidad no explicada o residual. Mide la dispersión de los datos observados alrededor de la línea de regresión.\n\n\n3.4.1 Coeficiente de determinación múltiple\nEl coeficiente de determinación, \\(R^2\\), es la medida de ajuste más popular. Responde a la pregunta: ¿Qué proporción de la variabilidad total de Y es explicada por las variables predictoras del modelo?\n\\[R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\\]\nPropiedades clave:\n\nSu valor siempre está entre 0 (el modelo no explica nada) y 1 (el modelo explica toda la variabilidad).\nPuede interpretarse como el cuadrado de la correlación entre los valores observados y los valores predichos, \\(R^2 = \\text{corr}^2(\\mathbf{y}, \\hat{\\mathbf{y}})\\).\nProblema: \\(R^2\\) nunca decrece al añadir una nueva variable predictora al modelo, incluso si esta es completamente irrelevante. Esto lo convierte en una métrica engañosa para comparar modelos con distinto número de predictores.\n\n\n\n3.4.2 El coeficiente de determinación ajustado\nPara solucionar el problema de \\(R^2\\), utilizamos el \\(R^2\\) ajustado, que introduce una penalización por cada variable añadida. Lo hace comparando las varianzas (sumas de cuadrados divididas por sus grados de libertad) en lugar de solo las sumas de cuadrados:\n\\[R^2_{adj} = 1 - \\frac{SSE/(n-p-1)}{SST/(n-1)} = 1 - \\frac{\\hat{\\sigma}^2}{s_Y^2}\\]\nDonde \\(s_Y^2\\) es la varianza muestral de \\(Y\\). El \\(R^2_{ajustado}\\) solo aumentará si la nueva variable mejora el modelo más de lo que se esperaría por puro azar. Es, por tanto, la métrica preferida para comparar la calidad de ajuste de modelos anidados.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>El modelo de regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "tema2.html#inferencia-estadística-en-el-modelo-múltiple",
    "href": "tema2.html#inferencia-estadística-en-el-modelo-múltiple",
    "title": "3  El modelo de regresión lineal múltiple",
    "section": "3.5 Inferencia Estadística en el Modelo Múltiple",
    "text": "3.5 Inferencia Estadística en el Modelo Múltiple\nLa estimación nos da los valores de los coeficientes para nuestra muestra, pero la inferencia nos permite usar esos valores para sacar conclusiones sobre los parámetros de la población. ¿Son estos coeficientes “reales” o podrían ser fruto del azar muestral? Para responder, nos basamos en las propiedades distributivas de nuestros estimadores.\n\n3.5.1 Contraste de hipótesis sobre los coeficientes\nEl test t nos permite decidir si una variable predictora \\(X_j\\) tiene una relación estadísticamente significativa con \\(Y\\), después de controlar por el efecto de todas las demás variables en el modelo.\n\nHipótesis: La hipótesis nula es que el coeficiente es cero en la población (\\(H_0: \\beta_j = 0\\)), lo que implicaría que \\(X_j\\) no tiene un efecto lineal sobre \\(Y\\) una vez que se consideran los otros predictores. La alternativa es que el coeficiente es distinto de cero (\\(H_1: \\beta_j \\neq 0\\)).\nEstadístico de contraste: Construimos el estadístico t, que mide cuántos errores estándar separan nuestro coeficiente estimado del valor nulo (cero). \\[\\frac{\\hat{\\beta}_j - \\beta_j}{\\text{se}(\\hat{\\beta}_j)} \\sim t_{n-p-1}\\] Bajo la hipótesis nula, el estadístico que calculamos con nuestra muestra es \\(t_{obs} = \\hat{\\beta}_j / \\text{se}(\\hat{\\beta}_j)\\).\nDecisión: Comparamos el valor observado \\(t_{obs}\\) con la distribución t de Student con \\(n-p-1\\) grados de libertad. Si el p-valor asociado es suficientemente pequeño (normalmente &lt; 0.05), rechazamos la hipótesis nula y concluimos que la variable es un predictor estadísticamente significativo.\n\n\n\n3.5.2 Intervalo de confianza para los coeficientes\nMientras que el test t nos da una decisión binaria, el intervalo de confianza nos proporciona un rango de valores plausibles para el verdadero parámetro poblacional \\(\\beta_j\\). Es una herramienta de estimación más informativa.\nLa estructura del intervalo se basa en la distribución t que acabamos de ver:\n\\[\\text{Estimación puntual} \\pm (\\text{Valor crítico}) \\times (\\text{Error estándar})\\]\nPara un nivel de confianza del \\(100(1-\\alpha)\\%\\), el intervalo para \\(\\beta_j\\) es:\n\\[\\hat{\\beta}_j \\pm t_{\\alpha/2, n-p-1} \\cdot \\text{se}(\\hat{\\beta}_j)\\]\n\nInterpretación: Tenemos una confianza del \\(100(1-\\alpha)\\%\\) de que el verdadero valor del parámetro poblacional \\(\\beta_j\\) se encuentra dentro de este rango.\nDualidad con el Contraste de Hipótesis: Existe una relación directa entre el intervalo de confianza y el test t. Si el valor 0 no está incluido en el intervalo de confianza del 95% para \\(\\hat{\\beta}_j\\), es matemáticamente equivalente a rechazar la hipótesis nula \\(H_0: \\beta_j = 0\\) con un nivel de significancia \\(\\alpha=0.05\\). Esto nos da dos formas de llegar a la misma conclusión sobre la significancia de un predictor.\n\n\n\n3.5.3 Inferencia sobre la significancia global del modelo\nEl test F evalúa si el modelo en su conjunto tiene poder predictivo. Es decir, contrasta si al menos uno de los predictores tiene una relación significativa con \\(Y\\).\n\nHipótesis: La hipótesis nula es que todos los coeficientes de las pendientes son simultáneamente cero (\\(H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\)), frente a la alternativa de que al menos uno es distinto de cero (\\(H_1: \\text{Algún } \\beta_j \\neq 0\\)).\nEstadístico de contraste: El estadístico F se construye a partir de la tabla ANOVA, comparando la varianza explicada por el modelo con la varianza residual, ajustando por sus respectivos grados de libertad. \\[F = \\frac{\\text{Varianza Explicada}}{\\text{Varianza No Explicada}} = \\frac{SSR / p}{SSE / (n-p-1)}\\]\nDecisión: Comparamos el valor del estadístico F con una distribución F de Snedecor con \\(p\\) y \\(n-p-1\\) grados de libertad. Un p-valor pequeño indica que el modelo es globalmente significativo y que, como conjunto, nuestros predictores explican una parte de la variabilidad de \\(Y\\) que no es atribuible al azar.\n\nEl test F es una herramienta fundamental, ya que representa el primer paso en la validación de cualquier modelo de regresión múltiple.\n\n\n\n\n\n\nInterpretación de las pruebas estadísticas\n\n\n\nEn nuestro ejemplo de viviendas:\nPrueba F global:\n\nF(5, 194) = 389.45, p &lt; 0.001\nConclusión: El modelo es globalmente significativo. Al menos una variable predictora tiene una relación real con el precio.\n\nPruebas t individuales (ejemplos):\n\nSuperficie: t = 24.78, p &lt; 0.001 → Significativa\nGaraje: t = 10.99, p &lt; 0.001 → Significativa\n\nIntervalos de confianza (95%):\n\nSuperficie: [1079, 1265] euros/m²\nNo incluye el 0, confirma la significancia estadística\n\nInterpretación práctica: Estamos 95% confiados de que el verdadero efecto de la superficie está entre 1079 y 1265 euros por m², controlando por las demás variables.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>El modelo de regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "tema2.html#predicción-con-el-modelo-múltiple",
    "href": "tema2.html#predicción-con-el-modelo-múltiple",
    "title": "3  El modelo de regresión lineal múltiple",
    "section": "3.6 Predicción con el modelo múltiple",
    "text": "3.6 Predicción con el modelo múltiple\nUna vez que hemos ajustado y validado nuestro modelo, podemos utilizarlo para uno de sus propósitos más poderosos: hacer predicciones para nuevas observaciones. Es fundamental distinguir entre dos objetivos de predicción diferentes, ya que cada uno conlleva un nivel de incertidumbre distinto.\nSupongamos que tenemos un nuevo conjunto de valores para las variables predictoras, representado por el vector \\(\\mathbf{x}_0^T = [1, x_{01}, x_{02}, \\dots, x_{0p}]\\). La predicción puntual en ambos casos es la misma:\n\\[\\hat{y}_0 = \\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}\\]\nSin embargo, esta estimación puntual está sujeta a error. Para cuantificar esta incertidumbre, construimos dos tipos de intervalos.\n\n3.6.1 Intervalo de confianza para la respuesta media\nEste intervalo responde a la pregunta: ¿cuál es el valor promedio de Y para todas las observaciones con las características \\(\\mathbf{x}_0\\)? Su objetivo es acotar la posición de la verdadera (pero desconocida) superficie de regresión poblacional en el punto \\(\\mathbf{x}_0\\).\nLa incertidumbre aquí proviene únicamente de la estimación de los coeficientes \\(\\hat{\\boldsymbol{\\beta}}\\). La varianza de esta predicción media es:\n\\[\\text{Var}(\\hat{y}_0) = \\text{Var}(\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}) = \\mathbf{x}_0^T \\text{Var}(\\hat{\\boldsymbol{\\beta}}) \\mathbf{x}_0 = \\sigma^2 \\mathbf{x}_0^T (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{x}_0\\]\nReemplazando \\(\\sigma^2\\) por su estimador insesgado \\(\\hat{\\sigma}^2\\), el intervalo de confianza al \\(100(1-\\alpha)\\%\\) para la respuesta media \\(E[Y|\\mathbf{X}=\\mathbf{x}_0]\\) es:\n\\[\\hat{y}_0 \\pm t_{\\alpha/2, n-p-1} \\cdot \\sqrt{\\hat{\\sigma}^2 \\mathbf{x}_0^T (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{x}_0}\\]\n\n\n3.6.2 Intervalo de predicción para una observación individual\nEste intervalo responde a una pregunta más ambiciosa: ¿entre qué valores se encontrará la respuesta de una única y nueva observación con las características \\(\\mathbf{x}_0\\)?\nEste intervalo debe considerar dos fuentes de incertidumbre:\n\nLa incertidumbre sobre la localización de la verdadera superficie de regresión (la misma que en el intervalo de confianza).\nLa variabilidad aleatoria inherente a una sola observación, que se desvía de la media poblacional (el error \\(\\varepsilon_0\\), cuya varianza es \\(\\sigma^2\\)).\n\nPor esta razón, un intervalo de predicción siempre será más ancho que un intervalo de confianza para el mismo nivel de significancia. La varianza del error de predicción es la suma de las dos fuentes de varianza:\n\\[\\text{Var}(y_0 - \\hat{y}_0) = \\text{Var}(\\varepsilon_0) + \\text{Var}(\\hat{y}_0) = \\sigma^2 + \\sigma^2 \\mathbf{x}_0^T (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{x}_0\\]\nEl intervalo de predicción al \\(100(1-\\alpha)\\%\\) para una observación individual \\(y_0\\) es:\n\\[\\hat{y}_0 \\pm t_{\\alpha/2, n-p-1} \\cdot \\sqrt{\\hat{\\sigma}^2 \\left(1 + \\mathbf{x}_0^T (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{x}_0\\right)}\\]\nLa diferencia clave es el “+1” dentro de la raíz cuadrada, que representa la varianza de la nueva observación. Visualmente, tanto la banda de confianza como la de predicción son más estrechas cerca del “centroide” de los datos (la media multivariante de los predictores) y se ensanchan a medida que nos alejamos hacia valores más extremos de los predictores.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>El modelo de regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "tema2.html#diagnóstico-del-modelo-múltiple",
    "href": "tema2.html#diagnóstico-del-modelo-múltiple",
    "title": "3  El modelo de regresión lineal múltiple",
    "section": "3.7 Diagnóstico del modelo múltiple",
    "text": "3.7 Diagnóstico del modelo múltiple\nAl igual que en la regresión simple, una vez ajustado el modelo es fundamental realizar un diagnóstico exhaustivo para verificar que los supuestos del modelo lineal se cumplen. La fiabilidad de todas nuestras inferencias (p-valores e intervalos de confianza) descansa sobre la validez de estos supuestos. El proceso sigue basándose en el análisis de los residuos, nuestra ventana a los errores teóricos no observables (Fox y Weisberg 2018; Harrell 2015).\n\n3.7.1 Verificación de los supuestos clásicos\nLos supuestos de linealidad, homocedasticidad, normalidad e independencia se verifican con herramientas muy similares a las vistas en el capítulo anterior, por lo que aquí las resumiremos y presentaremos una herramienta adicional para la linealidad.\n\nNormalidad: El gráfico Q-Q de los residuos estudentizados y el test de Shapiro-Wilk siguen siendo las herramientas principales para comprobar que los errores se distribuyen de forma Normal.\nIndependencia: Para datos de series temporales, el gráfico de residuos contra el tiempo y el test de Durbin-Watson se utilizan de la misma manera para detectar autocorrelación.\nHomocedasticidad (Varianza Constante): El gráfico Scale-Location y el test de Breusch-Pagan siguen siendo los métodos de referencia para detectar heterocedasticidad (patrones de embudo en la dispersión de los residuos).\n\nPara la linealidad, el gráfico de Residuos vs. Valores Ajustados sigue siendo la primera herramienta a inspeccionar. Una nube de puntos sin patrones alrededor del cero sugiere que la forma funcional del modelo es globalmente correcta. Sin embargo, en el contexto múltiple, este gráfico podría ocultar una relación no lineal con una variable específica. Para ello, disponemos de una herramienta más precisa.\n\n3.7.1.1 Gráficos de componente más residuo\nEl gráfico de Componente más Residuo (CPR o Partial Residual Plots) nos permite visualizar la relación entre la variable respuesta y un único predictor \\(X_j\\), ajustando por el efecto de todos los demás predictores. Para cada predictor \\(X_j\\), el gráfico muestra:\n\\[\\text{Residuo Parcial} = e_i + \\hat{\\beta}_j x_{ij} \\quad \\text{vs.} \\quad x_{ij}\\] Si la relación es lineal, los puntos deben seguir la línea de regresión. Una desviación sistemática (una curva) sugiere que la relación con esa variable específica no es lineal y que podría necesitar una transformación (p.ej., logarítmica o cuadrática).\n\n\n\n\n\n\nEjemplo práctico: Gráficos de componente + residuo\n\n\n\nLos gráficos CPR nos permiten visualizar si la relación de cada predictor con la respuesta es verdaderamente lineal:\n\n# Cargar librerías necesarias\nsuppressPackageStartupMessages(library(car))\n\n# Gráficos de Componente más Residuo (CPR)\ncrPlots(modelo, main = \"Gráficos de Componente + Residuo\")\n\n\n\n\n\n\n\n\nInterpretación:\n\nLínea sólida: Muestra la relación lineal esperada (pendiente = coeficiente de regresión)\nLínea punteada: Suavizado no paramétrico de los puntos\nCoincidencia de líneas: Sugiere linealidad adecuada\nDivergencia significativa: Indica posible no-linealidad que requiere transformación\n\nSi vemos curvas sistemáticas en algún gráfico CPR, es señal de que esa variable podría necesitar una transformación (log, cuadrática, etc.).\n\n\n\n\n\n3.7.2 Diagnóstico de multicolinealidad\nLa multicolinealidad es un problema que solo existe en la regresión múltiple. Ocurre cuando dos o más variables predictoras están fuertemente correlacionadas entre sí.\n\n3.7.2.1 Consecuencias de la multicolinealidad\nLa multicolinealidad no viola los supuestos de Gauss-Markov (los estimadores siguen siendo insesgados y eficientes), pero puede arruinar la interpretación práctica de un modelo:\n\nVarianza de los estimadores inflada: Los errores estándar de los coeficientes de las variables colineales se vuelven muy grandes. Esto dificulta o imposibilita declarar un predictor como estadísticamente significativo, incluso si lo es.\nInestabilidad de los coeficientes: Pequeños cambios en los datos (o añadir/quitar una variable) pueden provocar cambios drásticos en las estimaciones de los coeficientes, incluso cambiando su signo, lo que hace que la interpretación sea poco fiable.\nContradicciones en los contrastes: Se puede dar la paradoja de tener un modelo globalmente significativo (test F con p-valor bajo y \\(R^2\\) alto) pero con ningún predictor individual significativo (tests t con p-valores altos).\n\n\n\n3.7.2.2 Detección de la multicolinealidad\n\nMatriz de correlaciones: Un primer paso es examinar la matriz de correlaciones entre los predictores. Coeficientes de correlación altos (p. ej., &gt; 0.8) son una señal de alerta. Sin embargo, este método no detecta la colinealidad que involucra a tres o más variables.\nFactor de Inflación de la Varianza (VIF): Es la herramienta de diagnóstico definitiva. Para cada predictor \\(X_j\\), se calcula su VIF de la siguiente manera:\n\nSe ajusta una regresión lineal de \\(X_j\\) en función de todas las demás variables predictoras: \\(X_j \\sim X_1 + \\dots + X_{j-1} + X_{j+1} + \\dots + X_p\\).\nSe obtiene el \\(R^2\\) de este modelo auxiliar, que llamamos \\(R_j^2\\). Este valor nos dice qué proporción de la varianza de \\(X_j\\) es explicada por los otros predictores.\nEl VIF se calcula como: \\[VIF_j = \\frac{1}{1 - R_j^2}\\]\n\n\nInterpretación: El VIF nos dice por qué factor se infla la varianza del estimador \\(\\hat{\\beta}_j\\) debido a su relación con los otros predictores.\n\n\n\n\n\n\n\n\nReglas prácticas para interpretar VIF\n\n\n\n\nVIF = 1: Ausencia de colinealidad (ideal)\nVIF &gt; 5: Valores preocupantes que requieren atención\nVIF &gt; 10: Multicolinealidad seria que debe ser tratada\n\nRecordar: El VIF indica por qué factor se multiplica la varianza del coeficiente debido a la multicolinealidad.\n\n\n\n\n\n\n\n\nEjemplo: Diagnóstico de multicolinealidad\n\n\n\nCaso 1: Sin problemas de multicolinealidad (nuestro modelo actual)\n\n# Calculamos VIF para nuestro modelo de viviendas\nvif_values &lt;- vif(modelo)\ncat(\"VIF en nuestro modelo:\\n\")\n\nVIF en nuestro modelo:\n\nround(vif_values, 2)\n\n      superficie     habitaciones       antiguedad distancia_centro \n            1.40             1.40             1.01             1.01 \n          garaje \n            1.01 \n\n\nComo todos los VIF &lt; 5, no hay problemas de multicolinealidad.\nCaso 2: Ejemplo con multicolinealidad problemática\n\n# Simulamos un caso con multicolinealidad\nset.seed(456)\nn &lt;- 100\n\n# Creamos variables altamente correlacionadas\nsuperficie &lt;- runif(n, 50, 200)\nhabitaciones_sim &lt;- round(superficie/25 + rnorm(n, 0, 0.5))  # Muy correlacionada con superficie\nmetros_cuadrados &lt;- superficie + rnorm(n, 0, 5)  # Esencialmente la misma que superficie\nprecio_sim &lt;- 1000*superficie + 5000*habitaciones_sim + rnorm(n, 0, 10000)\n\n# Modelo con multicolinealidad\nmodelo_colineal &lt;- lm(precio_sim ~ superficie + habitaciones_sim + metros_cuadrados)\n\n# VIF del modelo problemático\nvif_problematico &lt;- vif(modelo_colineal)\ncat(\"\\nVIF en modelo con multicolinealidad:\\n\")\n\n\nVIF en modelo con multicolinealidad:\n\nround(vif_problematico, 2)\n\n      superficie habitaciones_sim metros_cuadrados \n           90.96            10.55            76.57 \n\n# Matriz de correlaciones para explicar el problema\ndatos_problema &lt;- data.frame(superficie, habitaciones_sim, metros_cuadrados)\ncor_problema &lt;- cor(datos_problema)\ncat(\"\\nMatriz de correlaciones:\\n\")\n\n\nMatriz de correlaciones:\n\nround(cor_problema, 3)\n\n                 superficie habitaciones_sim metros_cuadrados\nsuperficie            1.000            0.951            0.993\nhabitaciones_sim      0.951            1.000            0.942\nmetros_cuadrados      0.993            0.942            1.000\n\n\nInterpretación: - VIF &gt; 10: Problema serio de multicolinealidad\n- La correlación superficie-metros_cuadrados ≈ 1 explica el problema\n\n\n\n\n\n\n\n\nSoluciones a los problemas de multicolinealidad\n\n\n\n\n\nEnfrentar la multicolinealidad no siempre significa que debamos alterar el modelo. La solución adecuada depende de la severidad del problema (medido con el VIF) y, sobre todo, del objetivo de nuestro análisis.\n\nNo hacer nada: Si el objetivo principal del modelo es la predicción y no la interpretación de los coeficientes individuales, la multicolinealidad no es un problema grave. El modelo en su conjunto puede tener un buen poder predictivo, aunque los efectos individuales de las variables colineales sean inestables. Además, si las variables colineales no son las variables de interés de nuestra investigación, podemos ignorar su multicolinealidad.\nEliminar una de las variables correlacionadas: Esta es la solución más simple y común. Si dos o más variables miden esencialmente el mismo concepto (p. ej., “educación en años” y “nivel educativo alcanzado”), una de ellas es redundante. Se puede eliminar la que tenga menor relevancia teórica o la que, individualmente, tenga una menor correlación con la variable respuesta.\nCombinar las variables colineales: En lugar de eliminar información, podemos combinar las variables colineales en un único predictor compuesto. Por ejemplo, si tenemos gasto_en_publicidad_tv y gasto_en_publicidad_radio, podríamos crear una nueva variable gasto_total_en_medios. Para casos más complejos, se pueden utilizar técnicas de reducción de dimensionalidad como el Análisis de Componentes Principales (PCA) para crear un índice que capture la información conjunta de las variables correlacionadas.\nUtilizar métodos de estimación alternativos: Si no es posible o deseable modificar los predictores, se pueden usar técnicas de regresión penalizada que están diseñadas para manejar la multicolinealidad.\n\nRegresión Ridge: Es el método por excelencia para este problema. Añade un pequeño sesgo a las estimaciones de los coeficientes para reducir drásticamente su varianza, produciendo un modelo mucho más estable y fiable.\nLasso y Elastic Net: Son otras técnicas de regresión penalizada que también manejan bien la colinealidad y, además, pueden realizar selección de variables al hacer que algunos coeficientes sean exactamente cero.\n\nAumentar el tamaño de la muestra: En algunos casos, la multicolinealidad puede ser un artefacto de una muestra pequeña. Recolectar más datos puede, en ocasiones, reducir la correlación entre los predictores y disminuir la varianza de los coeficientes.\n\nLa elección de la estrategia debe ser una decisión meditada, sopesando la simplicidad, la interpretabilidad y la robustez del modelo final.\n\n\n\n\n\n\n3.7.3 Identificación de observaciones influyentes\nLos conceptos de outlier (residuo grande), leverage (valor atípico en los predictores) e influencia (impacto en el modelo) son los mismos que en regresión simple. Sin embargo, el caso múltiple nos ofrece herramientas de diagnóstico más específicas.\n\n3.7.3.1 DFBETAS: Influencia sobre coeficientes individuales\nMientras que la Distancia de Cook mide la influencia global de una observación sobre todos los coeficientes a la vez, los DFBETAS miden el impacto que tiene eliminar la observación \\(i\\) sobre cada coeficiente \\(\\beta_j\\) individualmente.\n\\[\\text{DFBETA}_{j,i} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(-i)}}{\\text{se}(\\hat{\\beta}_{j(-i)})}\\]\nUn DFBETA grande para el coeficiente \\(\\beta_j\\) indica que la observación \\(i\\) tiene un fuerte poder de atracción sobre la estimación de ese coeficiente en particular. Una regla común es considerar problemáticos los puntos con \\(|\\text{DFBETA}_{j,i}| &gt; \\frac{2}{\\sqrt{n}}\\).\n\n\n3.7.3.2 Gráficos de regresión parcial\nEstos gráficos son una de las herramientas visuales más potentes y elegantes de la regresión múltiple. Un gráfico de regresión parcial para un predictor \\(X_j\\) nos permite ver su relación con la respuesta \\(Y\\) después de haber eliminado el efecto lineal de todos los demás predictores.\nSe construye de la siguiente forma:\n\nSe calculan los residuos de la regresión de \\(Y\\) en función de todos los predictores excepto \\(X_j\\). Llamemos a estos residuos \\(e_{Y|X_{-j}}\\).\nSe calculan los residuos de la regresión de \\(X_j\\) en función de todos los demás predictores. Llamemos a estos residuos \\(e_{X_j|X_{-j}}\\).\nSe grafica \\(e_{Y|X_{-j}}\\) (eje Y) contra \\(e_{X_j|X_{-j}}\\) (eje X).\n\n\n\n\n\n\n\nPropiedad clave de los gráficos de regresión parcial\n\n\n\nLa magia de este gráfico: La pendiente de la línea de regresión ajustada a estos puntos es exactamente el coeficiente de regresión múltiple \\(\\hat{\\beta}_j\\).\nEsta equivalencia matemática es lo que hace que estos gráficos sean tan poderosos para entender la interpretación de los coeficientes en regresión múltiple.\n\n\nEstos gráficos son útiles para:\n\nVisualizar la magnitud y significancia del efecto de una variable “ajustada”.\nDetectar no-linealidades en la relación parcial de una variable.\nIdentificar observaciones que son influyentes para la estimación de un coeficiente específico (puntos con alto leverage o residuos grandes en este gráfico parcial).\n\n\n\n\n\n\n\nEjemplo: Gráficos de Regresión Parcial\n\n\n\nLos gráficos de regresión parcial nos permiten visualizar la relación “limpia” entre cada predictor y la variable respuesta:\n\n# Gráficos de regresión parcial para todas las variables\navPlots(modelo, main = \"Gráficos de regresión parcial\")\n\n\n\n\n\n\n\n\n¿Qué vemos en cada gráfico?\n\nEje X: Residuos de la regresión de \\(X_j\\) vs. todos los demás predictores\nEje Y: Residuos de la regresión de \\(Y\\) vs. todos los demás predictores (excepto \\(X_j\\))\nPendiente: Es exactamente el coeficiente \\(\\hat{\\beta}_j\\) del modelo múltiple\nPuntos alejados: Observaciones influyentes para ese coeficiente específico\n\nInterpretación práctica:\n\nUna relación lineal clara confirma la validez del modelo\nPuntos muy alejados de la línea pueden ser observaciones influyentes\nPatrones curvos sugieren no-linealidad en esa variable específica\n\n\n\n\n\n\n\nDraper, NR. 1998. Applied regression analysis. McGraw-Hill. Inc.\n\n\nFox, John, y Sanford Weisberg. 2018. An R companion to applied regression. Sage publications.\n\n\nHarrell, Frank E., Jr. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. Second. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2021. An Introduction to Statistical Learning with Applications in R. Second. Springer.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.\n\n\nWeisberg, S. 2005. «Applied linear regression». Wiley.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>El modelo de regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "tema3.html",
    "href": "tema3.html",
    "title": "4  Ingeniería de características: transformaciones de variables e interacciones",
    "section": "",
    "text": "4.1 Transformaciones de variables: propósitos y aplicaciones\nEn la práctica del análisis de datos, los datos en su estado bruto raramente están en la forma óptima para el modelado estadístico. La ingeniería de características es el proceso fundamental que transforma, combina y crea variables para maximizar la capacidad predictiva y la interpretabilidad de nuestros modelos (Kuhn y Johnson 2019; Zheng y Casari 2018).\nEste proceso abarca tres áreas principales que exploraremos en profundidad:\nTransformaciones de variables: Las transformaciones matemáticas nos permiten abordar múltiples problemas simultaneamente: linearizar relaciones no lineales, estabilizar la varianza (heterocedasticidad), aproximar la distribución de los errores a la normalidad, y reducir la influencia de valores atípicos. Dominar cuándo y cómo aplicar transformaciones logarítmicas, potenciales, Box-Cox o Yeo-Johnson es fundamental para optimizar nuestros modelos (Box y Cox 1964; Yeo y Johnson 2000).\nTratamiento de variables categóricas: Las variables categóricas requieren estrategias específicas de codificación que preserven la información relevante sin introducir supuestos erróneos. La elección entre codificación ordinal, one-hot encoding, o técnicas más avanzadas puede impactar significativamente el rendimiento del modelo (Potdar, Pardawala, y Pai 2017).\nInteracciones entre variables: Las interacciones capturan cómo el efecto de una variable puede cambiar según el nivel de otra variable, revelando patrones que los efectos principales por sí solos no pueden detectar. Comprender los diferentes tipos de interacciones y sus aplicaciones es clave para modelar relaciones complejas en el mundo real (Jaccard y Turrisi 2003).\nEn el análisis de datos y la construcción de modelos estadísticos, los datos en su forma original no siempre están preparados para obtener el máximo rendimiento de nuestros modelos. Las transformaciones de variables son herramientas fundamentales que nos permiten modificar la estructura matemática de nuestros datos para abordar problemas específicos y mejorar significativamente el ajuste del modelo (Box y Cox 1964; Carroll y Ruppert 1988).\nLa clave del éxito en las transformaciones está en diagnosticar correctamente el problema que enfrentamos y seleccionar la transformación apropiada. Cada transformación tiene propósitos específicos y consecuencias interpretativas que debemos comprender profundamente.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ingeniería de características: transformaciones de variables e interacciones</span>"
    ]
  },
  {
    "objectID": "tema3.html#transformaciones-de-variables-propósitos-y-aplicaciones",
    "href": "tema3.html#transformaciones-de-variables-propósitos-y-aplicaciones",
    "title": "4  Ingeniería de características: transformaciones de variables e interacciones",
    "section": "",
    "text": "4.1.1 Diagnóstico: ¿Cuándo transformar?\nEl arte de las transformaciones no está en aplicarlas indiscriminadamente, sino en diagnosticar correctamente cuál es el problema que enfrentamos y seleccionar la transformación más apropiada para resolverlo. Un diagnóstico erróneo puede llevarnos a aplicar una transformación innecesaria o, peor aún, contraproducente que distorsione las relaciones reales en los datos.\nLa práctica común de “probar transformaciones hasta que mejore el ajuste” es metodológicamente peligrosa. Este enfoque de transformación por ensayo y error puede llevarnos a:\n\nSobreajuste: Optimizar el modelo para los datos específicos que tenemos, perdiendo capacidad de generalización.\nPérdida de interpretabilidad: Aplicar transformaciones complejas sin comprender su significado teórico.\nViolación de supuestos: Resolver un problema creando otros nuevos (ej. transformar para normalidad pero introducir heterocedasticidad).\nSesgo de selección: Elegir la transformación que da los “mejores” resultados sin justificación teórica.\n\nEl proceso de diagnóstico debe ser sistemático y basado en evidencia visual y estadística. No basta con aplicar transformaciones porque “mejoran el R²”; debemos entender qué problema específico estamos resolviendo y cómo la transformación aborda ese problema desde una perspectiva teórica sólida.\nUn enfoque metodológicamente sólido sigue estos principios:\n\nDiagnóstico previo: Identificar problemas específicos mediante análisis visual y tests estadísticos antes de decidir transformar.\nJustificación teórica: Cada transformación debe tener una base conceptual sólida. Por ejemplo, usar logaritmos para relaciones multiplicativas o raíz cuadrada para estabilizar varianza Poisson.\nEvaluación integral: No solo considerar el ajuste estadístico, sino también la interpretabilidad, robustez y generalización del modelo transformado.\nValidación posterior: Verificar que la transformación realmente resuelve el problema identificado sin crear nuevos problemas.\nParsimonia: Preferir la transformación más simple que resuelva efectivamente el problema (principio de Occam aplicado a transformaciones).\n\n\n\n\n\n\n\nRecordatorio: Diagnóstico de problemas en regresión lineal\n\n\n\n\n\nIdentificación de no linealidad: La no linealidad es uno de los problemas más comunes que enfrentamos en el modelado. Diagnóstico visual: gráficos de dispersión (Y vs. X), gráficos de componente + residuo (CPR plots), análisis de residuos vs. valores ajustados. Diagnóstico estadístico: Test de Ramsey RESET que evalúa si potencias de los valores ajustados mejoran significativamente el modelo.\nDetección de heterocedasticidad: La heterocedasticidad (varianza no constante) viola supuestos fundamentales de la regresión lineal y sesga las inferencias estadísticas. Diagnóstico visual: gráfico de residuos vs. valores ajustados (patrón “embudo”), gráfico Scale-Location, residuos vs. variables predictoras individuales. Diagnóstico estadístico: Test de Breusch-Pagan (el más utilizado) y Test de White (más general).\nEvaluación de normalidad de residuos: Aunque la normalidad no es crítica para estimación de coeficientes, sí es importante para inferencia estadística. Diagnóstico visual: histograma de residuos, QQ-plot de residuos. Diagnóstico estadístico: Test de Shapiro-Wilk (muestras pequeñas n&lt;50) y Test de Jarque-Bera.\nDetección de outliers y observaciones influyentes: Es fundamental distinguir entre outliers (valores extremos en Y) y observaciones influyentes (impacto en coeficientes). Outliers: boxplot de variable respuesta, residuos estudentizados, criterios |t\\(_i\\)| &gt; 2. Observaciones influyentes: análisis de leverage, distancia de Cook, DFBETAS, DFFITS, con criterios específicos como h\\(_i\\) &gt; 2p/n y D\\(_i\\) &gt; 4/n. Las observaciones se clasifican en: normales, outliers no influyentes, influyentes sin ser outliers, y outliers influyentes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ingeniería de características: transformaciones de variables e interacciones</span>"
    ]
  },
  {
    "objectID": "tema3.html#escalado-y-normalización-preparando-variables-para-el-análisis",
    "href": "tema3.html#escalado-y-normalización-preparando-variables-para-el-análisis",
    "title": "4  Ingeniería de características: transformaciones de variables e interacciones",
    "section": "4.2 Escalado y normalización: preparando variables para el análisis",
    "text": "4.2 Escalado y normalización: preparando variables para el análisis\nAntes de aplicar transformaciones complejas, es fundamental asegurar que nuestras variables estén en escalas comparables. Aunque la regresión lineal ordinaria no requiere estrictamente el escalado de variables para obtener estimadores insesgados, el escalado se vuelve crítico para la interpretación y esencial en métodos avanzados de modelado.\nEn regresión múltiple, los coeficientes representan el cambio en Y por unidad de cambio en cada variable predictora. Cuando las variables tienen escalas muy diferentes, los coeficientes pierden comparabilidad directa. Una variable medida en miles de euros tendrá coeficientes numéricamente pequeños, mientras que una variable medida en porcentajes tendrá coeficientes grandes, independientemente de su importancia real en el modelo.\nEsta disparidad de escalas genera problemas interpretativos fundamentales: comparar la “importancia” relativa de las variables se vuelve imposible basándose únicamente en la magnitud de los coeficientes. El escalado resuelve este problema permitiendo que los coeficientes estandarizados (beta coefficients) representen cambios en desviaciones estándar, facilitando comparaciones directas entre predictores y proporcionando una base sólida para evaluar la importancia relativa de cada variable.\n\n\n\n\n\n\nEscalado en métodos de regularización\n\n\n\n\n\nEn regresión con regularización (Ridge, Lasso), el problema se agrava dramáticamente. Las penalizaciones L1 y L2 afectan desproporcionadamente a variables con escalas grandes, llevando a regularización injusta donde variables con unidades grandes son penalizadas más severamente que variables con unidades pequeñas, independientemente de su relevancia predictiva. Esto puede resultar en selección de variables sesgada y estimadores subóptimos. Este tema se desarrollará en profundidad en el siguiente capítulo sobre métodos de regularización.\n\n\n\nEl escalado no es solo una cuestión técnica, sino una decisión metodológica que afecta directamente la interpretación y validez de nuestros resultados. La elección entre estandarización, normalización min-max, o escalado robusto debe basarse en las características de los datos y los objetivos del análisis, considerando siempre el impacto en la interpretabilidad de los resultados finales.\n\n4.2.1 Estandarización (Z-Score)\nLa estandarización es la técnica de escalado más utilizada en estadística. Transforma cada variable para que tenga media cero y desviación estándar uno, preservando la forma de la distribución original.\n\\[X_{\\text{estandarizado}} = \\frac{X - \\bar{X}}{\\sigma_X}\\]\nPropiedades de la estandarización:\n\nPreserva la forma de la distribución: Si X era normal, X estandarizado también lo será.\nFacilita la comparación: Los coeficientes en regresión múltiple se vuelven comparables.\nRobusta ante outliers moderados: La media y desviación estándar son menos sensibles que min-max a valores extremos.\n\nCuándo usar estandarización:\n\nVariables con distribuciones aproximadamente normales.\nCuando necesitamos preservar la información sobre la variabilidad relativa.\nEn regresión múltiple para comparar la importancia relativa de las variables.\nComo paso previo a técnicas multivariantes (PCA, análisis discriminante).\n\n\n\n4.2.2 Normalización Min-Max\nLa normalización Min-Max escala las variables a un rango específico, típicamente [0,1], preservando las relaciones relativas entre los valores.\n\\[X_{\\text{normalizado}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\nPropiedades de la normalización Min-Max:\n\nRango acotado: Todas las variables transformadas tienen el mismo rango [0,1].\nPreserva relaciones: Las distancias relativas entre observaciones se mantienen.\nInterpretación intuitiva: 0 representa el mínimo observado, 1 el máximo observado.\n\nCuándo usar normalización Min-Max:\n\nCuando necesitamos un rango específico (ej. entradas de redes neuronales).\nVariables con distribuciones uniformes o sin outliers extremos.\nCuando la interpretación en términos de mínimo-máximo es relevante.\nEn algoritmos que requieren entradas en [0,1] (algunos métodos de ensemble).\n\nLimitaciones:\n\nMuy sensible a outliers: Un solo valor extremo puede comprimir toda la distribución.\nNo preserva la normalidad: Una distribución normal se vuelve uniforme tras Min-Max.\n\n\n\n4.2.3 Escalado robusto\nPara datos con outliers significativos, el escalado robusto utiliza la mediana y el rango intercuartílico (IQR) en lugar de la media y desviación estándar:\n\\[X_{\\text{robusto}} = \\frac{X - \\text{mediana}(X)}{\\text{IQR}(X)}\\]\nEste método es menos sensible a valores extremos y preserva mejor la estructura de los datos en presencia de outliers.\n\n\n\n\n\n\nEjemplo comparativo: Escalado de variables\n\n\n\n\n\n\n# Generar datos de ejemplo con diferentes escalas\nset.seed(123)\nn &lt;- 100\n\n# Variable de ejemplo: Ingresos en miles de euros\ningresos &lt;- rnorm(n, mean = 50, sd = 15)\n\n# Aplicar diferentes transformaciones\ningresos_std &lt;- scale(ingresos)[,1]  # Estandarización\ningresos_norm &lt;- (ingresos - min(ingresos)) / (max(ingresos) - min(ingresos))  # Min-Max\ningresos_robust &lt;- (ingresos - median(ingresos)) / IQR(ingresos)  # Robusto\n\n# Crear tabla comparativa\nlibrary(knitr)\ntabla_escalado &lt;- data.frame(\n  Método = c(\"Original\", \"Estandarización\", \"Min-Max\", \"Escalado Robusto\"),\n  Media = round(c(mean(ingresos), mean(ingresos_std), mean(ingresos_norm), median(ingresos_robust)), 3),\n  Desviación = round(c(sd(ingresos), sd(ingresos_std), sd(ingresos_norm), mad(ingresos_robust)), 3),\n  Mínimo = round(c(min(ingresos), min(ingresos_std), min(ingresos_norm), min(ingresos_robust)), 3),\n  Máximo = round(c(max(ingresos), max(ingresos_std), max(ingresos_norm), max(ingresos_robust)), 3),\n  Rango = round(c(\n    max(ingresos) - min(ingresos),\n    max(ingresos_std) - min(ingresos_std),\n    max(ingresos_norm) - min(ingresos_norm),\n    max(ingresos_robust) - min(ingresos_robust)\n  ), 3)\n)\n\nkable(tabla_escalado, caption = \"Comparación de métodos de escalado en variable Ingresos\")\n\n\nComparación de métodos de escalado en variable Ingresos\n\n\nMétodo\nMedia\nDesviación\nMínimo\nMáximo\nRango\n\n\n\n\nOriginal\n51.356\n13.692\n15.362\n82.810\n67.448\n\n\nEstandarización\n0.000\n1.000\n-2.629\n2.297\n4.926\n\n\nMin-Max\n0.534\n0.203\n0.000\n1.000\n1.000\n\n\nEscalado Robusto\n0.000\n0.750\n-2.000\n1.793\n3.792\n\n\n\n\n\nInterpretación de los resultados:\n\nOriginal: Ingresos en miles de euros con gran variabilidad (SD ≈ 15)\nEstandarización: Media = 0, SD = 1, preservando la forma de la distribución\nMin-Max: Valores acotados entre [0,1], comprimiendo toda la variabilidad en este rango\nEscalado Robusto: Centrado en la mediana, menos sensible a valores extremos\n\nCada método transforma los datos de manera diferente según el objetivo: comparabilidad (estandarización), rango específico (min-max), o robustez ante outliers (robusto).\n\n\n\n\n\n\n\n\n\nEjemplo con outliers: Escalado robusto\n\n\n\n\n\n\n# Datos con outliers\nset.seed(456)\ndatos_normales &lt;- rnorm(95, 10, 2)\noutliers &lt;- c(25, 30, 35, 40, 45)\ndatos_outliers &lt;- c(datos_normales, outliers)\n\n# Aplicar diferentes métodos de escalado\nstd_clasica &lt;- scale(datos_outliers)[,1]\nnorm_minmax &lt;- (datos_outliers - min(datos_outliers)) / (max(datos_outliers) - min(datos_outliers))\nescalado_robusto &lt;- (datos_outliers - median(datos_outliers)) / IQR(datos_outliers)\n\n# Crear tabla comparativa\nlibrary(knitr)\ntabla_outliers &lt;- data.frame(\n  Método = c(\"Original\", \"Estandarización\", \"Min-Max\", \"Escalado Robusto\"),\n  Media_Mediana = round(c(mean(datos_outliers), mean(std_clasica), mean(norm_minmax), median(escalado_robusto)), 3),\n  Desviación = round(c(sd(datos_outliers), sd(std_clasica), sd(norm_minmax), mad(escalado_robusto)), 3),\n  Mínimo = round(c(min(datos_outliers), min(std_clasica), min(norm_minmax), min(escalado_robusto)), 3),\n  Máximo = round(c(max(datos_outliers), max(std_clasica), max(norm_minmax), max(escalado_robusto)), 3),\n  Q25_Q75 = c(\n    paste(round(quantile(datos_outliers, c(0.25, 0.75)), 2), collapse = \" - \"),\n    paste(round(quantile(std_clasica, c(0.25, 0.75)), 2), collapse = \" - \"),\n    paste(round(quantile(norm_minmax, c(0.25, 0.75)), 2), collapse = \" - \"),\n    paste(round(quantile(escalado_robusto, c(0.25, 0.75)), 2), collapse = \" - \")\n  )\n)\n\nkable(tabla_outliers, caption = \"Comparación de métodos de escalado con outliers presentes\")\n\n\nComparación de métodos de escalado con outliers presentes\n\n\n\n\n\n\n\n\n\n\nMétodo\nMedia_Mediana\nDesviación\nMínimo\nMáximo\nQ25_Q75\n\n\n\n\nOriginal\n11.447\n5.996\n5.491\n45.000\n8.93 - 11.81\n\n\nEstandarización\n0.000\n1.000\n-0.993\n5.595\n-0.42 - 0.06\n\n\nMin-Max\n0.151\n0.152\n0.000\n1.000\n0.09 - 0.16\n\n\nEscalado Robusto\n0.000\n0.778\n-1.654\n12.108\n-0.45 - 0.55\n\n\n\n\n\nAnálisis del impacto de outliers:\n\nDatos originales: Los outliers extienden el rango de ~6-14 a 6-45, distorsionando las medidas centrales\nEstandarización: Afectada por outliers en media y desviación estándar, resultando en distribución asimétrica\nMin-Max: Extremadamente sensible. Los datos normales quedan comprimidos en un rango muy pequeño (~0.0-0.2)\nEscalado robusto: Mantiene mejor las proporciones de la distribución central, minimizando la influencia de valores extremos\n\nConclusión: El escalado robusto es superior cuando hay outliers, preservando la estructura de la mayoría de observaciones.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ingeniería de características: transformaciones de variables e interacciones</span>"
    ]
  },
  {
    "objectID": "tema3.html#catálogo-de-transformaciones-según-el-propósito",
    "href": "tema3.html#catálogo-de-transformaciones-según-el-propósito",
    "title": "4  Ingeniería de características: transformaciones de variables e interacciones",
    "section": "4.3 Catálogo de transformaciones según el propósito",
    "text": "4.3 Catálogo de transformaciones según el propósito\nUna vez realizado el diagnóstico, debemos seleccionar la transformación más apropiada. Cada transformación tiene propósitos específicos y efectos secundarios que debemos considerar. La clave está en entender no solo qué transformación aplicar, sino por qué esa transformación específica resuelve nuestro problema.\n\n4.3.1 Transformaciones para linearizar relaciones\nMuchas relaciones en el mundo real no son lineales, pero pueden linearizarse mediante transformaciones apropiadas. La linearización no solo mejora el ajuste del modelo, sino que también facilita la interpretación y cumple con los supuestos de la regresión lineal.\n\n4.3.1.1 Transformación logarítmica\nLa transformación logarítmica es probablemente la más utilizada en estadística aplicada debido a su versatilidad y propiedades interpretativas únicas.\nCuándo utilizarla:\n\nRelaciones exponenciales: Cuando Y crece exponencialmente respecto a X, \\(Y = ae^{bX}\\) se lineariza como \\(\\log(Y) = \\log(a) + bX\\)\nRelaciones multiplicativas: En modelos donde los efectos se combinan multiplicativamente\nProcesos de crecimiento proporcional: Donde la tasa de cambio es proporcional al nivel actual\nVariables con crecimiento acelerado: Ingresos, precios, donde cada unidad adicional tiene impacto decreciente\n\nPatrones de identificación:\n\nCurva cóncava que se aplana hacia la derecha (rendimientos decrecientes)\nRelación donde duplicar X no duplica Y, sino que el efecto se atenúa\nHeterocedasticidad donde la varianza aumenta con el nivel de Y\n\nAplicaciones matemáticas:\n\n\\(Y' = \\log(Y)\\): Lineariza relaciones exponenciales en Y\n\\(X' = \\log(X)\\): Lineariza relaciones de potencia en X\n\n\\(\\log(Y) = a + b\\log(X)\\): Modelo log-log que produce elasticidades constantes\n\nInterpretación especial: En modelos log-lineales y log-log, los coeficientes tienen interpretaciones económicas directas. En el modelo log-lineal \\(\\log(Y) = a + bX\\), el coeficiente b representa el cambio porcentual en Y por unidad de cambio en X. En el modelo log-log \\(\\log(Y) = a + b\\log(X)\\), b es la elasticidad.\nCasos típicos: Economía (relaciones ingreso-consumo, funciones de producción), biología (crecimiento poblacional, relaciones alométricas), finanzas (rendimientos de inversión).\n\n\n4.3.1.2 Transformación de potencia\nLas transformaciones de potencia son fundamentales cuando trabajamos con leyes físicas o relaciones alométricas donde esperamos relaciones del tipo \\(Y = aX^b\\).\nIdentificación y aplicación:\n\nRelaciones curvilíneas que en escala log-log se vuelven lineales\nMétodo de linearización: tomar logaritmo de ambas variables \\(\\log(Y) = \\log(a) + b\\log(X)\\)\nEl exponente b representa la elasticidad o exponente de escalamiento\n\nEjemplos clásicos: Ley de Stevens en psicofísica, relaciones masa-metabolismo (Ley de Kleiber), economía urbana donde PIB de ciudades escala con población elevada a una potencia.\n\n\n\n4.3.2 Transformaciones para estabilizar la varianza\nLa heterocedasticidad no solo viola supuestos del modelo, sino que también indica que diferentes observaciones tienen diferentes niveles de información. Las transformaciones de varianza estabilizan esta heterogeneidad.\n\n4.3.2.1 Transformación de raíz cuadrada\nLa transformación \\(Y' = \\sqrt{Y}\\) es especialmente útil para datos de conteo donde la varianza es proporcional a la media, característica típica de la distribución de Poisson.\nFundamento teórico: En una distribución de Poisson con parámetro \\(\\lambda\\), tanto la media como la varianza son iguales a \\(\\lambda\\). La transformación de raíz cuadrada estabiliza la varianza porque \\(\\text{Var}(\\sqrt{Y}) \\approx \\frac{1}{4}\\) (constante).\nCuándo aplicarla:\n\nConteos de eventos: número de defectos, llamadas telefónicas, accidentes, ventas por período\nDatos de frecuencia: número de visitas, clicks, transacciones\nVariables discretas con varianza creciente proporcional al nivel\n\nPatrón de diagnóstico: Gráfico de residuos con forma de embudo donde la dispersión aumenta linealmente con la media, y gráfico de varianza vs. media de grupos muestra relación lineal.\nLimitaciones: Solo apropiada para valores no negativos, interpretación complicada (unidades en raíz cuadrada), y para conteos con muchos ceros puede requerir \\(\\sqrt{Y + c}\\).\n\n\n4.3.2.2 Transformación logarítmica para heterocedasticidad multiplicativa\nCuando la varianza es proporcional al cuadrado de la media (heterocedasticidad multiplicativa), la transformación logarítmica es la solución natural.\nCaracterísticas típicas:\n\nVariables monetarias: ingresos, precios, costos donde el error relativo es constante\nPorcentajes de crecimiento: donde el error de medición es proporcional al nivel\nProcesos multiplicativos: donde los errores se acumulan multiplicativamente\n\nEfectos múltiples: La transformación logarítmica frecuentemente resuelve múltiples problemas simultáneamente: lineariza relaciones exponenciales, estabiliza varianza multiplicativa, reduce el impacto de outliers extremos, y aproxima distribuciones asimétricas a la normalidad.\n\n\n\n4.3.3 Transformaciones para normalizar residuos y controlar outliers\nAlgunas transformaciones son especialmente efectivas para aproximar distribuciones a la normalidad y reducir la influencia de valores extremos.\n\n4.3.3.1 Transformación inversa\nLa transformación inversa \\(Y' = \\frac{1}{Y}\\) o \\(X' = \\frac{1}{X}\\) es útil para relaciones hiperbólicas y distribuciones con colas pesadas hacia la derecha.\nIdentificación matemática:\n\nRelación hiperbólica: \\(Y = \\frac{a}{X} + b\\) se lineariza como \\(Y = a \\cdot \\frac{1}{X} + b\\)\nAsíntota horizontal: la relación se aproxima a un valor límite cuando X aumenta\n\nAplicaciones específicas: Tiempo hasta el evento (con asíntota natural), tasas de decaimiento, relaciones dosis-respuesta en farmacología, curvas de demanda con elasticidad precio variable.\nEfecto en outliers: La transformación inversa invierte la escala, comprimiendo fuertemente los valores grandes y expandiendo los pequeños. Útil para reducir influencia de outliers extremos, pero debe usarse con precaución ya que amplifica errores en valores pequeños.\nConsideraciones prácticas: Solo aplicable a valores estrictamente positivos (o negativos), los coeficientes representan el impacto en la escala inversa, y requiere tratamiento especial para valores cercanos a cero.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ingeniería de características: transformaciones de variables e interacciones</span>"
    ]
  },
  {
    "objectID": "tema3.html#transformación-de-box-cox",
    "href": "tema3.html#transformación-de-box-cox",
    "title": "4  Ingeniería de características: transformaciones de variables e interacciones",
    "section": "4.4 Transformación de Box-Cox",
    "text": "4.4 Transformación de Box-Cox\nLa transformación de Box-Cox es un método que optimiza automáticamente el parámetro de transformación para maximizar la normalidad y homocedasticidad de los residuos (Box y Cox 1964). En lugar de elegir manualmente entre transformación logarítmica, raíz cuadrada o inversa, Box-Cox encuentra el valor \\(\\lambda\\) (lambda) que mejor normaliza los datos.\n\n4.4.1 Definición matemática\n\\[Y(\\lambda) = \\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda}, & \\lambda \\neq 0 \\\\\n\\log(Y), & \\lambda = 0\n\\end{cases}\\]\nLos valores especiales de \\(\\lambda\\) corresponden a transformaciones clásicas:\n\n\\(\\lambda\\) = 1: Sin transformación (identidad)\n\\(\\lambda\\) = 0.5: Transformación de raíz cuadrada\n\n\\(\\lambda\\) = 0: Transformación logarítmica\n\\(\\lambda\\) = -1: Transformación inversa\n\n\n\n4.4.2 Propósito y ventajas\nPara qué sirve Box-Cox:\n\nEncuentra automáticamente la transformación óptima sin prueba y error\nMaximiza la verosimilitud del modelo, mejorando simultáneamente normalidad y homocedasticidad\nProporciona un método objetivo para seleccionar la transformación apropiada\nIncluye intervalos de confianza para \\(\\lambda\\), permitiendo evaluar la incertidumbre de la estimación\n\nProcedimiento de aplicación:\n\nSe ajusta el modelo original y se calculan los residuos\nSe evalúa la función de verosimilitud para diferentes valores de \\(\\lambda\\)\nSe selecciona el \\(\\lambda\\) que maximiza la verosimilitud\nSe aplica la transformación con el \\(\\lambda\\) óptimo encontrado\n\n\n\n4.4.3 Limitaciones importantes\nRestricción de dominio: Box-Cox requiere que todos los valores de Y sean estrictamente positivos. Esta es su limitación más importante, ya que muchos conjuntos de datos reales incluyen ceros o valores negativos.\nAplicación tradicional: Se aplica principalmente a la variable dependiente Y, no a las variables predictoras. Aunque técnicamente es posible aplicarla a X, la interpretación se complica considerablemente.\nInterpretación compleja: Cuando \\(\\lambda\\) no corresponde a valores “simples” (como 0, 0.5, o 1), la interpretación de los coeficientes se vuelve difícil. Por ejemplo, si \\(\\lambda\\) = 0.37, ¿cómo interpretar un coeficiente en la escala transformada?\nDependencia del modelo: El \\(\\lambda\\) óptimo depende del modelo específico (predictores incluidos), por lo que cambiar el modelo puede requerir recalcular la transformación.\n\n\n\n\n\n\nExtensión: Transformación de Yeo-Johnson\n\n\n\nLa transformación de Yeo-Johnson (Yeo y Johnson 2000) fue desarrollada específicamente para superar la limitación principal de Box-Cox: la restricción a valores positivos.\nVentajas de Yeo-Johnson sobre Box-Cox:\n\nSin restricción de dominio: Acepta cualquier valor real, incluyendo negativos y cero\nPreserva el signo: Los valores negativos permanecen negativos tras la transformación\nContinuidad: La transformación es continua en Y = 0, evitando discontinuidades\nCasos especiales familiares: Incluye como casos especiales todas las transformaciones comunes\n\nCuándo usar cada una:\n\nBox-Cox: Para datos estrictamente positivos, especialmente cuando se busca comparabilidad con literatura existente\nYeo-Johnson: Cuando los datos incluyen valores negativos o cero, o cuando se necesita mayor flexibilidad\n\nLa elección entre ambas depende fundamentalmente de las características de sus datos y los objetivos del análisis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ingeniería de características: transformaciones de variables e interacciones</span>"
    ]
  },
  {
    "objectID": "tema3.html#tratamiento-de-variables-categóricas",
    "href": "tema3.html#tratamiento-de-variables-categóricas",
    "title": "4  Ingeniería de características: transformaciones de variables e interacciones",
    "section": "4.5 Tratamiento de variables categóricas",
    "text": "4.5 Tratamiento de variables categóricas\nLas variables categóricas son fundamentales en el modelado estadístico, pero requieren una preparación especial antes de ser utilizadas en algoritmos que esperan entradas numéricas (Potdar, Pardawala, y Pai 2017). La elección del método de codificación puede impactar significativamente tanto la interpretabilidad como el rendimiento del modelo.\n\n4.5.1 Principios de codificación categórica\n¿Por qué codificar? La mayoría de algoritmos de machine learning y modelos estadísticos requieren entradas numéricas. Las variables categóricas deben transformarse preservando su información semántica sin introducir supuestos erróneos sobre relaciones entre categorías.\nCriterios de selección del método:\n\nNaturaleza de la variable: ¿Existe orden inherente entre categorías?\nNúmero de categorías: Variables con muchas categorías requieren consideraciones especiales\nInterpretabilidad: ¿Qué método facilita la interpretación de resultados?\nEficiencia computacional: Balance entre precisión y complejidad\n\n\n\n4.5.2 Codificación One-Hot (variables nominales)\nEl One-Hot Encoding transforma variables categóricas nominales en un conjunto de variables binarias (0/1), donde cada nueva variable representa la presencia o ausencia de una categoría específica. Esta técnica es fundamental cuando trabajamos con variables categóricas que no tienen orden inherente, como color, género, región geográfica, o tipo de producto.\nLa transformación convierte una variable categórica con k categorías en k nuevas columnas binarias (o k-1 para evitar colinealidad). Cada fila tendrá exactamente un “1” en la columna correspondiente a su categoría y “0” en todas las demás.\n\n\n\n\n\n\nDummy Variable Trap\n\n\n\nCuando se crean todas las columnas (k para k categorías), una puede expresarse como combinación lineal de las demás, causando colinealidad perfecta en modelos lineales.\nSolución: Eliminar una categoría de referencia (usar k-1 columnas).\n\n\n¿Por qué es necesario? La mayoría de algoritmos de machine learning requieren entradas numéricas y no pueden procesar directamente texto categórico. Más importante aún, el One-Hot Encoding no impone orden artificial entre categorías, tratando cada una como completamente independiente.\nEjemplo práctico: Consideremos una variable “Color” con valores [Rojo, Verde, Azul]. La codificación One-Hot creará tres columnas:\n\n\n\n\n\n\n\n\n\n\nID\nColor\nColor_Rojo\nColor_Verde\nColor_Azul\n\n\n\n\n1\nRojo\n1\n0\n0\n\n\n2\nVerde\n0\n1\n0\n\n\n3\nAzul\n0\n0\n1\n\n\n4\nRojo\n1\n0\n0\n\n\n\nCada observación queda representada por un vector binario que identifica unívocamente su categoría sin asumir relaciones ordinales entre colores.\nInterpretación en regresión: En un modelo de regresión lineal, cada variable binaria creada tendrá su propio coeficiente que representa la diferencia en la variable respuesta entre esa categoría específica y la categoría de referencia (la omitida). Por ejemplo, si omitimos “Azul”, el coeficiente de “Color_Rojo” indicará cuánto mayor (o menor) es el valor esperado de Y cuando el color es Rojo comparado con cuando es Azul.\n\n\n\n\n\n\nImplementación práctica\n\n\n\n\n\n\n# Crear datos de ejemplo\nsuppressPackageStartupMessages(library(caret))\ndatos &lt;- data.frame(\n  ID = 1:5,\n  Color = c(\"Rojo\", \"Verde\", \"Azul\", \"Rojo\", \"Verde\")\n)\n\nMétodo 1: usando model.matrix (incluye todas las categorías)\n\none_hot_completo &lt;- model.matrix(~ Color - 1, data = datos)\none_hot_completo\n\n  ColorAzul ColorRojo ColorVerde\n1         0         1          0\n2         0         0          1\n3         1         0          0\n4         0         1          0\n5         0         0          1\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$Color\n[1] \"contr.treatment\"\n\n\nMétodo 2: eliminando categoría de referencia (evita colinealidad)\n\none_hot_referencia &lt;- model.matrix(~ Color, data = datos)[, -1]\none_hot_referencia\n\n  ColorRojo ColorVerde\n1         1          0\n2         0          1\n3         0          0\n4         1          0\n5         0          1\n\n\nMétodo 3: usando caret::dummyVars con fullRank para evitar colinealidad\n\ndummy_vars &lt;- dummyVars(~ Color, data = datos, fullRank = TRUE)\none_hot_caret &lt;- predict(dummy_vars, newdata = datos)\none_hot_caret\n\n  ColorRojo ColorVerde\n1         1          0\n2         0          1\n3         0          0\n4         1          0\n5         0          1\n\n\n\n\n\n\n\n\n\n\n\nVentajas y desventajas\n\n\n\nVentajas del One-Hot Encoding:\n\nNo asume orden: Trata cada categoría como independiente\nInterpretabilidad: Cada coeficiente representa el efecto específico de esa categoría\nCompatibilidad: Funciona con todos los algoritmos numéricos\n\nDesventajas:\n\nDimensionalidad: Crea k columnas para k categorías (o k-1 con categoría de referencia)\nDispersión: Matrices resultantes son muy dispersas (muchos ceros)\nColinealidad: Riesgo de “dummy variable trap” sin categoría de referencia\n\n\n\n\n\n4.5.3 Codificación Ordinal (variables ordinales)\nLa codificación ordinal transforma variables categóricas ordinales en números enteros que preservan el orden jerárquico natural de las categorías. Esta técnica es fundamental cuando trabajamos with variables categóricas que tienen un orden inherente y significativo, como nivel educativo, satisfacción del cliente, grado de severidad, o calificaciones de crédito.\nLa transformación asigna números enteros consecutivos que reflejan la jerarquía natural de las categorías, preservando tanto la información categórica como el orden relativo entre ellas.\n\n\n\n\n\n\nCuándo usar codificación ordinal\n\n\n\nLa codificación ordinal es apropiada cuando las categorías tienen un orden natural claro y este orden es relevante para el fenómeno que estamos modelando. El modelo puede aprovechar esta información ordinal para capturar tendencias o patrones relacionados con la jerarquía de las categorías.\n\n\n¿Por qué preservar el orden? Los algoritmos de machine learning pueden aprovechar la información ordinal para identificar tendencias y patrones que se perderían con one-hot encoding. Cuando el orden es significativo, la codificación ordinal es más eficiente y puede mejorar el rendimiento del modelo.\nEjemplo práctico: Consideremos una variable “Satisfacción” con valores ordenados [Muy Insatisfecho, Insatisfecho, Neutral, Satisfecho, Muy Satisfecho]. La codificación ordinal asignará:\n\n\n\nID\nSatisfacción\nSatisfacción_Codificada\n\n\n\n\n1\nMuy Insatisfecho\n1\n\n\n2\nInsatisfecho\n2\n\n\n3\nNeutral\n3\n\n\n4\nSatisfecho\n4\n\n\n5\nMuy Satisfecho\n5\n\n\n\nCada observación queda representada por un número entero que preserva el orden jerárquico de las categorías originales.\nInterpretación en regresión: En un modelo de regresión lineal, el coeficiente de la variable ordinal codificada representa el cambio promedio en la variable respuesta por cada incremento de una unidad en el nivel ordinal. Por ejemplo, si el coeficiente es 2.5, esto significa que pasar del nivel 1 al 2, o del 3 al 4, se asocia en promedio con un aumento de 2.5 unidades en la variable respuesta, asumiendo intervalos uniformes entre niveles.\n\n\n\n\n\n\nImplementación práctica\n\n\n\n\n\n\n# Crear datos de ejemplo\ndatos &lt;- data.frame(\n  ID = 1:5,\n  Satisfaccion = c(\"Muy Insatisfecho\", \"Insatisfecho\", \"Neutral\", \"Satisfecho\", \"Muy Satisfecho\")\n)\n\n# Convertir en factor ordenado\ndatos$Satisfaccion_factor &lt;- factor(datos$Satisfaccion, \n                                     levels = c(\"Muy Insatisfecho\", \"Insatisfecho\", \"Neutral\", \"Satisfecho\", \"Muy Satisfecho\"), \n                                     ordered = TRUE)\n\n# Codificación ordinal manual\ndatos$Satisfaccion_codificada &lt;- as.numeric(datos$Satisfaccion_factor)\n\n# Verificar la codificación\ndatos\n\n  ID     Satisfaccion Satisfaccion_factor Satisfaccion_codificada\n1  1 Muy Insatisfecho    Muy Insatisfecho                       1\n2  2     Insatisfecho        Insatisfecho                       2\n3  3          Neutral             Neutral                       3\n4  4       Satisfecho          Satisfecho                       4\n5  5   Muy Satisfecho      Muy Satisfecho                       5\n\n\nEjemplo de uso en regresión:\n\n# Simular variable respuesta correlacionada con el orden\nset.seed(123)\ndatos$Puntuacion &lt;- c(2, 4, 6, 8, 10) + rnorm(5, mean = 0, sd = 0.5)\n\n# Modelo de regresión\nmodelo &lt;- lm(Puntuacion ~ Satisfaccion_codificada, data = datos)\nsummary(modelo)\n\n\nCall:\nlm(formula = Puntuacion ~ Satisfaccion_codificada, data = datos)\n\nResiduals:\n      1       2       3       4       5 \n-0.2090 -0.1279  0.6826 -0.1455 -0.2002 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -0.1552     0.4640  -0.335 0.759968    \nSatisfaccion_codificada   2.0840     0.1399  14.896 0.000657 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4424 on 3 degrees of freedom\nMultiple R-squared:  0.9867,    Adjusted R-squared:  0.9822 \nF-statistic: 221.9 on 1 and 3 DF,  p-value: 0.0006565\n\n\n\n\n\n\n\n\n\n\n\nVentajas y desventajas\n\n\n\nVentajas de la codificación ordinal:\n\nPreserva la jerarquía: Mantiene el orden natural entre categorías\nEficiencia dimensional: Una sola columna independiente del número de categorías\nInterpretabilidad: Coeficientes representan cambios por unidad de nivel ordinal\nEficiencia computacional: Menor uso de memoria y procesamiento\n\nDesventajas:\n\nSupuesto de intervalos uniformes: Asume que las diferencias entre niveles consecutivos son iguales\nRiesgo con variables no-ordinales: Puede imponer orden artificial en variables nominales\nPérdida de flexibilidad: No puede capturar relaciones no-lineales entre niveles ordinales\n\n\n\n\n\n4.5.4 Comparación directa: Ordinal vs One-Hot Encoding\nLa elección entre codificación ordinal y one-hot encoding depende fundamentalmente de la naturaleza de la variable categórica y los objetivos del análisis. Una decisión incorrecta puede llevar a interpretaciones erróneas y modelos subóptimos.\n\n\n\n\n\n\n\n\nCaracterística\nCodificación Ordinal\nOne-Hot Encoding\n\n\n\n\nPreserva el orden\nSí, refleja la jerarquía entre categorías\nNo, trata cada categoría como independiente\n\n\nDimensionalidad\nUna sola columna\nk columnas (o k-1)\n\n\nAdecuado para\nVariables con orden natural (educación, satisfacción)\nVariables nominales (color, género, región)\n\n\nInterpretación\nCambio por unidad de nivel\nDiferencia vs. categoría de referencia\n\n\nEficiencia computacional\nAlta (menos parámetros)\nMenor (más parámetros)\n\n\nRiesgo principal\nOrden artificial en variables nominales\nDimensionalidad excesiva",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ingeniería de características: transformaciones de variables e interacciones</span>"
    ]
  },
  {
    "objectID": "tema3.html#interacciones-entre-variables",
    "href": "tema3.html#interacciones-entre-variables",
    "title": "4  Ingeniería de características: transformaciones de variables e interacciones",
    "section": "4.6 Interacciones entre variables",
    "text": "4.6 Interacciones entre variables\nLas interacciones entre variables representan uno de los conceptos más poderosos y subestimados en el modelado estadístico. Mientras que los efectos principales capturan el impacto promedio de cada variable por separado, las interacciones revelan cómo el efecto de una variable cambia según el nivel de otra variable. Este fenómeno es omnipresente en el mundo real: el efecto del precio en las ventas depende del nivel de publicidad, el impacto de la experiencia en el salario varía según la educación, o la efectividad de un tratamiento médico puede diferir entre grupos demográficos.\n\n\n\n\n\n\nPrincipios para feature engineering efectivo\n\n\n\n\nJustificación teórica: Cada nueva variable debe tener sentido conceptual en el dominio\nValidación rigurosa: Evaluar el poder predictivo real en datos no vistos\nSimplicidad primero: Preferir transformaciones simples e interpretables\nDocumentación exhaustiva: Registrar el proceso de creación y la lógica detrás de cada feature\nMonitoreo continuo: Verificar que las relaciones se mantienen en producción\n\n\n\nIgnorar las interacciones relevantes puede llevar a conclusiones erróneas y pérdida significativa de poder predictivo. Por otro lado, incluir interacciones irrelevantes incrementa la complejidad del modelo sin beneficios, violando el principio de parsimonia. La clave está en identificar, interpretar y validar interacciones de manera sistemática y teoricamente fundamentada.\n\n4.6.1 Interacciones entre variables continuas\nEl caso más directo es la interacción entre dos variables continuas:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\varepsilon\\]\nInterpretación del coeficiente de interacción (\\(\\beta_3\\)):\n\nSi \\(\\beta_3 &gt; 0\\): El efecto de \\(X_1\\) se amplifica cuando \\(X_2\\) aumenta\nSi $_3 &lt; 0: El efecto de \\(X_1\\) se atenúa cuando \\(X_2\\) aumenta\nSi \\(\\beta_3 = 0\\): No hay interacción (efectos aditivos)\n\n\n\n\n\n\n\nEjemplo: Interacción precio-publicidad en ventas\n\n\n\n\n\n\n# Simulación: efecto de precio y publicidad en ventas\n# con interacción (mayor publicidad reduce sensibilidad al precio)\nset.seed(789)\nn &lt;- 200\nprecio &lt;- runif(n, 50, 150)  # precio en euros\npublicidad &lt;- runif(n, 0, 10)  # gasto en publicidad (miles €)\n\n# Efecto principal negativo del precio, positivo de publicidad\n# Interacción: mayor publicidad reduce sensibilidad negativa al precio\nventas &lt;- 1000 - 5*precio + 50*publicidad + 0.8*precio*publicidad/10 + \n          rnorm(n, 0, 50)\n\ndatos_inter &lt;- data.frame(precio, publicidad, ventas)\n\n# Modelo con interacción\nmodelo_interaccion &lt;- lm(ventas ~ precio * publicidad, data = datos_inter)\nsummary(modelo_interaccion)\n\n\nCall:\nlm(formula = ventas ~ precio * publicidad, data = datos_inter)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-158.256  -35.577    0.296   37.460  118.605 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       968.37048   27.53011  35.175   &lt;2e-16 ***\nprecio             -4.74324    0.26860 -17.659   &lt;2e-16 ***\npublicidad         55.77675    4.48175  12.445   &lt;2e-16 ***\nprecio:publicidad   0.03470    0.04432   0.783    0.435    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 50.87 on 196 degrees of freedom\nMultiple R-squared:  0.9518,    Adjusted R-squared:  0.9511 \nF-statistic:  1291 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretación numérica de los coeficientes:\n\n# Extraer coeficientes y p-valores para interpretación\ncoef_int &lt;- coef(modelo_interaccion)\nsummary_model &lt;- summary(modelo_interaccion)\np_valores &lt;- summary_model$coefficients[, \"Pr(&gt;|t|)\"]\n\n# Crear tabla de interpretación de efectos\ntabla_efectos &lt;- data.frame(\n  Nivel_Publicidad = c(\"0 (sin publicidad)\", \"5 (media)\", \"10 (alta)\"),\n  Efecto_Precio = c(\n    round(coef_int[2], 2),\n    round(coef_int[2] + 5*coef_int[4], 2),\n    round(coef_int[2] + 10*coef_int[4], 2)\n  )\n)\n\nkable(tabla_efectos, \n      caption = \"Efecto del precio según el nivel de publicidad (hipotético)\",\n      col.names = c(\"Nivel de Publicidad\", \"Efecto del Precio (€/unidad)\"))\n\n\nEfecto del precio según el nivel de publicidad (hipotético)\n\n\nNivel de Publicidad\nEfecto del Precio (€/unidad)\n\n\n\n\n0 (sin publicidad)\n-4.74\n\n\n5 (media)\n-4.57\n\n\n10 (alta)\n-4.40\n\n\n\n\n\nEvidencia estadística: La interacción tiene un coeficiente de 0.035 con un p-valor de 0.435. Dado que p &gt; 0.05, no hay evidencia estadística de interacción entre precio y publicidad.\nInterpretación del gráfico: El primer gráfico (scatter plot con líneas de tendencia por grupos de publicidad) muestra líneas prácticamente paralelas, confirmando la ausencia de interacción. Aunque visualmente las pendientes parecen ligeramente diferentes, esta variación está dentro del rango esperado por el ruido aleatorio.\nImplicaciones prácticas:\n\nEl efecto del precio sobre las ventas es constante (-€5 por unidad) independientemente del nivel de publicidad\nLos efectos son aditivos: cada €1000 adicional en publicidad aumenta las ventas base en ~50 unidades, sin modificar la sensibilidad al precio\nModelo recomendado: ventas ~ precio + publicidad (sin término de interacción)\n\nLección metodológica: Este caso demuestra la importancia de confiar en la evidencia estadística formal sobre las impresiones visuales cuando hay variabilidad muestral significativa.\nVisualización para verificar ausencia de interacción:\n\n# Visualización de la interacción\nlibrary(ggplot2)\n# Crear grupos de publicidad para visualizar\ndatos_inter$pub_grupo &lt;- cut(datos_inter$publicidad, \n                            breaks = 3, \n                            labels = c(\"Baja\", \"Media\", \"Alta\"))\n\nggplot(datos_inter, aes(x = precio, y = ventas, color = pub_grupo)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, linewidth = 1.2) +\n  labs(title = \"Efectos Aditivos: Precio y Publicidad\",\n       subtitle = \"Pendientes similares confirman ausencia de interacción\",\n       x = \"Precio (€)\", y = \"Ventas\", color = \"Publicidad\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretación crítica: Visual vs Estadística\nObservando el gráfico, las líneas parecen tener pendientes diferentes, lo que visualmente sugeriría la presencia de interacción. Sin embargo, el análisis estadístico formal nos indica que esta diferencia no es estadísticamente significativa (p = 0.435).\n¿Por qué esta aparente contradicción?\n\nVariabilidad aleatoria: Las diferencias observadas pueden deberse al ruido aleatorio en los datos\nTamaño de muestra: Puede no ser suficiente para detectar una interacción débil si realmente existe\nPoder estadístico: El test puede no tener suficiente poder para detectar efectos pequeños\nAgrupación artificial: Los grupos de publicidad se crearon artificialmente para visualización, no reflejan la variable continua real\n\nDecisión metodológica correcta:\n\nConfiar en la estadística formal: El p-valor &gt; 0.05 indica no significancia\nModelo parsimonioso: Eliminar el término de interacción no significativo\nInterpretación conservadora: Los efectos son aditivos hasta que se demuestre lo contrario\n\nLección crucial:\nEste ejemplo demuestra por qué la inspección visual nunca debe ser el único criterio para decidir sobre la inclusión de términos de interacción. La estadística inferencial formal debe prevalecer sobre las impresiones visuales, especialmente cuando hay incertidumbre debido a la variabilidad muestral.\nModelo final recomendado: ventas ~ precio + publicidad (sin interacción)\n\n\n\n\n\n4.6.2 Interacciones entre variables categóricas\nCuando ambas variables son categóricas, las interacciones representan efectos específicos de combinaciones de categorías que no pueden explicarse por los efectos principales por separado.\nPara dos variables categóricas A (con niveles i) y B (con niveles j), el modelo incluye:\n\\[Y = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon\\]\nDonde \\((\\alpha\\beta)_{ij}\\) representa la interacción específica entre el nivel i de A y el nivel j de B.\nLa interacción \\((\\alpha\\beta)_{ij}\\) indica cuánto la combinación específica (i,j) se desvía del efecto que esperaríamos si solo sumáramos los efectos principales \\(\\alpha_i + \\beta_j\\).\n\n\n\n\n\n\nEjemplo: Interacción género-departamento en salarios\n\n\n\n\n\n\n# Simulación: salarios por género y departamento con interacción\n# (brecha salarial varía según departamento)\nset.seed(456)\nn &lt;- 300\n\n# Variables categóricas\ngenero &lt;- sample(c(\"Masculino\", \"Femenino\"), n, replace = TRUE)\ndepartamento &lt;- sample(c(\"Ventas\", \"IT\", \"RRHH\"), n, replace = TRUE)\n\n# Efectos principales y de interacción simulados\nefecto_base &lt;- 40000  # salario base\nefecto_masculino &lt;- ifelse(genero == \"Masculino\", 2000, 0)\nefecto_it &lt;- ifelse(departamento == \"IT\", 8000, 0)\nefecto_rrhh &lt;- ifelse(departamento == \"RRHH\", 3000, 0)\n\n# Interacción: brecha de género mayor en IT\ninteraccion &lt;- ifelse(genero == \"Masculino\" & departamento == \"IT\", 4000, 0)\n\nsalario &lt;- efecto_base + efecto_masculino + efecto_it + efecto_rrhh + \n           interaccion + rnorm(n, 0, 3000)\n\ndatos_cat &lt;- data.frame(genero, departamento, salario)\n\n# Modelo con interacción\nmodelo_cat &lt;- lm(salario ~ genero * departamento, data = datos_cat)\nsummary(modelo_cat)\n\n\nCall:\nlm(formula = salario ~ genero * departamento, data = datos_cat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8037.9 -1930.7    48.4  1968.1  7486.9 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         47627.4      461.2 103.270  &lt; 2e-16 ***\ngeneroMasculino                      6166.3      593.4  10.391  &lt; 2e-16 ***\ndepartamentoRRHH                    -4433.3      633.9  -6.994 1.80e-11 ***\ndepartamentoVentas                  -7744.7      597.4 -12.964  &lt; 2e-16 ***\ngeneroMasculino:departamentoRRHH    -4007.2      857.1  -4.675 4.48e-06 ***\ngeneroMasculino:departamentoVentas  -3793.8      814.4  -4.659 4.83e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2917 on 294 degrees of freedom\nMultiple R-squared:  0.7372,    Adjusted R-squared:  0.7327 \nF-statistic: 164.9 on 5 and 294 DF,  p-value: &lt; 2.2e-16\n\n# Medias por grupo para interpretar la interacción\nmedias_grupo &lt;- aggregate(salario ~ genero + departamento, data = datos_cat, FUN = mean)\nmedias_grupo &lt;- medias_grupo[order(medias_grupo$departamento, medias_grupo$genero), ]\nkable(medias_grupo, caption = \"Salario promedio por género y departamento\")\n\n\nSalario promedio por género y departamento\n\n\ngenero\ndepartamento\nsalario\n\n\n\n\nFemenino\nIT\n47627.44\n\n\nMasculino\nIT\n53793.71\n\n\nFemenino\nRRHH\n43194.15\n\n\nMasculino\nRRHH\n45353.23\n\n\nFemenino\nVentas\n39882.76\n\n\nMasculino\nVentas\n42255.23\n\n\n\n\n# Visualización de la interacción\nlibrary(ggplot2)\nggplot(datos_cat, aes(x = departamento, y = salario, fill = genero)) +\n  geom_boxplot(position = \"dodge\") +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, \n               position = position_dodge(0.75)) +\n  labs(title = \"Interacción Género-Departamento en Salarios\",\n       x = \"Departamento\", y = \"Salario (€)\", fill = \"Género\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEvidencia visual clara: El segundo gráfico (boxplots por género y departamento) revela patrones de interacción marcados que se manifiestan de forma diferente en cada departamento.\nPatrones específicos por departamento:\n\nIT: La brecha de género es máxima (~€8,000). Los hombres tienen salarios significativamente superiores y mayor variabilidad salarial\nRRHH: Brecha moderada (~€3,000) con distribuciones más similares entre géneros\nVentas: Menor brecha de género (~€1,500), con salarios más homogéneos entre grupos\n\nInterpretación de la interacción: Las líneas no paralelas en el patrón de medias confirman que el efecto del género sobre el salario varía significativamente según el departamento. Esto sugiere:\n\nDiferencias en culturas departamentales respecto a equidad salarial\nEstructuras de compensación variables entre departamentos\nPosibles diferencias en poder de negociación o demanda de talento\n\nImplicaciones organizacionales: La interacción indica que las políticas salariales no son uniformes y que intervenciones de equidad deberían ser diferenciadas por departamento.\nGráfico de interacción clásico:\n\n# Calcular medias y errores estándar por grupo para el gráfico\nsuppressPackageStartupMessages(library(dplyr))\nmedias_se &lt;- datos_cat %&gt;%\n  group_by(genero, departamento) %&gt;%\n  summarise(\n    media = mean(salario),\n    se = sd(salario) / sqrt(n()),\n    .groups = \"drop\"\n  )\n\n# Mostrar los datos calculados\nkable(medias_se, caption = \"Medias y errores estándar por grupo\")\n\n\nMedias y errores estándar por grupo\n\n\ngenero\ndepartamento\nmedia\nse\n\n\n\n\nFemenino\nIT\n47627.44\n463.4703\n\n\nFemenino\nRRHH\n43194.15\n443.9292\n\n\nFemenino\nVentas\n39882.76\n378.5932\n\n\nMasculino\nIT\n53793.71\n371.1923\n\n\nMasculino\nRRHH\n45353.23\n425.9146\n\n\nMasculino\nVentas\n42255.23\n414.4745\n\n\n\n\n\n\n# Gráfico de interacción estilo clásico\nggplot(medias_se, aes(x = departamento, y = media, color = genero, group = genero)) +\n  geom_point(size = 3) +\n  geom_line(linewidth = 1) +\n  geom_errorbar(aes(ymin = media - se, ymax = media + se), width = 0.1) +\n  labs(title = \"Gráfico de Interacción: Género × Departamento\",\n       subtitle = \"Las líneas no paralelas indican presencia de interacción\",\n       x = \"Departamento\", y = \"Salario promedio (€)\", color = \"Género\") +\n  theme_minimal() +\n  theme(plot.subtitle = element_text(size = 10, color = \"gray50\"))\n\n\n\n\n\n\n\n\nInterpretación del gráfico: Las líneas no son paralelas, confirmando la presencia de interacción significativa. La brecha salarial de género varía considerablemente: mayor en IT, moderada en RRHH, y menor en Ventas.\n\n\n\n\n\n4.6.3 Interacciones mixtas (continua × categórica)\nLas interacciones mixtas son especialmente útiles para modelar cómo el efecto de una variable continua varía entre grupos categóricos. Esto es fundamental cuando sospechamos que la relación funcional cambia según el contexto definido por la variable categórica.\nFormulación matemática: \\[Y = \\beta_0 + \\beta_1 X + \\beta_2 D + \\beta_3 X \\cdot D + \\varepsilon\\]\nDonde D es una variable dummy (0/1) que representa la variable categórica.\nInterpretación geométrica: La interacción permite que cada grupo categórico tenga:\n\nIntercepto diferente: \\(\\beta_0\\) (grupo de referencia) vs. \\(\\beta_0 + \\beta_2\\) (otro grupo)\n\nPendiente diferente: \\(\\beta_1\\) (grupo de referencia) vs. \\(\\beta_1 + \\beta_3\\) (otro grupo)\n\n\n\n\n\n\n\nEjemplo: Interacción experiencia-género en salarios\n\n\n\n\n\n\n# Simulación: efecto de experiencia en salario varía según género\nset.seed(789)\nn &lt;- 250\n\nexperiencia &lt;- runif(n, 0, 20)  # años de experiencia\ngenero &lt;- sample(c(\"Femenino\", \"Masculino\"), n, replace = TRUE)\n\n# Efecto diferencial: pendiente de experiencia menor para mujeres\nsalario_base &lt;- 35000\nefecto_experiencia_hombres &lt;- 2000  # €2000 por año para hombres\nefecto_experiencia_mujeres &lt;- 1200  # €1200 por año para mujeres (brecha creciente)\nefecto_genero_base &lt;- ifelse(genero == \"Masculino\", 3000, 0)\n\n# Crear variable dummy para interacción\ndummy_masculino &lt;- ifelse(genero == \"Masculino\", 1, 0)\n\n# Salario con interacción\nsalario &lt;- salario_base + \n           efecto_experiencia_mujeres * experiencia +  # pendiente base (mujeres)\n           efecto_genero_base * dummy_masculino +      # diferencia intercepto\n           (efecto_experiencia_hombres - efecto_experiencia_mujeres) * experiencia * dummy_masculino +  # interacción\n           rnorm(n, 0, 4000)\n\ndatos_mixta &lt;- data.frame(experiencia, genero, salario, dummy_masculino)\n\n# Modelo con interacción\nmodelo_mixta &lt;- lm(salario ~ experiencia * genero, data = datos_mixta)\nsummary(modelo_mixta)\n\n\nCall:\nlm(formula = salario ~ experiencia * genero, data = datos_mixta)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12340.2  -2742.7    162.8   2586.6   9293.8 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 35650.27     765.04  46.599   &lt;2e-16 ***\nexperiencia                  1123.03      69.74  16.103   &lt;2e-16 ***\ngeneroMasculino              2613.33    1038.08   2.517   0.0125 *  \nexperiencia:generoMasculino   905.46      92.44   9.795   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3995 on 246 degrees of freedom\nMultiple R-squared:  0.8889,    Adjusted R-squared:  0.8876 \nF-statistic: 656.2 on 3 and 246 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretación detallada de los coeficientes:\n\n# Extraer coeficientes para interpretación\ncoef_mixta &lt;- coef(modelo_mixta)\n\n# Crear tabla de interpretación por género\ntabla_genero &lt;- data.frame(\n  Parámetro = c(\"Intercepto (salario inicial)\", \"Pendiente (€ por año experiencia)\"),\n  Mujeres = c(\n    paste0(\"€\", format(round(coef_mixta[1], 0), big.mark = \",\")),\n    paste0(\"€\", round(coef_mixta[2], 0))\n  ),\n  Hombres = c(\n    paste0(\"€\", format(round(coef_mixta[1] + coef_mixta[3], 0), big.mark = \",\")),\n    paste0(\"€\", round(coef_mixta[2] + coef_mixta[4], 0))\n  ),\n  Diferencia = c(\n    paste0(\"€\", format(round(coef_mixta[3], 0), big.mark = \",\")),\n    paste0(\"€\", round(coef_mixta[4], 0), \" adicionales\")\n  )\n)\n\nkable(tabla_genero, \n      caption = \"Comparación de parámetros del modelo por género\")\n\n\nComparación de parámetros del modelo por género\n\n\nParámetro\nMujeres\nHombres\nDiferencia\n\n\n\n\nIntercepto (salario inicial)\n€35,650\n€38,264\n€2,613\n\n\nPendiente (€ por año experiencia)\n€1123\n€2028\n€905 adicionales\n\n\n\n\n\nImplicaciones de la interacción:\nEl coeficiente de interacción (905) indica que cada año adicional de experiencia aumenta el salario masculino en €905 más que el salario femenino. Esto crea una brecha creciente: inicialmente la diferencia es de €2,613, pero después de 20 años de experiencia, la brecha total alcanza €20,722.\nVisualización de la divergencia salarial:\n\n# Visualización de líneas de regresión por grupo\nggplot(datos_mixta, aes(x = experiencia, y = salario, color = genero)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = TRUE, linewidth = 1.2) +\n  labs(title = \"Interacción Experiencia-Género: Brechas Crecientes\",\n       subtitle = \"La brecha salarial se amplía con la experiencia\",\n       x = \"Años de experiencia\", y = \"Salario (€)\", color = \"Género\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Femenino\" = \"#E69F00\", \"Masculino\" = \"#0072B2\")) +\n  scale_y_continuous(labels = scales::comma_format(suffix = \"€\"))\n\n\n\n\n\n\n\n\nEvidencia de brechas crecientes: El tercer gráfico (scatter plot con líneas de regresión) demuestra claramente el patrón de interacción experiencia-género mediante líneas divergentes con pendientes notablemente diferentes.\nInterpretación cuantitativa de la divergencia:\n\nPunto de inicio (0 años): Brecha inicial de €2,613 a favor de los hombres\nPendientes diferenciadas: Los hombres ganan €2028 adicionales por año vs. €1123 para las mujeres\nBrecha acumulativa: Cada año adicional de experiencia amplía la brecha en €905 adicionales\n\nImplicaciones del patrón de interacción: La divergencia progresiva visible en las líneas revela que:\n\nEl retorno a la experiencia es sistemáticamente mayor para hombres que para mujeres\nA los 20 años de experiencia, la brecha total alcanza €20,722 (inicial + acumulativa)\nEste patrón sugiere barreras estructurales que impiden que las mujeres capitalicen plenamente su experiencia\n\nSignificancia social: Esta interacción documenta un fenómeno preocupante donde la inequidad salarial se agrava con el tiempo, indicando que las brechas de género no son meramente diferencias de entrada sino desventajas acumulativas a lo largo de la carrera profesional.\n\n\n\n\n\n4.6.4 Identificación y detección de interacciones\nLa detección sistemática de interacciones requiere combinar justificación teórica, exploración visual y validación estadística. No debemos buscar interacciones aleatoriamente, sino guiados por el conocimiento del dominio y patrones observables en los datos.\nLa justificación teórica previa es fundamental: la teoría del dominio debe sugerir dónde pueden existir interacciones. Por ejemplo, en economía esperamos efectos precio-publicidad o educación-experiencia; en medicina son comunes las interacciones dosis-edad o tratamiento-comorbilidad; en marketing encontramos interacciones producto-canal o temporada-promoción.\nLa exploración visual sistemática complementa la teoría con evidencia empírica. Para variables continuas utilizamos gráficos de dispersión coloreados por grupos categóricos; para variables categóricas empleamos gráficos de interacción (interaction plots); para interacciones mixtas analizamos líneas de regresión por grupo.\nLos tests estadísticos formales proporcionan validación objetiva: el test F para interacciones compara modelos con y sin términos de interacción, el test de significancia individual evalúa coeficientes específicos mediante t-test, y los criterios de información (AIC/BIC) guían la selección entre modelos alternativos.\n\n\n\n\n\n\nEstrategia de modelado jerárquico\n\n\n\nPrincipio de jerarquía: Si incluimos una interacción A×B, siempre debemos incluir los efectos principales A y B, incluso si no son significativos individualmente. Esto preserva la interpretabilidad y evita sesgos en los coeficientes de interacción.\nProceso de construcción del modelo:\n\nModelo base: Solo efectos principales\nModelo con interacciones: Agregar términos de interacción teoricamente justificados\nComparación: Test F para evaluar mejora significativa\nSelección: Usar criterios estadísticos y de parsimonia\nValidación: Verificar supuestos y estabilidad en datos de prueba\n\n\n\n\n\n4.6.5 Consideraciones prácticas y limitaciones\nLas interacciones incrementan exponencialmente la complejidad interpretativa del modelo. Un modelo con k efectos principales puede tener hasta \\(k(k-1)/2\\) interacciones de segundo orden, y el número crece exponencialmente con interacciones de orden superior. Esta explosión combinatorial hace que incluso modelos aparentemente simples se vuelvan rápidamente inmanejables desde el punto de vista interpretativo. Como reglas prácticas para la complejidad, recomendamos limitar a máximo 2-3 interacciones de segundo orden en modelos explicativos, evitar interacciones de tercer orden salvo justificación teórica muy sólida, y priorizar interacciones con efectos grandes sobre mera significancia estadística.\n\n\n\n\n\n\nAdvertencia sobre interpretación\n\n\n\nCuidado con la interpretación automática. Las interacciones en escalas transformadas tienen significados diferentes que en escalas originales. Siempre verificar la interpretación en el contexto de la transformación aplicada y considerar la retransformación para comunicación con audiencias no técnicas.\n\n\nUn problema adicional surge cuando las interacciones crean multicolinealidad severa, especialmente cuando las variables principales están correlacionadas, se incluyen múltiples interacciones con variables comunes, o se usan variables categóricas con muchos niveles. Esta multicolinealidad puede hacer que los coeficientes individuales sean inestables y difíciles de interpretar, incluso cuando el modelo en conjunto funcione bien predictivamente. Las estrategias de mitigación incluyen el centrado de variables continuas para reducir la correlación entre X y X×Z, la selección cuidadosa de interacciones sin incluir todas las combinaciones posibles, y el uso de métodos de regularización como Ridge o Lasso cuando hay múltiples interacciones.\nLa situación se complica aún más cuando las variables están transformadas (logarítmica, Box-Cox). En estos casos, la interpretación de las interacciones adquiere significados completamente diferentes que en escalas originales. Por ejemplo, en un modelo como \\(\\log(Y) = \\beta_0 + \\beta_1 \\log(X_1) + \\beta_2 X_2 + \\beta_3 \\log(X_1) \\cdot X_2 + \\varepsilon\\), el coeficiente \\(\\beta_3\\) representa cómo cambia la elasticidad de Y respecto a X₁ cuando X₂ aumenta en una unidad, lo que requiere una interpretación mucho más sofisticada que una interacción en escalas lineales.\n\n\n\n\n\n\nPrincipios para el uso de interacciones\n\n\n\n\nJustificación teórica primero: No buscar interacciones sin base conceptual\nPrincipio de jerarquía: Mantener efectos principales cuando se incluyen interacciones\nParsimonia: Preferir modelos simples que expliquen bien sobre modelos complejos\nValidación robusta: Verificar estabilidad en múltiples contextos\nInterpretación cuidadosa: Asegurar comprensión completa antes de conclusiones\nComunicación efectiva: Usar visualizaciones para explicar efectos complejos\n\n\n\nLas interacciones son herramientas poderosas que pueden revelar patrones importantes ocultos en los efectos principales. Sin embargo, su uso requiere disciplina metodológica, justificación teórica sólida, y validación rigurosa para evitar conclusiones espurias y modelos sobreajustados.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ingeniería de características: transformaciones de variables e interacciones</span>"
    ]
  },
  {
    "objectID": "tema3.html#ingeniería-de-características-avanzada-combinaciones-ratios-y-transformaciones",
    "href": "tema3.html#ingeniería-de-características-avanzada-combinaciones-ratios-y-transformaciones",
    "title": "4  Ingeniería de características: transformaciones de variables e interacciones",
    "section": "4.7 Ingeniería de características avanzada: combinaciones, ratios y transformaciones",
    "text": "4.7 Ingeniería de características avanzada: combinaciones, ratios y transformaciones\nMás allá de las transformaciones individuales e interacciones, la ingeniería de características avanzada implica crear nuevas variables predictivas mediante combinaciones matemáticas, ratios y transformaciones compuestas que capturen relaciones complejas no evidentes en las variables originales (Kuhn y Johnson 2019). Esta aproximación es fundamental cuando las variables individuales contienen información parcial que, al combinarse, revelan patrones predictivos más potentes.\n\n4.7.1 Combinaciones lineales y no lineales\nLas combinaciones lineales crean nuevas variables mediante sumas ponderadas de variables existentes, útiles especialmente cuando trabajamos con variables que miden aspectos relacionados del mismo fenómeno pero con diferentes escalas o unidades.\nEjemplos de combinaciones lineales efectivas:\n\nÍndices compuestos: Combinan múltiples indicadores en un score único que captura un constructo multidimensional. Por ejemplo, un índice de riesgo cardiovascular podría definirse como índice_salud = 0.4×presión_arterial_normalizada + 0.3×colesterol_normalizada + 0.3×IMC_normalizado. Los pesos (0.4, 0.3, 0.3) reflejan la importancia relativa establecida por evidencia médica, creando una métrica integrada que es más informativa que cualquier indicador individual. Este tipo de índices son especialmente valiosos en dominios donde múltiples factores contribuyen conjuntamente al outcome de interés.\nScores balanceados: Representan equilibrios o trade-offs entre dimensiones competitivas. Un ejemplo típico es balance_trabajo_vida = horas_trabajo / (tiempo_personal + tiempo_familia + tiempo_descanso). Esta métrica captura no solo la intensidad laboral, sino también su contexto relativo dentro del estilo de vida completo. Valores altos indican desbalance hacia el trabajo, mientras que valores cercanos a 1.0 sugieren equilibrio saludable. Los scores balanceados son fundamentales para capturar dinámicas de compensación que no son evidentes en variables absolutas.\nFactores sintéticos: Cuando múltiples variables correlacionadas miden aspectos del mismo constructo subyacente, pueden condensarse en un factor común que preserve la información esencial eliminando redundancia. Por ejemplo, si tenemos variables ingresos, educación, y prestigio_ocupacional (todas correlacionadas), podemos crear un factor estatus_socioeconomico que capture la varianza común. Esto es especialmente útil cuando la colinealidad entre predictores compromete la estabilidad del modelo, pero cada variable aporta información valiosa.\n\nLas combinaciones no lineales van más allá de las sumas ponderadas para capturar interacciones multiplicativas, sinergias y compensaciones entre variables mediante productos, cocientes, potencias y funciones más complejas:\n\nProductos de eficiencia: Capturan sinergias multiplicativas donde el rendimiento depende de la combinación simultánea de múltiples factores. Por ejemplo, rendimiento_efectivo = capacidad_instalada × utilización_porcentual × factor_calidad. Esta métrica reconoce que el rendimiento real no es aditivo: tener alta capacidad pero baja utilización, o alta utilización con problemas de calidad, resulta en rendimiento subóptimo. Los productos de eficiencia son esenciales en contextos operacionales donde el desempeño emerge de la coordinación entre recursos.\nRatios de rendimiento ajustado por riesgo: Normalizan beneficios por su costo o riesgo asociado, creando métricas comparables entre contextos diferentes. Un ejemplo financiero sería eficiencia_ajustada = (rendimiento_esperado - tasa_libre_riesgo) / (volatilidad + costos_transaccion). Esta formulación reconoce que los rendimientos absolutos son engañosos sin considerar el riesgo asumido y los costos incurridos. Los ratios ajustados son cruciales para decisiones de optimización donde debemos comparar alternativas con perfiles de riesgo-retorno heterogéneos.\nFunciones de utilidad: Capturan percepciones subjetivas o valores no lineales mediante transformaciones que reflejan preferencias reales. Por ejemplo, valor_percibido = √(calidad_producto) × precio⁻⁰·⁵ reconoce que la utilidad del consumidor tiene rendimientos decrecientes tanto en calidad como en ahorro de precio. La raíz cuadrada de la calidad refleja que mejoras incrementales tienen menor impacto en niveles altos, mientras que el exponente negativo del precio captura la sensibilidad decreciente a cambios de precio en productos caros.\n\n\n\n4.7.2 Ratios y proporciones como features\nLos ratios son especialmente poderosos porque normalizan automáticamente las diferencias de escala y pueden revelar relaciones proporcionales fundamentales que permanecen ocultas en variables absolutas. A diferencia de las medidas absolutas, los ratios capturan relaciones estructurales que son invariantes bajo cambios de escala y contexto, lo que los convierte en herramientas fundamentales para crear features robustos y comparables.\nLa potencia de los ratios radica en su capacidad para transformar información absoluta en información relativa. Por ejemplo, una empresa con €1M en ventas y €100K en marketing tiene un ratio ventas/marketing de 10, igual que una empresa con €10M en ventas y €1M en marketing. Esta normalización automática permite comparaciones directas y elimina sesgos de tamaño que podrían distorsionar el análisis.\nCategorización detallada de ratios efectivos:\n\nRatios de eficiencia: Miden qué tan efectivamente se convierten los inputs en outputs, revelando productividad y optimización operacional. Ejemplos fundamentales incluyen:\n\nROI_marketing = (ventas_generadas - gasto_marketing) / gasto_marketing: Captura el retorno neto por euro invertido\neficiencia_produccion = unidades_producidas / (horas_trabajo + costo_materiales): Normaliza productividad por recursos consumidos\nconversion_rate = ventas_completadas / visitantes_web: Revela la efectividad del funnel de conversión\n\nEstos ratios son especialmente valiosos porque eliminan el efecto escala y permiten comparar unidades de diferentes tamaños en términos de eficiencia pura.\nRatios de riesgo ajustado: Normalizan retornos o beneficios por la incertidumbre o costo asociado, proporcionando métricas de valor ajustado por riesgo:\n\nsharpe_ratio = (rendimiento_promedio - tasa_libre_riesgo) / volatilidad: Mide retorno por unidad de riesgo asumido\nstability_score = beneficio_promedio / desviacion_estandar_beneficios: Indica consistencia en el desempeño\nrisk_adjusted_growth = crecimiento_promedio / max_drawdown: Captura crecimiento sostenible\n\nEstos ratios son cruciales en análisis financiero y gestión de riesgos, donde los valores absolutos pueden ser engañosos sin considerar la variabilidad subyacente.\nRatios temporales: Capturan dinámicas y tendencias mediante comparaciones entre períodos, revelando momentum y patrones estacionales:\n\nmomentum_growth = crecimiento_último_trimestre / crecimiento_promedio_histórico: Identifica aceleración o desaceleración\nestacionalidad = ventas_período_actual / media_móvil_12_meses: Captura variaciones cíclicas\ntrend_strength = (valor_actual - valor_hace_12_meses) / volatilidad_histórica: Mide significancia de cambios\n\nEstos ratios son especialmente útiles en análisis de series temporales donde necesitamos distinguir entre variación normal y cambios estructurales significativos.\nRatios de composición: Revelan la estructura interna de agregados mediante proporciones parte-todo, fundamentales para análisis de portafolios y segmentación:\n\nconcentracion_cliente = ventas_top3_clientes / ventas_totales: Mide dependencia y riesgo de concentración\ndiversificacion_producto = 1 - suma(proportion_i²): Índice de Herfindahl para medir dispersión\nmarket_share = ventas_empresa / ventas_mercado_total: Posición relativa competitiva\n\nLos ratios de composición son esenciales para gestión de riesgos y análisis estratégico, revelando vulnerabilidades y fortalezas estructurales.\n\nVentajas metodológicas profundizadas:\n\nNormalización automática: Los ratios eliminan efectos de escala absoluta, haciendo comparables entidades de diferentes tamaños. Una startup con €10K en ventas y €2K en marketing tiene el mismo ratio ventas/marketing (5.0) que una multinacional con €100M y €20M respectivamente, permitiendo benchmarking directo de eficiencia.\nInterpretación intuitiva: Los ratios tienen significados naturales que facilitan la comunicación con stakeholders. Un ratio deuda/patrimonio de 0.3 es inmediatamente comprensible como “30 céntimos de deuda por cada euro de patrimonio”, mientras que valores absolutos requieren más contexto.\nRobustez ante outliers: Los ratios suelen ser menos sensibles a valores extremos que las variables absolutas. Si una empresa tiene ventas anómalamente altas pero también marketing proporcionalmente alto, el ratio ventas/marketing permanece estable, mientras que ambas variables individuales serían outliers.\nInvarianza bajo transformaciones: Los ratios mantienen sus relaciones bajo cambios de unidades o inflación. El ratio precio/ingresos de una acción es el mismo si se mide en euros o dólares, proporcionando estabilidad interpretativa a lo largo del tiempo y contextos.\n\nConsideraciones para construcción robusta de ratios:\nLos ratios requieren cuidado especial en su construcción para evitar interpretaciones erróneas o inestabilidad numérica. Es fundamental evitar denominadores cercanos a cero, considerar transformaciones logarítmicas para ratios con rangos amplios, y validar que el ratio tenga significado conceptual en el dominio de aplicación.\n\n\n4.7.3 Tratamiento de variables colineales mediante feature engineering\nCuando enfrentamos multicolinealidad entre predictores informativos, la ingeniería de características ofrece alternativas más sofisticadas que simplemente eliminar variables o usar interacciones sin efectos principales. Este escenario es común en la práctica: tenemos múltiples variables que aportan información valiosa individualmente, pero están suficientemente correlacionadas como para crear problemas de estabilidad e interpretación en el modelo.\nEl enfoque tradicional de “eliminar variables correlacionadas” es problemático porque puede resultar en pérdida significativa de información predictiva. Si variable_A y variable_B tienen correlación r = 0.75, ambas comparten 56% de varianza, pero cada una retiene 44% de información única. Eliminar cualquiera de ellas descarta información potencialmente valiosa que podría mejorar el poder predictivo del modelo.\nLa ingeniería de características para colinealidad busca condensar la información redundante mientras preserva la información única, creando nuevas variables que capturen la esencia predictiva de las variables originales sin los problemas de multicolinealidad. Esta aproximación es especialmente valiosa cuando la correlación entre variables tiene significado teórico: por ejemplo, diferentes medidas de solvencia financiera que capturan aspectos relacionados pero distintos del riesgo crediticio.\nEstrategias avanzadas de condensación de información:\n\nComponentes principales (PCA): Extraen direcciones de máxima varianza común, creando variables ortogonales que preservan la mayor cantidad de información con la menor dimensionalidad:\n# Ejemplo: Variables financieras correlacionadas\npc_financiero &lt;- prcomp(~ ingresos + patrimonio + crédito + liquidez, scale = TRUE)\nscore_financiero &lt;- pc_financiero$x[,1]  # Primer componente (mayor varianza)\nscore_diversificacion &lt;- pc_financiero$x[,2]  # Segundo componente (varianza residual)\nVentajas del PCA: Elimina completamente la multicolinealidad, preserva máxima varianza, proporciona interpretación de “factores latentes”. Desventajas: Pérdida de interpretabilidad directa, todos los componentes dependen de todas las variables originales, sensible a outliers.\nRatios informativos: Crean cocientes que preservan la información relativa más relevante, eliminando efectos de escala común:\n# Ratios que capturan relaciones estructurales fundamentales\nratio_debt_income &lt;- deuda_total / ingresos_anuales  # Capacidad de endeudamiento\nratio_assets_equity &lt;- activos / patrimonio_neto     # Apalancamiento\nratio_liquidity &lt;- activos_liquidos / pasivos_corrientes  # Solvencia a corto plazo\nVentajas de los ratios: Mantienen interpretabilidad económica directa, eliminan efectos de escala, capturan relaciones estructurales clave. Aplicabilidad: Especialmente efectivos cuando las variables correlacionadas miden aspectos del mismo fenómeno subyacente (ej. diferentes medidas de tamaño empresarial).\nÍndices ponderados: Combinan variables usando pesos derivados de conocimiento teórico o empírico, creando métricas compuestas más robustas que sus componentes individuales:\n# Índice de solvencia con pesos basados en evidencia empírica\nindice_solvencia &lt;- 0.4 * (ingresos/gastos) + 0.3 * (activos/deudas) + 0.3 * score_crediticio_normalizado\n\n# Índice de crecimiento balanceado\nindice_crecimiento &lt;- 0.5 * crecimiento_ventas + 0.3 * crecimiento_beneficios + 0.2 * crecimiento_empleados\nDeterminación de pesos: Pueden derivarse de análisis factorial confirmatorio, regresión ridge, conocimiento experto, o optimización empírica. Los pesos deben justificarse teóricamente y validarse en datos independientes.\nDiferencias y cambios relativos: Capturan dinámicas temporales y patrones de co-movimiento que revelan información única no presente en niveles absolutos:\n# Dinámicas de crecimiento relativo\ncrecimiento_relativo &lt;- (valor_actual - valor_anterior) / valor_anterior\naceleracion &lt;- (crecimiento_t - crecimiento_t_1) / crecimiento_t_1\n\n# Medidas de estabilidad y volatilidad\nvolatilidad &lt;- sd(ultimos_12_meses) / mean(ultimos_12_meses)\nconsistencia &lt;- 1 / (1 + cv(ultimos_periodos))  # Coeficiente de variación invertido\nAplicabilidad temporal: Especialmente útiles para series temporales donde variables están correlacionadas en niveles pero divergen en tasas de cambio, revelando dinámicas diferenciales ocultas en análisis de niveles.\n\n\n\n\n\n\n\nCriterios de selección de estrategia\n\n\n\n\nPCA: Cuando la interpretabilidad no es crítica y maximizar la retención de varianza es prioritario\nRatios: Cuando existe significado teórico claro en las relaciones proporcionales entre variables\nÍndices ponderados: Cuando hay conocimiento previo sobre la importancia relativa de cada componente\nCambios relativos: Cuando las dinámicas temporales son más informativas que los niveles absolutos\n\n\n\nLa ingeniería de características es tanto arte como ciencia: requiere creatividad para identificar combinaciones útiles, pero también rigor metodológico para validar que las nuevas variables realmente aportan valor predictivo estable y generalizable.\n\n\n\n\nBox, George EP, y David R Cox. 1964. «An analysis of transformations». Journal of the Royal Statistical Society: Series B (Methodological) 26 (2): 211-43.\n\n\nCarroll, Raymond J, y David Ruppert. 1988. «Transformation and weighting in regression». Monographs on Statistics and Applied Probability.\n\n\nJaccard, James, y Robert Turrisi. 2003. Interaction effects in multiple regression. Sage.\n\n\nKuhn, Max, y Kjell Johnson. 2019. Feature engineering and selection: a practical approach for predictive models. CRC Press.\n\n\nPotdar, Kedar, Taher S Pardawala, y Chinmay D Pai. 2017. «A comparative study of categorical variable encoding techniques for neural network classifiers». International journal of computer applications 175 (4): 7-9.\n\n\nYeo, In-Kwon, y Richard A Johnson. 2000. «A new family of power transformations to improve normality or symmetry». Biometrika 87 (4): 954-59.\n\n\nZheng, Alice, y Amanda Casari. 2018. Feature engineering for machine learning: principles and techniques for data scientists. O’Reilly Media.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ingeniería de características: transformaciones de variables e interacciones</span>"
    ]
  },
  {
    "objectID": "tema4.html",
    "href": "tema4.html",
    "title": "5  Selección de variables, regularización y validación",
    "section": "",
    "text": "5.1 Proceso completo de construcción y optimización del modelo\nEn los modelos de regresión, especialmente cuando se trabaja con conjuntos de datos que incluyen un gran número de variables predictoras, es común enfrentarse al desafío ntficar qué variables son realmente relevantes para explicar la variable respuesta. La inclusión de demasiadas variables en un modelo puede llevar a problemas como el sobreajuste, pérdida de interpretabilidad y complejidad innecesaria, mientras que la exclusión de variables importantes puede resultar en modelos subóptimos.\nEste tema aborda uno de los aspectos más críticos en la construcción de modelos de regresión: cómo seleccionar el subconjunto óptimo de variables predictoras y cómo validar la calidad del modelo resultante. Una vez realizado el análisis exploratorio y el ajuste inicial del modelo, surge la necesidad crítica de optimizar la selección de variables. Cuando se dispone de \\(p\\) variables explicativas, es posible construir hasta \\(2^p\\) modelos diferentes considerando todas las combinaciones posibles. Sin embargo, explorar de manera exhaustiva todos estos modelos puede ser computacionalmente inviable cuando \\(p\\) es grande.\nPara superar este desafío, en este tema nos enfocaremos en cinco enfoques principales:\nCada enfoque tiene sus propias ventajas y limitaciones, siendo apropiados para diferentes situaciones según el tamaño del dataset, el número de variables y los objetivos del análisis. El objetivo es presentar las técnicas más relevantes para la selección de variables y regularización, entender sus fundamentos teóricos, y aplicarlas a casos prácticos, culminando con métodos robustos de validación que aseguren la calidad y generalización del modelo final.\nLa construcción de un modelo de regresión múltiple es un proceso sistemático que busca explicar la relación entre una variable respuesta (\\(Y\\)) y múltiples variables predictoras (\\(X_1, X_2, \\dots, X_k\\)). Este proceso consta de varias etapas clave (Kutner et al. 2005), que en este tema nos enfocaremos particularmente en las etapas de reducción de variables y validación:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables, regularización y validación</span>"
    ]
  },
  {
    "objectID": "tema4.html#proceso-completo-de-construcción-y-optimización-del-modelo",
    "href": "tema4.html#proceso-completo-de-construcción-y-optimización-del-modelo",
    "title": "5  Selección de variables, regularización y validación",
    "section": "",
    "text": "Definición del problema y variables de interés:\n\nIdentificar claramente el objetivo del análisis, ya sea realizar predicciones, evaluar relaciones o controlar por efectos de variables confusoras.\nSeleccionar las variables predictoras potenciales en función de su relevancia teórica, conocimiento previo o exploración inicial de los datos.\n\nRecogida de datos:\n\n\nLa calidad de los datos recogidos influye directamente en la validez de los resultados y conclusiones obtenidas. El proceso de recogida de datos consiste en recopilar información de manera organizada y sistemática para responder a las preguntas de investigación planteadas. Dependiendo del diseño del estudio y los objetivos del análisis, se pueden emplear diferentes tipos de experimentos o métodos de recogida de datos.\nDebemos asegurar las siguientes características sobre los datos.\n\nFiabilidad: Asegurar que los datos sean consistentes y puedan reproducirse bajo condiciones similares.\nValidez: Garantizar que los datos recojan realmente la información necesaria para responder a las preguntas de investigación.\nÉtica: Asegurar la privacidad y el consentimiento informado de los participantes.\nControl de Sesgos: Diseñar el estudio de manera que se minimicen los sesgos que puedan distorsionar los resultados.\n\n\n\n\n\n\n\n\nTipos de experimentos\n\n\n\n\n\nLa elección del tipo de experimento o método de recogida de datos dependerá de la naturaleza del problema a investigar, los recursos disponibles y las limitaciones del estudio. Una correcta planificación y ejecución de esta etapa sienta las bases para un análisis robusto y confiable.\n\nExperimentos controlados:\n\nLos experimentos controlados son diseñados de manera que los investigadores manipulan deliberadamente una o más variables independientes (llamadas factores o variables controladas) para observar su efecto en la variable dependiente.\nIncluyen la aleatorización de sujetos entre grupos (por ejemplo, grupos de control y tratamiento) para minimizar sesgos y asegurar comparabilidad.\nEn muchas ocasiones la información suplementaria no se puede incorporar en el diseño del experimento. A esas variables, no controladas, se les suel llamar covariables.\nEjemplo: Un estudio clínico donde se prueba un nuevo medicamento y se compara su efecto con un placebo.\n\nEstudios observacionales exploratorios:\n\nEn este enfoque, los datos se recogen sin intervenir ni manipular las condiciones. Los investigadores observan y registran los fenómenos tal como ocurren en la naturaleza.\nPueden clasificarse en:\n\nEstudios transversales: Los datos se recogen en un único punto temporal.\nEstudios longitudinales: Los datos se recogen durante un periodo para analizar cambios a lo largo del tiempo.\n\nEjemplo: Investigar los hábitos alimenticios y su asociación con enfermedades cardiovasculares en una población.\n\nEstudios observacionales confirmatorios:\n\nEn este enfoque, los datos se recogen para testear (confirmar o no) hipótesis derivadas de estudios previos o de ideas que pueden tener los investigadores.\nEn este contexto, las variables que aparecen involucradas en la hipótesis que se quiere confirmar se denominan variables primarias, y las variables explicativas que se sabe inluyen en la respuesta se llaman variables de control (en Epidemiología nos referimos a ellas como factores de riesgo)\nEjemplo: Un equipo de investigadores, basándose en estudios previos, plantea la hipótesis de que existe una relación positiva entre el hábito de fumar (variable explicativa principal) y la incidencia de cáncer de pulmón (variable respuesta). Para confirmar esta hipótesis, realizan un estudio observacional en el que recopilan datos de una población durante un periodo determinado. Dado que no es ético inducir a las personas a fumar para realizar un experimento controlado, este estudio se realiza de forma observacional. Los datos se analizan para evaluar la asociación entre las variables, permitiendo confirmar (o refutar) la hipótesis planteada con un diseño adecuado y controlando los posibles factores de confusión.\n\nEncuestas y cuestionarios:\n\nLas encuestas son una técnica común para recoger datos de manera estructurada sobre actitudes, opiniones, comportamientos o características demográficas.\nPueden aplicarse en formato presencial, en línea, por teléfono o mediante correo.\nEjemplo: Una encuesta para medir el grado de satisfacción de los clientes con un servicio.\n\nExperimentos naturales:\n\nSe producen cuando un fenómeno natural o social actúa como una intervención en un entorno sin que los investigadores tengan control sobre el experimento.\nEste tipo de estudio aprovecha eventos únicos para analizar sus impactos.\nEjemplo: Estudiar los efectos económicos de una nueva política fiscal aplicada en una región específica.\n\nEstudios de simulación:\n\nLos datos se generan a través de modelos matemáticos o computacionales que representan un sistema real o hipotético.\nEste método se usa cuando es difícil o costoso realizar experimentos reales.\nEjemplo: Simular el comportamiento de un mercado financiero bajo diferentes escenarios económicos.\n\nRecogida de datos secundarios:\n\nEn lugar de recoger datos nuevos, se utilizan datos ya existentes recopilados por terceros, como censos, registros administrativos o bases de datos públicas.\nAunque es eficiente en tiempo y costos, el investigador tiene menor control sobre la calidad y las características de los datos.\nEjemplo: Analizar datos de encuestas nacionales para estudiar tendencias sociales.\n\n\n\n\n\n\nAnálisis Exploratorio de Datos (EDA):\n\nInspeccionar los datos mediante análisis descriptivo y visual para identificar posibles problemas como valores atípicos, datos faltantes y multicolinealidad.\nEscalar o transformar las variables si es necesario, especialmente si están en diferentes escalas o presentan distribuciones no lineales.\n\nAjuste del modelo:\n\nEspecificar el modelo de regresión múltiple en su forma general:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\varepsilon,\n\\] donde \\(\\varepsilon\\) representa los errores aleatorios.\nEstimar los coeficientes del modelo (\\(\\beta_0, \\beta_1, \\dots, \\beta_p\\)) utilizando el método de mínimos cuadrados, que minimiza la suma de los errores al cuadrado.\n\nEvaluación del modelo:\n\nAnalizar el ajuste general del modelo utilizando métricas como \\(R^2\\) y \\(R^2\\) ajustado, que miden la proporción de la variabilidad explicada.\nExaminar la tabla ANOVA para evaluar la significancia global del modelo.\nRealizar pruebas de hipótesis para los coeficientes individuales, verificando si las variables predictoras tienen un efecto significativo en la variable respuesta.\n\nDiagnóstico del modelo:\n\nExaminar los residuos para evaluar supuestos como la linealidad, homocedasticidad, normalidad de los errores y ausencia de autocorrelación.\nIdentificar observaciones atípicas, leverage y puntos de influencia utilizando herramientas como la distancia de Cook, DFBETAS y DFFITS.\n\nReducción de variables:\n\nEn análisis de regresión, especialmente cuando se trabaja con conjuntos de datos de alta dimensionalidad, es común enfrentar situaciones en las que el número de variables explicativas es muy grande. Esto puede llevar a problemas como el sobreajuste, dificultades en la interpretación del modelo y una mayor complejidad computacional. Por ello, reducir el número de variables explicativas, sin perder información relevante, se convierte en un paso crucial para construir modelos más eficientes y robustos.\n\nValidación del modelo:\n\nEvaluar el rendimientodel modelo con datos de validación o mediante técnicas como validación cruzada para garantizar su capacidad predictiva en nuevos conjuntos de datos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables, regularización y validación</span>"
    ]
  },
  {
    "objectID": "tema4.html#filtrado-basado-en-información-básica",
    "href": "tema4.html#filtrado-basado-en-información-básica",
    "title": "5  Selección de variables, regularización y validación",
    "section": "5.2 Filtrado basado en información básica",
    "text": "5.2 Filtrado basado en información básica\nAntes de aplicar métodos sofisticados de selección de variables, es fundamental realizar un filtrado preliminar basado en información básica. Este primer paso consiste en identificar y descartar variables que claramente no aportan información relevante al modelo, reduciendo significativamente el espacio de búsqueda y mejorando la eficiencia de los métodos posteriores (James et al. 2013).\nLos criterios principales para este filtrado incluyen:\n1. Variabilidad de las variables predictoras\nVariables con varianza muy baja o constantes proporcionan poca información discriminatoria. Se descartan variables donde:\n\\[\\text{Var}(X_j) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{ij} - \\bar{x}_j)^2 &lt; \\epsilon\\]\npara algún umbral pequeño \\(\\epsilon\\) (típicamente \\(\\epsilon = 0.01\\)).\n2. Correlación con la variable respuesta\nVariables con correlación muy baja con \\(Y\\) pueden ser candidatas a eliminación. Se calcula:\n\\[r_{X_j,Y} = \\frac{\\sum_{i=1}^{n}(x_{ij} - \\bar{x}_j)(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_{ij} - \\bar{x}_j)^2\\sum_{i=1}^{n}(y_i - \\bar{y})^2}}\\]\ny típicamente se establece un umbral mínimo \\(|r_{X_j,Y}| &gt; \\delta\\) (ej: \\(\\delta = 0.1\\)).\n3. Multicolinealidad extrema\nVariables altamente correlacionadas entre sí pueden ser redundantes. Se calcula:\n\\[r_{X_j,X_k} = \\frac{\\text{Cov}(X_j, X_k)}{\\sqrt{\\text{Var}(X_j)\\text{Var}(X_k)}}\\]\nSi \\(|r_{X_j,X_k}| &gt; 0.95\\), se considera eliminar una de las dos variables.\n4. Factor de Inflación de la Varianza (VIF)\nPara detectar multicolinealidad más compleja se calcula:\n\\[VIF_j = \\frac{1}{1-R^2_j}\\]\ndonde \\(R^2_j\\) es el coeficiente de determinación de la regresión de \\(X_j\\) sobre las demás variables predictoras. Valores \\(VIF_j &gt; 10\\) indican multicolinealidad problemática.\n\n\n\n\n\n\nEjemplo de filtrado inicial\n\n\n\n\n\nEn este ejemplo aplicamos el proceso completo de filtrado basado en información a un conjunto de datos simulado con diferentes características.\n\n# Configuración y generación de datos\nset.seed(123)\nn &lt;- 100\np &lt;- 15\n\n# Generar datos con diferentes características\nX &lt;- matrix(rnorm(n * p), n, p)\ncolnames(X) &lt;- paste0(\"X\", 1:p)\n\n# Variable constante (sin variabilidad)\nX[, 1] &lt;- 5\n\n# Variable con muy baja variabilidad  \nX[, 2] &lt;- 5 + rnorm(n, 0, 0.01)\n\n# Variables moderadamente correlacionadas\nX[, 4] &lt;- X[, 3] + rnorm(n, 0, 0.5)\nX[, 5] &lt;- 0.7 * X[, 3] + rnorm(n, 0, 0.6)\n\n# Variable respuesta con coeficientes conocidos\nbeta &lt;- c(0, 0, 2, 1.5, 1.2, -1, 0.8, rep(0, 8))\ny &lt;- X %*% beta + rnorm(n)\n\ndatos &lt;- data.frame(y = y, X)\n\nsuppressPackageStartupMessages(library(car))\n\n# 1. Análisis de variabilidad\nvarianzas &lt;- apply(X, 2, var)\nvars_baja_var &lt;- which(varianzas &lt; 0.01)\n\n# 2. Filtrar por correlación con Y\nX_filtrada &lt;- if(length(vars_baja_var) &gt; 0) X[, -vars_baja_var] else X\ncorrelaciones &lt;- cor(X_filtrada, y)\nvars_baja_corr_idx &lt;- which(abs(correlaciones) &lt; 0.1)\nvars_baja_corr &lt;- if(length(vars_baja_corr_idx) &gt; 0) {\n  as.numeric(gsub(\"X\", \"\", colnames(X_filtrada)[vars_baja_corr_idx]))\n} else {\n  c()\n}\n\n# 3. Identificar correlaciones altas entre predictores\ncor_matrix &lt;- cor(X_filtrada)\nhigh_cor &lt;- which(abs(cor_matrix) &gt; 0.8 & abs(cor_matrix) &lt; 1, arr.ind = TRUE)\n\n# 4. Calcular VIF para variables restantes\nvars_eliminar &lt;- unique(c(vars_baja_var, vars_baja_corr))\ndatos_final &lt;- if(length(vars_eliminar) &gt; 0) {\n  datos[, -(vars_eliminar + 1)] # +1 porque datos incluye y en primera columna\n} else {\n  datos\n}\n\nvif_valores &lt;- if(ncol(datos_final) &gt; 2) {\n  modelo &lt;- lm(y ~ ., data = datos_final)\n  vif(modelo)\n} else {\n  NULL\n}\n\n# Preparar resumen de resultados\nresumen_vars_baja_var &lt;- if(length(vars_baja_var) &gt; 0) {\n  paste(\"X\", vars_baja_var, collapse=\", \")\n} else {\n  \"Ninguna\"\n}\n\nresumen_vars_baja_corr &lt;- if(length(vars_baja_corr) &gt; 0) {\n  paste(\"X\", vars_baja_corr, collapse=\", \")\n} else {\n  \"Ninguna\"\n}\n\nresumen_high_cor &lt;- if(nrow(high_cor) &gt; 0) {\n  correlaciones_altas &lt;- character()\n  for(i in 1:nrow(high_cor)) {\n    var1 &lt;- colnames(X_filtrada)[high_cor[i,1]]\n    var2 &lt;- colnames(X_filtrada)[high_cor[i,2]]\n    corr_val &lt;- round(cor_matrix[high_cor[i,1], high_cor[i,2]], 3)\n    correlaciones_altas &lt;- c(correlaciones_altas, paste(var1, \"y\", var2, \":\", corr_val))\n  }\n  correlaciones_altas\n} else {\n  \"Ninguna\"\n}\n\nvars_restantes &lt;- ncol(datos_final) - 1\nvars_originales &lt;- p\n\nresumen_vif &lt;- if(!is.null(vif_valores)) {\n  vif_summary &lt;- character()\n  for(i in 1:length(vif_valores)) {\n    estado &lt;- if(vif_valores[i] &gt; 10) \" (ALTO)\" else if(vif_valores[i] &gt; 5) \" (moderado)\" else \"\"\n    vif_summary &lt;- c(vif_summary, paste(names(vif_valores)[i], \":\", round(vif_valores[i], 2), estado))\n  }\n  vif_summary\n} else {\n  \"No calculado\"\n}\n\nResultados del filtrado basado en información:\n1. Variables eliminadas por baja variabilidad: X 1, X 2\n2. Variables eliminadas por baja correlación con Y: X 8, X 9, X 10, X 11, X 12, X 13, X 14\n3. Correlaciones altas entre predictores (|r| &gt; 0.8): X4 y X3 : 0.846 , X3 y X4 : 0.846\n4. Variables restantes: 6 de 15 originales\n5. Factores VIF de variables finales: X3 : 4.87 , X4 : 3.56 , X5 : 2.26 , X6 : 1.06 , X7 : 1.03 , X15 : 1.07\nEste proceso de filtrado redujo el conjunto original de 15 variables a 6 variables, eliminando efectivamente las variables con problemas de variabilidad y correlación identificados.\n\n\n\nEl proceso de filtrado se implementa secuencialmente: (1) eliminar variables constantes o con varianza cercana a cero, (2) eliminar variables con correlación muy baja con la variable respuesta, (3) identificar grupos de variables multicolineales y retener solo la más relevante de cada grupo, y (4) calcular VIF y eliminar variables con valores muy altos. Este filtrado inicial típicamente reduce el conjunto de variables candidatas, facilitando significativamente los pasos posteriores de selección.\nEs importante considerar que este filtrado no es definitivo, ya que variables eliminadas en esta etapa pueden ser importantes en combinaciones específicas. Además, está basado en relaciones lineales y puede omitir relaciones no lineales importantes. Por tanto, requiere validación posterior del modelo resultante y los umbrales deben ajustarse según el dominio de aplicación específico.\n¡Claro! El contenido que tienes es excelente y muy completo. Para hacerlo menos esquemático, lo he reescrito en un formato más narrativo, conectando las ideas en párrafos fluidos. La idea es transformar las listas y tablas en una explicación discursiva, como si lo estuvieras contando en una clase, lo que se adapta mejor al formato de un libro.\nAquí tienes la propuesta:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables, regularización y validación</span>"
    ]
  },
  {
    "objectID": "tema4.html#criterios-de-bondad-de-ajuste",
    "href": "tema4.html#criterios-de-bondad-de-ajuste",
    "title": "5  Selección de variables, regularización y validación",
    "section": "5.3 Criterios de Bondad de Ajuste",
    "text": "5.3 Criterios de Bondad de Ajuste\nUna vez completado el filtrado preliminar de variables, nos enfrentamos a una de las tareas más importantes del modelado: seleccionar la combinación óptima de predictores. El objetivo es encontrar un equilibrio delicado. Un modelo con muy pocas variables puede ser demasiado simple y no capturar la relación real (subajuste o underfitting), mientras que un modelo con demasiadas variables puede ajustarse al ruido de la muestra y no generalizar bien a nuevos datos (sobreajuste u overfitting).\nPara navegar este compromiso, utilizamos criterios de información que cuantifican la calidad de un modelo, equilibrando su capacidad explicativa con su complejidad. Estos nos permiten comparar modelos con diferente número de predictores de forma rigurosa y objetiva. Los tres criterios más influyentes en la estadística clásica son el Criterio de Información de Akaike (AIC), el Criterio de Información Bayesiano (BIC) y el estadístico Cp de Mallows.\n\n5.3.1 Criterio de Información de Akaike\nEl Criterio de Información de Akaike (AIC), desarrollado por Hirotugu Akaike, es una métrica fundamentada en la teoría de la información (James et al. 2013). Su propósito es estimar la pérdida de información que ocurre cuando usamos un modelo para representar la realidad. El modelo que minimice esta pérdida de información será considerado el mejor.\nLa fórmula del AIC para un modelo de regresión lineal es:\n\\[AIC = n \\ln\\left(\\frac{SSE}{n}\\right) + 2(p+1)\\]\nEn esta ecuación, \\(n\\) es el tamaño de la muestra, \\(SSE\\) es la Suma de Cuadrados del Error y \\(p\\) es el número de variables predictoras. La fórmula equilibra dos fuerzas opuestas:\n\nBondad de ajuste: El primer término, \\(n \\ln(SSE/n)\\), está directamente relacionado con la función de log-verosimilitud del modelo. Disminuye a medida que el modelo se ajusta mejor a los datos (es decir, a medida que el SSE se reduce).\nPenalización por complejidad: El segundo término, \\(2(p+1)\\), actúa como un castigo. Aumenta en 2 unidades por cada parámetro adicional que se incluye en el modelo (p pendientes + 1 intercepto).\n\nEn la práctica, calculamos el AIC para varios modelos candidatos y seleccionamos aquel con el valor de AIC más bajo. Este criterio es asintóticamente eficiente, lo que significa que, con muestras suficientemente grandes, tiende a seleccionar el modelo que minimiza el error de predicción esperado en nuevos datos.\n\n\n5.3.2 Criterio de Información Bayesiano\nEl Criterio de Información Bayesiano (BIC), propuesto por Gideon Schwarz, es un competidor directo del AIC, pero con fundamentos en la estadística bayesiana (Hastie et al. 2009). Mientras que el AIC busca el mejor modelo para la predicción, el BIC está diseñado para encontrar el modelo más probable de ser el “verdadero” generador de los datos.\nSu fórmula es muy similar a la del AIC, pero la penalización por complejidad es diferente y más severa:\n\\[BIC = n \\ln\\left(\\frac{SSE}{n}\\right) + (p+1) \\ln(n)\\]\nLa diferencia clave reside en el término de penalización. En lugar de \\(2(p+1)\\), el BIC utiliza \\((p+1)\\ln(n)\\). Dado que el logaritmo natural de \\(n\\) es mayor que 2 para cualquier muestra con más de 7 observaciones (\\(e^2 \\approx 7.4\\)), la penalización del BIC es casi siempre más fuerte que la del AIC. Esta penalización más estricta le confiere al BIC una tendencia hacia la parsimonia, favoreciendo modelos más simples. Una de sus propiedades teóricas más importantes es la consistencia: si el modelo verdadero se encuentra entre los candidatos, la probabilidad de que el BIC lo seleccione tiende a 1 a medida que el tamaño de la muestra crece.\n\n\n5.3.3 Estadístico Cp de Mallows\nA diferencia del AIC y el BIC, el estadístico Cp de Mallows no se basa en la teoría de la información ni en la estadística bayesiana, sino que aborda directamente el error cuadrático medio de predicción del modelo (James et al. 2013). Su objetivo es encontrar un modelo que tenga un bajo sesgo y una baja varianza.\nLa fórmula para el estadístico Cp es:\n\\[C_p = \\frac{SSE_p}{MSE_{full}} - n + 2(p+1)\\]\nAquí, \\(SSE_p\\) es la suma de cuadrados del error del modelo candidato con \\(p\\) variables, y \\(MSE_{full}\\) es el error cuadrático medio del modelo completo (el que incluye todas las variables predictoras disponibles), que se utiliza como una estimación insesgada de la varianza del error poblacional, \\(\\sigma^2\\).\nLa interpretación del Cp es particularmente intuitiva. Si un modelo está bien especificado (es decir, no incluye un sesgo significativo), se espera que su valor de \\(C_p\\) sea cercano al número de parámetros, \\(p+1\\).\nPor lo tanto, la estrategia de selección consiste en elegir el modelo que tenga el valor de Cp más bajo. Este modelo representa el mejor equilibrio entre el sesgo y la varianza. Generalmente, observaremos dos cosas en un gráfico de Cp vs. p:\n\nLos modelos con pocas variables y \\(C_p\\) muy por encima de la línea \\(p+1\\) sufren de un sesgo elevado (subajuste).\nEl modelo con el \\(C_p\\) más bajo es el preferido. Normalmente, este valor mínimo también estará cerca de la línea \\(p+1\\), confirmando su buen ajuste.\n\n\n\n\n\n\n\nVisualización del Cp de Mallows\n\n\n\n\n\nUna de las aplicaciones más útiles del Cp de Mallows es su visualización gráfica para identificar el modelo óptimo. El siguiente ejemplo muestra cómo crear un gráfico de Cp que facilita la interpretación y selección del mejor modelo.\n\n# Cargar librerías necesarias\nsuppressPackageStartupMessages({\n  library(leaps)     # Para best subset selection\n  library(ggplot2)   # Para gráficos\n  library(dplyr)     # Para manipulación de datos\n})\n\n# Usar el dataset mtcars para el ejemplo\ndata(mtcars)\n\n# 1. Realizar best subset selection\nbest_subset &lt;- regsubsets(mpg ~ ., data = mtcars, nvmax = 10)\nsubset_summary &lt;- summary(best_subset)\n\n# 2. Extraer información relevante para el gráfico\nplot_data &lt;- data.frame(\n  n_variables = 1:length(subset_summary$cp),\n  Cp = subset_summary$cp\n)\n\n# 3. Identificar el mejor modelo según el criterio Cp\n#    La regla es simple: escoger el modelo con el menor valor de Cp.\nmejor_cp_idx &lt;- which.min(subset_summary$cp)\nmejor_cp_valor &lt;- subset_summary$cp[mejor_cp_idx]\n\n# 4. Crear el gráfico del Cp de Mallows\nggplot(plot_data, aes(x = n_variables, y = Cp)) +\n  # Línea de referencia ideal (Cp = p+1)\n  geom_abline(intercept = 1, slope = 1, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  # Puntos y línea de los valores Cp de los modelos\n  geom_point(color = \"#0072B2\", size = 3) +\n  geom_line(color = \"#0072B2\") +\n  # Resaltar el mejor punto (el que tiene el Cp mínimo)\n  geom_point(aes(x = mejor_cp_idx, y = mejor_cp_valor), color = \"red\", size = 5, shape = 17) +\n  # Etiqueta para el mejor punto\n  annotate(\"text\", x = mejor_cp_idx, y = mejor_cp_valor + 1.5,\n           label = paste(\"Óptimo Cp\\n(\", mejor_cp_idx, \"variables)\"), color = \"red\", size = 4) +\n  labs(\n    title = \"Criterio Cp de Mallows para Selección de Variables (mtcars)\",\n    subtitle = \"Se busca el modelo con el valor de Cp más bajo\",\n    x = \"Número de Variables Predictoras (p)\",\n    y = \"Valor de Cp\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(plot.title = element_text(face = \"bold\")) +\n  scale_x_continuous(breaks = 1:10)\n\nWarning in geom_point(aes(x = mejor_cp_idx, y = mejor_cp_valor), color = \"red\", : All aesthetics have length 1, but the data has 10 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n# 5. Mostrar las variables del modelo seleccionado\nvariables_mejor_modelo_cp &lt;- names(which(subset_summary$which[mejor_cp_idx, -1]))\n\nEl gráfico resultante nos permite diagnosticar visualmente la calidad de los modelos candidatos. La línea discontinua roja representa la referencia para un modelo sin sesgo (\\(C_p = p+1\\)).\nLa estrategia de selección es clara: identificar el modelo que minimice el estadístico Cp. Este punto representa el mejor equilibrio teórico entre el sesgo del modelo (subajuste) y su varianza (sobreajuste). La línea roja nos ayuda a confirmar visualmente que el modelo elegido, además de ser el de menor Cp, tiene un sesgo bajo.\nComo se observa, el estadístico Cp disminuye drásticamente al pasar de uno a dos predictores. El modelo con 3 variables es el seleccionado como óptimo porque alcanza el valor de Cp más bajo de todos los candidatos, con un valor de 0.1.\nEl análisis sugiere que el modelo más parsimonioso y con el mejor rendimiento predictivo se compone de las siguientes variables: wt, qsec, am. Añadir más predictores más allá de este punto óptimo no mejora el modelo; de hecho, el valor de Cp comienza a aumentar, lo que indica que estamos añadiendo una complejidad innecesaria y empezando a sobreajustar los datos.\n\n\n\n\n\n5.3.4 ¿Cuándo Usar Cada Criterio?\nLa existencia de varios criterios plantea una pregunta natural: ¿cuál debemos usar? La respuesta depende en gran medida del objetivo final de nuestro análisis.\nSi el objetivo principal es la predicción, el AIC suele ser la opción preferida. Su diseño para minimizar el error de predicción lo hace ideal en contextos de pronóstico, donde el rendimiento en datos nuevos es lo más importante. Su penalización más moderada permite incluir variables que, aunque no sean “verdaderas” en un sentido causal, ayudan a mejorar la precisión de las predicciones.\nPor otro lado, si el objetivo es la explicación o la inferencia —es decir, identificar el modelo más parsimonioso que probablemente representa el verdadero proceso generador de los datos—, el BIC es la elección más sólida. Su penalización más fuerte protege de forma más robusta contra el sobreajuste y, en muestras grandes, su propiedad de consistencia le da una base teórica más fuerte para la selección del “modelo verdadero”.\nEl Cp de Mallows es especialmente valioso en un contexto más exploratorio, cuando queremos entender explícitamente el compromiso entre el sesgo y la varianza. Al graficar \\(C_p\\) frente a \\(p+1\\) para diferentes subconjuntos de modelos, podemos visualizar claramente el punto en el que añadir más variables deja de reducir el sesgo y solo empieza a inflar la varianza, ofreciendo una visión muy clara del “codo” de complejidad óptima.\nEs común que estos criterios no coincidan en su selección. Cuando esto ocurre, no debe verse como un fracaso, sino como una indicación de que no existe un único modelo “mejor” de forma inequívoca. En tales casos, el juicio del analista es clave, y se pueden usar herramientas adicionales como la validación cruzada (cross-validation) para comparar el rendimiento predictivo de los modelos finalistas y tomar una decisión informada.\n\n\n\n\n\n\nEl Principio de Parsimonia en la Selección de Modelos\n\n\n\nEl principio de parsimonia, también conocido como la “navaja de Occam”, es un concepto fundamental que subyace a todos los criterios de bondad de ajuste. Este principio establece que, entre modelos que explican igualmente bien un fenómeno, se debe preferir el más simple.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables, regularización y validación</span>"
    ]
  },
  {
    "objectID": "tema4.html#métodos-de-selección-exhaustiva",
    "href": "tema4.html#métodos-de-selección-exhaustiva",
    "title": "5  Selección de variables, regularización y validación",
    "section": "5.4 Métodos de selección exhaustiva",
    "text": "5.4 Métodos de selección exhaustiva\nLos métodos de selección exhaustiva son un enfoque fundamental en la búsqueda de un subconjunto óptimo de variables predictoras en modelos de regresión. Este enfoque evalúa de manera sistemática diferentes combinaciones de variables para identificar cuál de ellas proporciona el mejor ajuste al modelo en función de un criterio predefinido, como el coeficiente de determinación ajustado (\\(R^2\\) ajustado) o criterios de información como AIC o BIC.\nA diferencia de los métodos automáticos, los métodos de selección exhaustiva no dependen de un proceso iterativo de adición o eliminación de variables. En cambio, buscan exhaustivamente (o mediante aproximaciones computacionalmente más eficientes) entre todas las posibles combinaciones de variables, lo que garantiza un análisis completo de las interacciones y relevancias potenciales.\nEl método más conocido dentro de este enfoque es la selección del mejor subconjunto (Best Subset Selection), que evalúa todos los subconjuntos posibles de variables y selecciona el mejor para cada tamaño específico. Es el enfoque más completo pero también el más exigente computacionalmente. Para un conjunto de \\(p\\) variables predictoras, este método construye todos los modelos posibles que incluyen \\(k\\) variables, donde \\(k = 1, 2, ..., p\\), seleccionando el mejor modelo de cada tamaño según el criterio elegido.\n\n\n\n\n\n\nEjemplo de selección exhaustiva\n\n\n\n\n\n\n# Ejemplo de Best Subset Selection maximizando R² ajustado\nsuppressPackageStartupMessages(library(leaps))\n\n# Usando el dataset mtcars\ndata(mtcars)\n\n# Realizar best subset selection\nbest_subset &lt;- regsubsets(mpg ~ ., data = mtcars, nvmax = 10)\n\n# Obtener estadísticas del mejor modelo según R² ajustado\nsubset_summary &lt;- summary(best_subset)\nmejor_modelo_idx &lt;- which.max(subset_summary$adjr2)\nmejor_r2_adj &lt;- max(subset_summary$adjr2)\ntotal_variables &lt;- ncol(mtcars) - 1  # Excluir variable respuesta\nvariables_seleccionadas &lt;- mejor_modelo_idx\n\nEl método de selección exhaustiva aplicado al conjunto de datos mtcars identifica el modelo óptimo que maximiza el R² ajustado. Este modelo alcanza un R² ajustado de 0.8375, utilizando 5 variables del total de 10 variables predictoras disponibles. Esta selección representa un equilibrio óptimo entre la capacidad explicativa del modelo y la penalización por complejidad, demostrando cómo la evaluación exhaustiva puede identificar el subconjunto de variables que mejor explica la variabilidad en el consumo de combustible.\n\n\n\nEsta aproximación presenta importantes ventajas: garantiza encontrar el mejor subconjunto según el criterio elegido (optimalidad garantizada), examina todas las posibles combinaciones de variables ofreciendo una evaluación completa, y proporciona un estándar sólido para comparar otros métodos de selección. Sin embargo, también tiene limitaciones significativas: la complejidad computacional crece exponencialmente ya que con \\(p\\) variables se generan \\(2^p\\) modelos posibles, lo que hace que sea impracticable para \\(p\\) grande (típicamente \\(p &gt; 15-20\\)). Además, sin una validación cruzada adecuada, puede seleccionar modelos sobreajustados.\nEstos métodos son especialmente útiles cuando el número de predictores no es demasiado grande, ya que el esfuerzo computacional crece exponencialmente con el número de variables. Aunque el costo computacional puede ser elevado en datasets amplios, los métodos de selección exhaustiva proporcionan una referencia sólida y transparente para evaluar qué variables son fundamentales en el modelo, siendo particularmente valiosos en estudios donde la interpretabilidad y la certeza sobre la selección de variables son prioritarias.\n¡Perfecto! El texto que tienes es una excelente introducción. Ahora vamos a expandir cada uno de esos puntos para darles la profundidad teórica y práctica que necesitan en el libro, explicando el algoritmo de cada método, sus criterios de decisión y sus ventajas y limitaciones.\nAquí tienes una propuesta para desarrollar esa sección.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables, regularización y validación</span>"
    ]
  },
  {
    "objectID": "tema4.html#métodos-automáticos-paso-a-paso",
    "href": "tema4.html#métodos-automáticos-paso-a-paso",
    "title": "5  Selección de variables, regularización y validación",
    "section": "5.5 Métodos automáticos paso a paso",
    "text": "5.5 Métodos automáticos paso a paso\nLos métodos automáticos de selección de variables, a menudo llamados métodos secuenciales o por pasos (stepwise), son algoritmos diseñados para explorar el espacio de posibles modelos de una manera computacionalmente eficiente. A diferencia del método de mejores subconjuntos (best subset selection), que evalúa todos los modelos posibles, estos enfoques siguen un camino restringido, añadiendo o quitando predictores de uno en uno.\nEl principio clave es construir un modelo de forma iterativa, tomando en cada paso una decisión “localmente óptima” basada en un criterio estadístico. Los criterios más comunes son el p-valor de un predictor, o el cambio que este produce en un indicador global como el AIC, el BIC o el \\(R^2\\) ajustado.\n\n5.5.1 Selección progresiva (Forward Selection)\nEsta estrategia es la más intuitiva: parte de la nada y construye el modelo pieza por pieza, añadiendo en cada paso el predictor que aporta la mayor mejora.\nEl Algoritmo\n\nInicio: Se comienza con el modelo nulo, que solo contiene el intercepto (\\(Y \\sim 1\\)).\nPrimer Paso: Se ajustan \\(p\\) modelos de regresión simple, uno para cada una de las \\(p\\) variables predictoras disponibles. Se elige la variable que mejor explica la respuesta (la que tiene el p-valor más bajo en su test t, o la que produce el AIC/BIC más bajo). Esta variable se convierte en el primer predictor del modelo.\nPasos Siguientes: Se ajustan \\(p-1\\) nuevos modelos, cada uno de los cuales contiene la(s) variable(s) ya seleccionada(s) más una de las variables restantes. De nuevo, se selecciona y se añade la variable que produce la mayor mejora en el criterio elegido.\nFinalización: El proceso se repite y se detiene cuando ninguna de las variables restantes mejora el modelo de forma significativa al ser añadida (por ejemplo, ninguna tiene un p-valor por debajo de un umbral predefinido, o el AIC/BIC del modelo deja de disminuir).\n\nVentajas y Limitaciones\n\nVentaja: Es computacionalmente muy eficiente. Puede aplicarse en situaciones con un número de predictores muy grande, incluso cuando hay más predictores que observaciones (\\(p &gt; n\\)).\nLimitación: Su principal debilidad es su “miopía”. Una variable seleccionada en una etapa temprana se queda en el modelo para siempre. Sin embargo, es posible que esa variable se vuelva redundante una vez que se añadan otros predictores. El método forward no puede rectificar decisiones pasadas, por lo que no garantiza encontrar el mejor modelo posible.\n\n\n\n5.5.2 Eliminación regresiva (Backward Elimination)\nEsta estrategia adopta el enfoque opuesto: empieza con todo y va eliminando lo que no es útil, como un escultor que retira el mármol sobrante.\nEl Algoritmo\n\nInicio: Se comienza con el modelo completo, que incluye todas las \\(p\\) variables predictoras disponibles (\\(Y \\sim X_1 + X_2 + \\dots + X_p\\)).\nPrimer Paso: Se ajusta el modelo completo y se examina la significancia de cada predictor. Se identifica la variable menos significativa, es decir, aquella con el p-valor más alto en su test t (o la que, al ser eliminada, produce la menor disminución en la calidad del modelo según AIC/BIC).\nPasos Siguientes: Si el p-valor de esa variable supera un umbral de permanencia (p. ej., \\(\\alpha_{out} = 0.10\\)), se elimina del modelo. A continuación, se vuelve a ajustar el modelo con las \\(p-1\\) variables restantes.\nFinalización: El proceso de identificar y eliminar la variable menos significativa se repite hasta que todas las variables que quedan en el modelo son estadísticamente significativas (es decir, todas tienen un p-valor por debajo del umbral de permanencia).\n\nVentajas y Limitaciones\n\nVentaja: Generalmente se considera superior al método forward porque empieza evaluando el efecto de cada variable en presencia de todas las demás. Esto proporciona un contexto inicial más completo.\nLimitación: No se puede utilizar si el número de predictores es mayor que el número de observaciones (\\(p &gt; n\\)), ya que es imposible ajustar el modelo completo inicial. Además, al igual que el método forward, una vez que una variable es eliminada, no puede volver a entrar, lo que podría llevar a eliminar por error una variable que es importante en combinación con un subconjunto más pequeño de predictores.\n\n\n\n5.5.3 Selección paso a paso (Stepwise Regression)\nEste método es un híbrido que intenta combinar lo mejor de las dos estrategias anteriores, permitiendo un proceso de “prueba y error” más flexible.\nEl Algoritmo\nLa selección stepwise es esencialmente una selección forward con un añadido crucial: en cada paso, después de añadir una nueva variable, se realiza una verificación hacia atrás para comprobar si alguna de las variables que ya estaban en el modelo se ha vuelto redundante.\n\nPaso Adelante (Forward): Al igual que en la selección progresiva, se añade la variable que más mejora el modelo.\nPaso Atrás (Backward): Después de añadir esa variable, se examinan todas las variables ya incluidas en el modelo. Si alguna de ellas ha perdido su significancia (su p-valor ha aumentado por encima de un umbral de eliminación), se elimina.\nRepetición: El proceso continúa, alternando pasos hacia adelante y hacia atrás, hasta que se alcanza un punto de equilibrio en el que ninguna variable puede ser añadida ni eliminada según los umbrales establecidos.\n\nVentajas y Limitaciones\n\nVentaja: Es más robusto que los métodos forward o backward puros, ya que puede corregir decisiones anteriores. Una variable que fue importante al principio puede ser eliminada más tarde si otra la hace redundante.\nLimitación: A pesar de su flexibilidad, sigue siendo un algoritmo “codicioso” (greedy) que no explora todo el espacio de modelos. Por tanto, tampoco garantiza encontrar el mejor modelo global.\n\n\n\n\n\n\n\nAdvertencia sobre los métodos automáticos\n\n\n\nAunque estos métodos son herramientas útiles para un primer cribado de variables, deben usarse con extrema cautela. Su naturaleza automática puede llevar a conclusiones erróneas si no se supervisan con criterio.\n\nNo garantizan el mejor modelo: Al seguir un camino fijo, pueden pasar por alto el subconjunto de variables verdaderamente óptimo.\nInvalidez de los p-valores: Los p-valores, errores estándar e intervalos de confianza del modelo final están sesgados y son excesivamente optimistas. El proceso de selección ha “elegido a los ganadores” de antemano, y la teoría de la inferencia estándar no se aplica a un modelo que ha sido seleccionado de esta manera.\nInestabilidad: Los resultados pueden ser muy sensibles a pequeñas variaciones en los datos. Añadir o quitar unas pocas observaciones puede cambiar drásticamente el modelo seleccionado.\n\nPor estas razones, los métodos automáticos deben considerarse como herramientas exploratorias para generar modelos candidatos, no como un procedimiento definitivo. La selección final siempre debe estar guiada por el conocimiento del dominio, la teoría subyacente y un diagnóstico riguroso.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables, regularización y validación</span>"
    ]
  },
  {
    "objectID": "tema4.html#métodos-basados-en-regularización",
    "href": "tema4.html#métodos-basados-en-regularización",
    "title": "5  Selección de variables, regularización y validación",
    "section": "5.6 Métodos basados en regularización",
    "text": "5.6 Métodos basados en regularización\nEn los modelos de regresión, especialmente cuando se trabaja con un gran número de variables predictoras o con datos multicolineales, los métodos tradicionales de selección de variables pueden resultar ineficaces o inestables. En estos casos, los métodos basados en regularización surgen como una alternativa poderosa que no solo selecciona variables, sino que también mejora la estabilidad y la precisión del modelo.\nLa regularización consiste en introducir una penalización en la función de ajuste del modelo, lo que tiene dos efectos principales: controlar el sobreajuste al reducir la complejidad del modelo y forzar la selección de un subconjunto más parsimonioso de predictores. Estas penalizaciones ajustan los coeficientes de las variables predictoras, favoreciendo soluciones más simples y robustas (James et al. 2013).\nEntre los métodos de regularización más destacados se encuentran:\n\nRidge Regression: Aplica una penalización proporcional al cuadrado de los coeficientes, lo que permite manejar problemas de multicolinealidad pero no conduce a la eliminación completa de variables.\nLasso (Least Absolute Shrinkage and Selection Operator): Introduce una penalización basada en el valor absoluto de los coeficientes, lo que no solo reduce su magnitud, sino que también puede anularlos completamente, realizando una selección automática de variables.\nElastic Net: Combina las penalizaciones de Ridge y Lasso, ofreciendo mayor flexibilidad en situaciones donde hay una gran correlación entre los predictores.\n\nEstos métodos son especialmente útiles en problemas donde el número de variables predictoras excede el número de observaciones, o cuando se desea un modelo más interpretable. En esta sección, exploraremos en detalle los fundamentos teóricos, la implementación práctica y las aplicaciones de cada uno de estos métodos, destacando sus ventajas en escenarios complejos y desafiantes.\n\n5.6.1 Ridge regression\nLa regresión Ridge introduce una penalización en la estimación de los coeficientes de regresión, lo que ayuda a reducir la varianza del modelo y mejora su capacidad predictiva en presencia de datos altamente correlacionados o con muchas variables (Marquardt y Snee 1975). El modelo de regresión Ridge es una extensión de la regresión lineal estándar. Con datos observados, escribimos:\n\\[\n\\mathbf{y}= \\mathbf{X} \\, \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]\ndonde:\n\n\\(\\mathbf{y}\\) es el vector de respuesta observado de dimensión \\(n \\times 1\\).\n\\(\\mathbf{X}\\) es la matriz de diseño observada de dimensión \\(n \\times (p+1)\\) (la primera columna suele ser de unos para el intercepto).\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_p)^T\\) es el vector de coeficientes.\n\\(\\boldsymbol{\\varepsilon}\\) es el vector de errores.\n\nEn mínimos cuadrados ordinarios (OLS), los coeficientes se estiman minimizando la suma de los errores al cuadrado:\n\\[\nSSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\|\n\\mathbf{y} - \\mathbf{X} \\, \\boldsymbol{\\beta} \\|^2.\n\\]\nSin embargo, cuando hay multicolinealidad, la matriz \\(\\mathbf{X}^T \\mathbf{X}\\) puede ser casi singular, generando coeficientes inestables. Para evitar esto, la regresión Ridge añade un término de penalización \\(\\lambda\\), de la siguiente manera (sin penalizar el intercepto \\(\\beta_0\\)):\n\\[\nSSE_{ridge} = \\| \\mathbf{y} - \\mathbf{X} \\, \\boldsymbol{\\beta} \\|^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2.\n\\]\nEste término adicional, es un término de penalización (\\(L_2=\\sum \\beta_j^2\\)) impone una restricción sobre los coeficientes, evitando que tomen valores excesivamente grandes. La estimación de \\(\\boldsymbol{\\beta}\\) en Ridge se obtiene resolviendo:\n\\[\n\\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = (\\mathbf{X}^T \\mathbf{X} + \\lambda \\, \\mathbf{P})^{-1}\n\\mathbf{X}^T \\mathbf{y}.\n\\]\ndonde \\(\\mathbf{P}\\) es diagonal con \\(P_{11}=0\\) (no penalizamos el intercepto) y \\(P_{jj}=1\\) para \\(j=2,\\dots,p+1\\), y \\(\\lambda \\geq 0\\) controla la cantidad de penalización aplicada. (Cuando no hay intercepto o se reparametriza, a menudo se escribe con \\(I\\) para simplificar.)\nInterpretación del parámetro \\(\\lambda\\)\n\nSi \\(\\lambda = 0\\), el modelo Ridge es equivalente a la regresión lineal tradicional (OLS).\nA medida que \\(\\lambda\\) aumenta, los coeficientes \\(\\beta_j\\) (pendientes) se reducen en magnitud, lo que ayuda a controlar la varianza del modelo y a prevenir el sobreajuste.\nSi \\(\\lambda\\) es demasiado grande, los coeficientes se acercan a cero y el modelo puede perder interpretabilidad.\n\nLa elección óptima de \\(\\lambda\\) se determina generalmente mediante validación cruzada.\n\n\n\n\n\n\nAviso\n\n\n\nLos detalles de la validación cruzada son tratados en la asignatura de Minería de Datos.\n\n\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n\nManejo de la multicolinealidad: La regularización reduce la sensibilidad del modelo cuando los predictores están altamente correlacionados.\nMenor varianza en las predicciones: El modelo Ridge tiende a ser más estable en comparación con OLS, lo que mejora la capacidad de generalización en conjuntos de datos nuevos.\nNo realiza selección de variables: A diferencia de Lasso, Ridge no anula coeficientes, sino que reduce su magnitud. Esto es útil cuando se sospecha que todas las variables tienen algún grado de importancia en el modelo.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar librerías\nsuppressPackageStartupMessages(library(glmnet))\n\n# Datos simulados\nset.seed(123)\nX &lt;- matrix(rnorm(100 * 10), 100, 10)  # 100 observaciones, 10 predictores\nY &lt;- X %*% rnorm(10) + rnorm(100)  # Variable de respuesta con ruido\n\n# Ajustar modelo Ridge\nmodelo_ridge &lt;- glmnet(X, Y, alpha = 0)  # alpha = 0 indica regresión Ridge\n\n# Seleccionar lambda óptimo con validación cruzada\ncv_ridge &lt;- cv.glmnet(X, Y, alpha = 0)\nlambda_optimo &lt;- cv_ridge$lambda.min  # Mejor valor de lambda\n\nprint(lambda_optimo)\n\n[1] 0.2583753\n\n# Ajustar modelo final con lambda óptimo\nmodelo_ridge_final &lt;- glmnet(X, Y, alpha = 0, lambda = lambda_optimo)\n\nmodelo_ridge_final\n\n\nCall:  glmnet(x = X, y = Y, alpha = 0, lambda = lambda_optimo) \n\n  Df  %Dev Lambda\n1 10 93.55 0.2584\n\n# Comparación modelo clásico\n\nmodelo_lm &lt;- lm(Y~X)\n\n# Mostrar coeficientes\noutput=cbind(round(coef(modelo_ridge_final),3),\n            round(coef(modelo_lm),3))\n\ncolnames(output)=c(\"RIDGE\",\"OLS\")\n\noutput\n\n11 x 2 sparse Matrix of class \"dgCMatrix\"\n             RIDGE    OLS\n(Intercept)  0.118  0.132\nV1          -0.874 -0.995\nV2          -1.019 -1.131\nV3           0.040  0.039\nV4           0.002  0.001\nV5          -2.500 -2.703\nV6           1.001  1.104\nV7           0.247  0.274\nV8           2.125  2.244\nV9           0.635  0.658\nV10         -0.390 -0.427\n\n\n\n\n\nLa regresión Ridge es una técnica poderosa para mejorar la estabilidad de los modelos de regresión en presencia de multicolinealidad. A diferencia de OLS, que puede generar coeficientes inestables, Ridge introduce una penalización que reduce la magnitud de los coeficientes, evitando valores extremos. Aunque Ridge no realiza selección de variables, su capacidad para reducir la varianza y mejorar la capacidad predictiva lo convierte en una herramienta esencial en el análisis de datos modernos.\nEn la siguiente sección, exploraremos la regresión Lasso, que extiende este concepto permitiendo la eliminación de variables irrelevantes del modelo.\n\n\n5.6.2 Regresión Lasso\nCuando se tiene un conjunto de predictores con posibles redundancias o ruido, Lasso permite identificar cuáles son las variables más relevantes para el modelo, lo que facilita la interpretación y reduce la complejidad del análisis.\nAl igual que en Ridge, el modelo de regresión Lasso se define sobre datos observados mediante la minimización (Ranstam y Cook 2018): \\[\nSSE_{\\text{lasso}} = \\| \\mathbf{y} - \\mathbf{X} \\, \\boldsymbol{\\beta} \\|^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\]\ndonde el término de penalización (\\(L_1=\\sum |\\beta_j|\\)) no penaliza el intercepto \\(\\beta_0\\) y hace que algunos coeficientes de pendiente se reduzcan exactamente a cero, eliminando variables del modelo.\nLa diferencia clave con Ridge Regressión, visto anteriormente, es que Ridge reduce la magnitud de los coeficientes pero no los anula, mientras que Lasso puede eliminar variables por completo.\nInterpretación del parámetro \\(\\lambda\\)\n\nSi \\(\\lambda = 0\\), el modelo es equivalente a la regresión lineal tradicional (OLS).\nA medida que \\(\\lambda\\) aumenta, más coeficientes de pendiente se reducen a cero, lo que equivale a realizar selección de variables.\nSi \\(\\lambda\\) es demasiado grande, se eliminan demasiadas variables, lo que puede resultar en un modelo subóptimo.\n\nAl igual que en el método Ridge, la selección óptima de \\(\\lambda\\) se realiza generalmente mediante validación cruzada.\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n\nSelección de variables automática: Lasso no solo regulariza, sino que también selecciona las variables más importantes eliminando aquellas menos relevantes.\nManejo de la multicolinealidad: Puede mejorar la interpretación del modelo cuando hay muchas variables correlacionadas.\nSimplicidad y interpretabilidad: Un modelo con menos variables es más fácil de interpretar y aplicar en la práctica.\nReduce el sobreajuste: La penalización \\(L_1\\) evita que el modelo se ajuste demasiado a los datos de entrenamiento, mejorando su capacidad predictiva en datos nuevos.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Ajustar modelo Lasso\nmodelo_lasso &lt;- glmnet(X, Y, alpha = 1)  # alpha = 1 indica regresión Lasso\n\n# Seleccionar lambda óptimo con validación cruzada\ncv_lasso &lt;- cv.glmnet(X, Y, alpha = 1)\nlambda_optimo &lt;- cv_lasso$lambda.min  # Mejor valor de lambda\n\nprint(lambda_optimo)\n\n[1] 0.03260326\n\n# Ajustar modelo final con lambda óptimo\nmodelo_lasso_final &lt;- glmnet(X, Y, alpha = 1, lambda = lambda_optimo)\n\n\n# Mostrar coeficientes\noutput=cbind(round(coef(modelo_lasso_final),3),output)\n\ncolnames(output)=c(\"LASSO\",\"RIDGE\",\"OLS\")\n\noutput\n\n11 x 3 sparse Matrix of class \"dgCMatrix\"\n             LASSO  RIDGE    OLS\n(Intercept)  0.131  0.118  0.132\nV1          -0.950 -0.874 -0.995\nV2          -1.078 -1.019 -1.131\nV3           0.006  0.040  0.039\nV4           .      0.002  0.001\nV5          -2.652 -2.500 -2.703\nV6           1.058  1.001  1.104\nV7           0.235  0.247  0.274\nV8           2.213  2.125  2.244\nV9           0.629  0.635  0.658\nV10         -0.392 -0.390 -0.427\n\n\n\n\n\nConsideraciones Importantes\nLa regresión Lasso es una poderosa técnica de regularización que no solo mejora la estabilidad del modelo en presencia de muchas variables predictoras, sino que también realiza una selección automática de las más relevantes. Su capacidad para reducir coeficientes a cero la convierte en una herramienta esencial en el análisis de datos de alta dimensión.\n\nLasso puede eliminar demasiadas variables si \\(\\lambda\\) es demasiado grande, lo que puede llevar a la pérdida de información importante.\nNo maneja bien grupos de predictores altamente correlacionados, ya que selecciona solo uno de ellos y elimina los demás.\nElastic Net, que combina Ridge y Lasso, puede ser una mejor opción cuando hay multicolinealidad fuerte en los datos.\n\nEn la siguiente sección, exploraremos Elastic Net, una técnica híbrida que combina las ventajas de Ridge y Lasso para mejorar la selección de variables en presencia de predictores altamente correlacionados.\n\n\n5.6.3 Elastic Net\nLa regresión Elastic Net es una técnica de regularización que combina las propiedades de Ridge y Lasso, abordando algunas de sus limitaciones individuales (Zou y Hastie 2005). Mientras que Ridge es útil para manejar la multicolinealidad sin eliminar variables y Lasso selecciona un subconjunto de predictores, Elastic Net equilibra ambos enfoques permitiendo la selección de variables en presencia de alta correlación entre los predictores.\nEste método es particularmente efectivo cuando el número de predictores es grande y existe multicolinealidad, ya que permite controlar simultáneamente la reducción de la magnitud de los coeficientes y la eliminación de variables irrelevantes.\nElastic Net introduce una penalización que combina los términos de Ridge (\\(L_2\\)) y Lasso (\\(L_1\\)), sobre datos observados:\n\\[\nSSE_{\\text{Elastic Net}} = \\| \\mathbf{y} - \\mathbf{X} \\, \\boldsymbol{\\beta} \\|^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2 \\]\ndonde:\n\n\\(\\lambda_1\\) (asociado a Lasso) controla la cantidad de coeficientes que se reducen a cero.\n\\(\\lambda_2\\) (asociado a Ridge) controla la reducción de magnitud de los coeficientes sin anularlos.\n\\(\\alpha\\) es un parámetro adicional que pondera la combinación entre Lasso y Ridge, con:\n\n\\(\\alpha = 1\\) → Elastic Net se comporta como Lasso.\n\\(\\alpha = 0\\) → Elastic Net se comporta como Ridge.\n\\(0 &lt; \\alpha &lt; 1\\) → Elastic Net combina ambos métodos.\n\n\nLa estimación de los coeficientes en Elastic Net se obtiene resolviendo (habitualmente sin penalizar el intercepto):\n\\[\n\\hat{\\boldsymbol{\\beta}}_{\\text{EN}} = \\arg \\min_{\\boldsymbol{\\beta}} \\left( \\| \\mathbf{y} - \\mathbf{X} \\, \\boldsymbol{\\beta} \\|^2 + \\lambda \\left( \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1 - \\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right) \\right)\n\\]\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n\nManejo de la Multicolinealidad: A diferencia de Lasso, que selecciona solo una de las variables correlacionadas y elimina las demás, Elastic Net distribuye la penalización entre todas las variables correlacionadas, evitando una selección arbitraria.\nSelección de variables más estable: La combinación de Lasso y Ridge permite una selección más robusta, manteniendo información relevante del modelo sin eliminar predictores clave.\nMejora del rendimiento predictivo: Al utilizar validación cruzada para seleccionar los hiperparámetros \\(\\lambda_1\\), \\(\\lambda_2\\) y \\(\\alpha\\), se optimiza la capacidad del modelo para generalizar a nuevos datos.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Ajustar modelo Elastic Net\nmodelo_elastic_net &lt;- glmnet(X, Y, alpha = 0.5)  # Alpha = 0.5 (50% Ridge, 50% Lasso)\n\n# Seleccionar lambda óptimo con validación cruzada\ncv_elastic_net &lt;- cv.glmnet(X, Y, alpha = 0.5)\nlambda_optimo &lt;- cv_elastic_net$lambda.min  # Mejor valor de lambda\n\nprint(lambda_optimo)\n\n[1] 0.0213522\n\n# Ajustar modelo final con lambda óptimo\nmodelo_elastic_final &lt;- glmnet(X, Y, alpha = 0.5, lambda = lambda_optimo)\n\n# Mostrar coeficientes\noutput=cbind(round(coef(modelo_elastic_final),3),output)\n\ncolnames(output)=c(\"ELASTIC\",\"LASSO\",\"RIDGE\",\"OLS\")\n\noutput\n\n11 x 4 sparse Matrix of class \"dgCMatrix\"\n            ELASTIC  LASSO  RIDGE    OLS\n(Intercept)   0.131  0.131  0.118  0.132\nV1           -0.975 -0.950 -0.874 -0.995\nV2           -1.108 -1.078 -1.019 -1.131\nV3            0.028  0.006  0.040  0.039\nV4            .      .      0.002  0.001\nV5           -2.677 -2.652 -2.500 -2.703\nV6            1.084  1.058  1.001  1.104\nV7            0.260  0.235  0.247  0.274\nV8            2.229  2.213  2.125  2.244\nV9            0.647  0.629  0.635  0.658\nV10          -0.414 -0.392 -0.390 -0.427\n\n\n\n\n\nPara determinar el mejor valor de \\(\\alpha\\), se usa validación cruzada probando distintos valores entre \\(0\\) y 1. Algunas estrategias comunes incluyen:\n\nSi hay muchas variables irrelevantes, se recomienda \\(\\alpha\\) cercano a 1 (Lasso).\nSi hay fuerte multicolinealidad, se recomienda \\(\\alpha\\) cercano a 0 (Ridge).\nSi se desea un balance entre selección y estabilidad, se suele usar \\(\\alpha = 0.5\\).\n\nLa regresión Elastic Net combina lo mejor de Ridge y Lasso, ofreciendo un método de regularización robusto para modelos con muchas variables predictoras y posible multicolinealidad. Su capacidad para seleccionar variables sin eliminar información clave lo convierte en una opción ideal para modelos complejos y de alta dimensionalidad.\n\n\n5.6.4 Comparación de los métodos de Regularización\n\n\n\n\n\n\n\n\nMétodo\nPenalización\nEfecto sobre los coeficientes\n\n\n\n\nOLS\nNinguna\nSin restricción, puede haber multicolinealidad\n\n\nRidge\n\\(L_2\\)\nReduce la magnitud de los coeficientes, pero no los anula\n\n\nLasso\n\\(L_1\\)\nPuede anular coeficientes, permitiendo selección de variables\n\n\nElastic Net\n\\(L_1 + L_2\\)\nCombinación de Ridge y Lasso\n\n\n\n\nLasso es especialmente útil cuando se sospecha que muchas variables son irrelevantes, mientras que Ridge es preferido cuando se espera que todas las variables aporten información al modelo.\nElastic Net es ideal cuando hay muchas variables correlacionadas y se desea un modelo estable y parsimonioso.\n\nElastic Net mejora la estabilidad del modelo en comparación con Lasso, especialmente cuando hay variables predictoras altamente correlacionadas.\nEs más flexible que Ridge y Lasso individualmente, permitiendo un ajuste más fino a distintos tipos de problemas.\nRequiere la selección de hiperparámetros (\\(\\lambda\\) y \\(\\alpha\\)), por lo que debe usarse validación cruzada para encontrar la combinación óptima.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables, regularización y validación</span>"
    ]
  },
  {
    "objectID": "tema4.html#validación-del-modelo",
    "href": "tema4.html#validación-del-modelo",
    "title": "5  Selección de variables, regularización y validación",
    "section": "5.7 Validación del Modelo",
    "text": "5.7 Validación del Modelo\nHemos ajustado un modelo, interpretado sus coeficientes y evaluado su significancia estadística. Pero, ¿cómo podemos estar seguros de que funcionará bien en el futuro, con datos que nunca ha visto? Esta es la pregunta fundamental que la validación del modelo busca responder.\nImagina que estás preparando un examen. Si solo memorizas las respuestas de los exámenes de años anteriores (tus datos de entrenamiento), puede que saques una nota perfecta en ellos. Sin embargo, cuando te enfrentes al examen real con preguntas nuevas (los datos de prueba), es probable que tu rendimiento sea decepcionante. Esto, en esencia, es el sobreajuste (overfitting): un modelo que se aprende los datos de entrenamiento “de memoria”, incluyendo su ruido y peculiaridades, pero que pierde su capacidad de generalizar a nuevas observaciones.\nLa validación es el proceso de simular este “examen final” para obtener una estimación honesta del rendimiento predictivo de nuestro modelo en el mundo real (James et al. 2013). Se compone de dos elementos clave: las estrategias de validación, que nos dicen cómo simular el examen, y las métricas de evaluación, que nos dicen cómo calificarlo.\n\n5.7.1 Estrategias de Validación\nPara evaluar la capacidad de generalización, necesitamos probar el modelo en datos que no se usaron para entrenarlo. Las siguientes estrategias nos permiten hacer precisamente eso.\n\n\n\n\n\n\nEl primer paso no negociable: La partición inicial\n\n\n\nAntes de escribir una sola línea de código para ajustar un modelo, seleccionar variables o ejecutar una validación cruzada, el procedimiento siempre debe comenzar con una única acción:\nDividir el conjunto de datos completo en dos partes y guardar una de ellas bajo llave.\n\nDatos de modelado (p. ej., 80% del total): Este es el conjunto de datos con el que trabajarás. Lo usarás para todas tus tareas de construcción y evaluación de modelos: entrenar, comparar diferentes conjuntos de variables, y ejecutar estrategias como la validación cruzada.\nConjunto de prueba Final (p. ej., 20% restante): Este conjunto de datos debe ser guardado y no ser utilizado bajo ninguna circunstancia durante el proceso de modelado. Es tu “examen final sorpresa”, tu única oportunidad de obtener una estimación verdaderamente honesta y no sesgada del rendimiento del modelo que has seleccionado como el campeón definitivo.\n\nLa validación cruzada y la división simple train/test que veremos a continuación son técnicas que se aplican dentro de los “datos de modelado”.\n\n\n\n5.7.1.1 El Conjunto de entrenamiento y test (Train/Test Split)\nLa estrategia más directa es tomar nuestros “Datos de Modelado” (el 80% inicial) y volver a dividirlos, creando un único “examen” para el proceso de construcción del modelo (Hastie et al. 2009):\n\nConjunto de entrenamiento (Training Set): Usualmente, el 70-80% de los datos. El modelo se construye y se ajusta usando únicamente esta porción. Es aquí donde el modelo “aprende”.\nConjunto de test (Test Set): El 20-30% restante. Estos datos se mantienen “ocultos” durante el entrenamiento. Una vez que el modelo está finalizado, lo usamos para predecir la variable respuesta en este conjunto. La comparación entre las predicciones (\\(\\hat{y}\\)) y los valores reales (\\(y\\)) nos da una medida no sesgada de su rendimiento.\n\nAunque es simple y computacionalmente barata, esta técnica tiene una debilidad importante: los resultados pueden depender mucho de la división aleatoria específica que se haya hecho. Si por mala suerte en el conjunto de prueba caen observaciones muy atípicas, nuestra evaluación del modelo será excesivamente pesimista. Si caen puntos muy fáciles de predecir, será demasiado optimista. Esta alta variabilidad es un problema, especialmente con muestras de datos pequeñas.\n\n\n5.7.1.2 Validación cruzada (Cross-Validation)\nLa validación cruzada es la solución a la variabilidad de la división simple y se aplica, de nuevo, sobre el conjunto total de “datos de modelado”. En lugar de hacer un único “examen final”, la validación cruzada promedia los resultados de múltiples mini-exámenes, proporcionando una estimación del error mucho más estable y fiable (James et al. 2013).\nEl método más común es la validación cruzada de k-particiones (k-fold cross-validation). Su nombre describe el proceso: los datos se dividen en k particiones y se “cruzan” los roles de entrenamiento y validación.\nEl resultado final de este procedimiento es el error de validación cruzada, que se calcula promediando los errores obtenidos en cada una de las k particiones. Esto nos da una única métrica de rendimiento para el modelo.\n\\[CV_{(k)} = \\frac{1}{k}\\sum_{i=1}^{k} \\text{Métrica}_i\\]\ndonde \\(\\text{Métrica}_i\\) es la métrica de error (como RMSE o MAE) calculada en la i-ésima iteración. La elección de k suele ser 5 o 10, ya que se ha demostrado que estos valores ofrecen un buen equilibrio entre el sesgo y la varianza de la estimación del error.\n\n\n\n\n\n\nProcedimiento de k-particiones\n\n\n\n\nDivisión: Dividir aleatoriamente los datos en k particiones de tamaño similar.\nIteración: Para cada partición \\(i = 1, 2, ..., k\\):\n\nUsar la partición \\(i\\) como conjunto de test.\nUsar las restantes \\(k-1\\) particiones como conjunto de entrenamiento.\nAjustar el modelo y calcular las métricas de desempeño en el conjunto de test.\n\nPromedio: Calcular el promedio de las métricas a través de las k iteraciones.\n\n\n\nUn caso extremo de este método es la validación cruzada “dejando uno fuera” (LOOCV), donde k es igual al número de observaciones, n. En cada iteración, se entrena el modelo con n-1 datos y se prueba en el único punto restante. Aunque es computacionalmente muy costoso, en regresión lineal existe una afortunada fórmula matemática que nos permite calcular el error LOOCV con la misma rapidez que un solo ajuste:\n\\[CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\frac{y_i - \\hat{y}_i}{1-h_{ii}}\\right)^2\\]\ndonde \\(h_{ii}\\) es el apalancamiento (leverage) de la i-ésima observación.\n\n\n\n\n\n\nGuía para seleccionar estrategia de validación\n\n\n\nUsa Train/Test cuando:\n\nEl dataset es grande (n &gt; 1000)\nLos recursos computacionales son limitados\n\nSe requiere una evaluación rápida\n\nUsa validación cruzada k-fold cuando:\n\nEl dataset es de tamaño moderado (n &lt; 1000)\nSe requiere una estimación más estable del desempeño\nSe dispone de recursos computacionales adecuados\n\nUsa LOOCV cuando:\n\nEl dataset es pequeño (n &lt; 100)\nSe requiere la estimación menos sesgada posible\nEl tiempo computacional no es una restricción crítica\n\n\n\n\n\n\n5.7.2 Métricas de rendimiento\nUna vez que usamos una estrategia de validación para generar predicciones sobre datos no vistos, necesitamos una “nota” para cuantificar qué tan buenos fueron esos pronósticos. Aquí es donde entran las métricas de error.\n\n5.7.2.1 Raíz del error cuadrático medio\nLa métrica más utilizada es la raíz del error cuadrático medio (RMSE). Es como una desviación típica de los residuos, y nos da una idea de la magnitud promedio de los errores de predicción.\n\\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\]\nLa característica clave del RMSE es que, al elevar los errores al cuadrado, penaliza de forma desproporcionada los errores grandes. Un solo error de predicción de 10 unidades contribuye al RMSE mucho más que 10 errores de 1 unidad. Esto lo hace muy sensible a valores atípicos. Su gran ventaja es que se expresa en las mismas unidades que la variable respuesta, facilitando su interpretación.\n\n\n5.7.2.2 Error absoluto medio\nUna alternativa popular es el error absoluto medio (MAE), que simplemente promedia el valor absoluto de los errores.\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\\]\nA diferencia del RMSE, el MAE no eleva los errores al cuadrado, por lo que trata todos los errores de forma proporcional a su magnitud. Un error de 10 unidades es simplemente el doble de malo que un error de 5. Esto hace que el MAE sea más robusto frente a valores atípicos y, para muchos, más fácil de interpretar como “el error promedio” que cometemos en nuestras predicciones.\nEn resumen, la validación nos obliga a confrontar nuestro modelo con la realidad de datos nuevos. Usando una estrategia robusta como la validación cruzada para calcular una métrica interpretable como el RMSE o el MAE, podemos obtener una estimación fiable de su rendimiento predictivo y construir modelos en los que realmente podamos confiar. Claro, aquí tienes ese contenido reorganizado y resumido dentro de un recuadro (callout), ideal para destacar esta idea clave en tu libro.\n\n\n\n\n\n\nInterpretando el Error\n\n\n\nLa comparación entre el error del modelo en los datos que ha visto (entrenamiento) y en datos que no ha visto (validación) es la herramienta de diagnóstico más importante para entender el ajuste del modelo.\nLa regla general es que el error de entrenamiento siempre será más bajo (más optimista) que el de test. La clave está en analizar la diferencia entre ambos.\nSobreajuste (Overfitting)\n\nSíntoma: Error de entrenamiento bajo + Error de test mucho más alto.\nDiagnóstico: El modelo ha “memorizado” el ruido de los datos de entrenamiento y no es capaz de generalizar a nuevos datos.\nSolución: Simplificar el modelo (usar menos variables, aplicar regularización como Ridge o Lasso).\n\nSubajuste (Underfitting)\n\nSíntoma: Error de entrenamiento alto + Error de test alto y similar.\nDiagnóstico: El modelo es demasiado simple y no tiene la capacidad de capturar la estructura subyacente de los datos.\nSolución: Aumentar la complejidad del modelo (añadir más variables, incluir interacciones o términos no lineales).\n\n\n\n\n\n\n\n\n\nLa maldición del sobreajuste\n\n\n\n\n\nPara ilustrar por qué la validación es indispensable, realizaremos un experimento controlado. Crearemos un conjunto de datos donde conocemos la verdad: sabemos exactamente qué variables influyen en la respuesta y cuáles son puro ruido. Luego, compararemos dos modelos:\n\nModelo Completo: Un modelo que incluye todas las variables disponibles, tanto las útiles como las de ruido.\nModelo Correcto: Un modelo que incluye únicamente las variables que realmente tienen un efecto sobre la respuesta.\n\nEl objetivo es ver cuál de los dos modelos predice mejor en datos “no vistos”, utilizando la validación cruzada para simular este escenario.\n1. Simulación de Datos\nCreamos un dataset con 100 observaciones. La variable y dependerá de X1, X2 y X3. Las variables X4 a X10 no tendrán ninguna relación real con y; serán predictores de ruido.\n\n# Cargar la librería 'caret', que simplifica enormemente el proceso de validación\nsuppressPackageStartupMessages(library(caret))\n\n# Para reproducibilidad\nset.seed(42)\n\n# Crear datos de ejemplo\nn &lt;- 100\n# 3 predictores verdaderos y 7 de ruido\nX &lt;- matrix(rnorm(n * 10), n, 10)\ncolnames(X) &lt;- paste0(\"X\", 1:10)\n\n# La respuesta 'y' depende SOLO de X1, X2 y X3\nbeta_true &lt;- c(2.5, -1.5, 3, 0, 0, 0, 0, 0, 0, 0) \ny &lt;- X %*% beta_true + rnorm(n, sd = 2)\n\n# Combinar en un data frame\ndatos &lt;- data.frame(y = y, X)\n\nindices_modelado &lt;- createDataPartition(datos$y, p = 0.8, list = FALSE)\ndatos_modelado &lt;- datos[indices_modelado, ]\ndatos_prueba_final &lt;- datos[-indices_modelado, ]\n\n2. Ajuste y Evaluación de Modelos con Validación Cruzada\nUsaremos la función train() del paquete caret, que es una herramienta increíblemente potente para ajustar y validar modelos. Configuraremos una validación cruzada de 10 particiones (10-fold CV) para estimar el error de predicción (RMSE) de nuestros dos modelos.\n\n# Configurar el método de validación cruzada de 10 particiones\ncontrol_cv &lt;- trainControl(method = \"cv\", number = 10)\n\n# Ajustar y validar el MODELO COMPLETO (incluye predictores de ruido)\nmodelo_completo &lt;- train(\n  y ~ ., \n  data = datos_modelado, \n  method = \"lm\",\n  trControl = control_cv\n)\n\n# Ajustar y validar el MODELO CORRECTO (solo los predictores relevantes)\nmodelo_correcto &lt;- train(\n  y ~ X1 + X2 + X3, \n  data = datos_modelado, \n  method = \"lm\",\n  trControl = control_cv\n)\n\n# Comparar los resultados de la validación cruzada de ambos modelos\nresultados_cv &lt;- resamples(list(COMPLETO = modelo_completo, CORRECTO = modelo_correcto))\nresumen_cv &lt;- summary(resultados_cv)\nresumen_cv\n\n\nCall:\nsummary.resamples(object = resultados_cv)\n\nModels: COMPLETO, CORRECTO \nNumber of resamples: 10 \n\nMAE \n              Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nCOMPLETO 0.8638906 1.284897 1.585880 1.685953 1.895840 2.783321    0\nCORRECTO 0.7192667 1.279226 1.602489 1.687326 2.283342 2.477780    0\n\nRMSE \n              Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nCOMPLETO 1.0722692 1.592663 2.105678 2.116903 2.523848 3.119017    0\nCORRECTO 0.9052294 1.474694 1.934304 2.020464 2.480041 3.124041    0\n\nRsquared \n              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nCOMPLETO 0.5917775 0.7793610 0.8556182 0.8189137 0.8882997 0.9505874    0\nCORRECTO 0.5019737 0.7988599 0.8513481 0.8017851 0.9048191 0.9385882    0\n\n# Extraer métricas RMSE para uso en el texto\nrmse_correcto &lt;- round(resumen_cv$statistics$RMSE[\"CORRECTO\", \"Mean\"], 3)\nrmse_completo &lt;- round(resumen_cv$statistics$RMSE[\"COMPLETO\", \"Mean\"], 3)\n\n3. Análisis de Resultados\nAl comparar el RMSE promedio obtenido en la validación cruzada, la conclusión es clara: el Modelo Correcto (2.02) es consistentemente mejor (menor error) que el Modelo Completo (2.117).\nEl Modelo Completo sufre de sobreajuste. Al incluir las 7 variables de ruido, se esfuerza por encontrar patrones en datos puramente aleatorios. “Aprende” estas relaciones falsas en los datos de entrenamiento, pero falla al predecir en datos nuevos. El Modelo Correcto, al ser más parsimonioso, captura la estructura fundamental y generaliza mejor.\nEste ejemplo demuestra la lección más importante del modelado: un buen ajuste en los datos de entrenamiento no garantiza un buen rendimiento predictivo. La validación es el único método fiable para estimar la verdadera calidad de un modelo.\n4. El Veredicto Final en el Conjunto de Prueba\nLa validación cruzada nos ha servido como un juez imparcial para comparar nuestros modelos candidatos y seleccionar el Modelo Correcto como el claro ganador. Ahora, para obtener una estimación final y no sesgada de su rendimiento en el mundo real, tomamos ese modelo elegido y lo enfrentamos a los datos_prueba_final, el conjunto de datos que ha permanecido intacto durante todo el proceso.\n\n# Usamos el modelo ganador (modelo_correcto) para predecir sobre los datos de prueba\npredicciones_finales &lt;- predict(modelo_correcto, newdata = datos_prueba_final)\n\n# Calculamos el RMSE final comparando las predicciones con los valores reales de prueba\nrmse_final &lt;- RMSE(predicciones_finales, datos_prueba_final$y)\n\nAl evaluar nuestro modelo final, obtenemos un RMSE en el conjunto de prueba de 1.966.\nEste valor es nuestra estimación más honesta del error de predicción que podemos esperar de nuestro modelo al enfrentarse a nuevos datos. Es crucial compararlo con el error que estimamos durante la validación cruzada (2.02). El hecho de que ambos valores sean muy similares confirma que nuestro proceso de validación fue robusto y que no hemos sobreajustado el modelo al conjunto de datos de modelado. Este RMSE final es el que reportaríamos como la medida definitiva del rendimiento predictivo de nuestro modelo.\n\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, y Jerome H Friedman. 2009. The elements of statistical learning: data mining, inference, and prediction. Vol. 2. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An introduction to statistical learning. Vol. 112. Springer.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.\n\n\nMarquardt, Donald W, y Ronald D Snee. 1975. «Ridge regression in practice». The American Statistician 29 (1): 3-20.\n\n\nRanstam, Jonas, y Jonathan A Cook. 2018. «LASSO regression». Journal of British Surgery 105 (10): 1348-48.\n\n\nZou, Hui, y Trevor Hastie. 2005. «Regularization and variable selection via the elastic net». Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (2): 301-20.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables, regularización y validación</span>"
    ]
  },
  {
    "objectID": "tema5.html",
    "href": "tema5.html",
    "title": "6  Modelos de regresión generalizada",
    "section": "",
    "text": "6.1 Introducción a los GLM\nHasta ahora hemos estuadiado la regresión lineal como una herramienta poderosa para modelar la relación entre una variable dependiente continua y un conjunto de variables independientes. Sin embargo, en muchos contextos del mundo real, las suposiciones de la regresión lineal tradicional no son adecuadas. ¿Qué sucede si la variable dependiente es binaria, como en un diagnóstico médico (enfermo/sano)? ¿O si estás modelando el número de accidentes en una intersección o la cantidad de compras realizadas por un cliente?\nPara abordar estos desafíos, se utilizan los llamados Modelos Lineales Generalizados (GLM). Esta clase de modelos amplía la regresión lineal al permitir que la variable dependiente tenga distribuciones diferentes a la normal, como la binomial o la de Poisson. Además, los GLM utilizan funciones de enlace que transforman la relación entre la variable dependiente y los predictores, permitiendo una mayor flexibilidad en el modelado.\nAlgunos de los modelos más comunes dentro de los GLM son:\nEn este tema, exploraremos cómo utilizar estos modelos para resolver problemas del mundo real, interpretar sus resultados y evaluar su ajuste.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema5.html#introducción-a-los-glm",
    "href": "tema5.html#introducción-a-los-glm",
    "title": "6  Modelos de regresión generalizada",
    "section": "",
    "text": "6.1.1 ¿Qué son los modelos lineales generalizados?\nLos Modelos Lineales Generalizados (GLM) son una extensión de los modelos de regresión lineal que permiten manejar una mayor variedad de tipos de datos y relaciones entre variables (Nelder y Wedderburn 1972). Mientras que la regresión lineal tradicional asume que la variable dependiente es continua y sigue una distribución normal, los GLM permiten trabajar con variables dependientes que:\n\nSon binarias (como éxito/fracaso o sí/no).\nRepresentan conteos de eventos (número de llamadas, accidentes, etc.).\nSon continuas positivas y no siguen una distribución normal (como tiempos o costos).\n\nLos GLM proporcionan una estructura flexible para modelar la relación entre una o más variables independientes y una variable dependiente que sigue alguna distribución de la familia exponencial (binomial, Poisson, gamma, entre otras).\n\n\n6.1.2 Componentes de un modelo lineal generalizado\nUn GLM se define por tres componentes clave:\n\nComponente Aleatorio:\nEste componente describe la distribución de la variable dependiente. En la regresión lineal, la variable dependiente sigue una distribución normal. En los GLM, puede seguir otras distribuciones de la familia exponencial, como:\n\nDistribución Binomial: Para variables categóricas binarias (0/1, éxito/fracaso).\nDistribución de Poisson: Para datos de conteo (número de eventos).\nDistribución Gamma: Para variables continuas y positivas (como costos o tiempos).\n\nComponente Sistemático:\nEste componente describe cómo las variables independientes se combinan linealmente en el modelo. Se define como:\n\\[\n\\eta = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\nDonde \\(\\eta\\) es el predictor lineal y \\(\\boldsymbol{\\beta}\\) representa los coeficientes del modelo.\nFunción de Enlace:\nLa función de enlace conecta el componente sistemático con la media de la variable dependiente. Mientras que en la regresión lineal la relación es directa (\\(y = \\eta\\)), en los GLM se utiliza una función de enlace \\(g(\\mu)\\) para transformar la media \\(\\mu\\) y ajustar diferentes tipos de datos.\n\\[\ng(\\mu) = \\eta\n\\]\n\nEjemplos de funciones de enlace:\n\nLogística (Logit): Para la regresión logística, que modela la probabilidad de un evento. \\[\ng(\\mu) = \\log\\left(\\frac{\\mu}{1 - \\mu}\\right)\n\\]\nLogarítmica: Para la regresión de Poisson, que modela tasas de eventos. \\[\ng(\\mu) = \\log(\\mu)\n\\]\nIdentidad: Para la regresión lineal estándar. \\[\ng(\\mu) = \\mu\n\\]\n\n\n\n\n\n\n\nAplicaciones\n\n\n\n\n\nLos GLM se utilizan en una amplia variedad de disciplinas para resolver problemas del mundo real:\nRegresión Logística (para variables binarias):\n\nMedicina: Predicción de la presencia o ausencia de una enfermedad basada en factores de riesgo.\nMarketing: Determinación de la probabilidad de que un cliente compre un producto.\nFinanzas: Evaluación de la probabilidad de incumplimiento de pago de un préstamo.\n\nRegresión de Poisson (para datos de conteo):\n\nTransporte: Modelado del número de accidentes en una carretera en un período de tiempo.\nEcología: Conteo de especies en un área determinada.\nTelecomunicaciones: Número de llamadas recibidas por un centro de atención.\n\nRegresión Binomial Negativa (para conteos con sobredispersión):\n\nSalud Pública: Modelado del número de visitas al médico o incidentes de una enfermedad en una población.\n\nModelos Gamma (para variables continuas positivas):\n\nSeguros: Estimación de los costos de reclamos de seguros.\nIngeniería: Modelado de tiempos de falla en procesos industriales.\n\n\n\n\n\n\n6.1.3 Diferencias clave entre la regresión lineal y los GLM\n\n\n\n\n\n\n\n\nCaracterística\nRegresión Lineal\nModelos Lineales Generalizados (GLM)\n\n\n\n\nDistribución de la variable dependiente\nNormal\nFamilia exponencial (binomial, Poisson, gamma, etc.)\n\n\nTipo de variable dependiente\nContinua\nBinaria, de conteo, continua positiva\n\n\nRelación entre las variables\nLineal directa\nRelación transformada mediante una función de enlace\n\n\nFunción de Enlace\nIdentidad (\\(g(\\mu) = \\mu\\))\nLogit, logarítmica, inversa, etc.\n\n\n\n\nLas ventajas principales de los GLM son:\n\nFlexibilidad: Los GLM permiten modelar diferentes tipos de variables dependientes, lo que amplía significativamente el rango de problemas que se pueden abordar.\nInterpretación Coherente: Aunque se utilizan funciones de enlace, los coeficientes de los GLM pueden interpretarse de manera similar a los modelos lineales, proporcionando información sobre el impacto de cada variable independiente.\nEvaluación Estadística Robusta: Los GLM permiten la realización de pruebas de hipótesis, la construcción de intervalos de confianza y la evaluación de la bondad del ajuste mediante medidas ya conocidas como el AIC o el BIC.\n\nLos Modelos Lineales Generalizados amplían el alcance de la regresión lineal clásica, proporcionando herramientas para modelar una amplia variedad de tipos de datos, desde variables binarias hasta datos de conteo y variables continuas no normales. A través del uso de funciones de enlace y distribuciones flexibles, los GLM permiten resolver problemas complejos del mundo real en campos tan diversos como la medicina, el marketing, la ingeniería y las ciencias sociales.\nEn las próximas secciones, exploraremos en detalle cómo aplicar estos modelos específicos, como la regresión logística y la regresión de Poisson, y cómo interpretar sus resultados en diferentes contextos.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema5.html#estimación-de-parámetros-en-glm",
    "href": "tema5.html#estimación-de-parámetros-en-glm",
    "title": "6  Modelos de regresión generalizada",
    "section": "6.2 Estimación de parámetros en GLM",
    "text": "6.2 Estimación de parámetros en GLM\nLa estimación de parámetros en los Modelos Lineales Generalizados representa un aspecto fundamental que diferencia estos modelos de la regresión lineal clásica. Mientras que en la regresión lineal utilizamos mínimos cuadrados ordinarios para obtener estimadores con propiedades óptimas, en los GLM necesitamos métodos más sofisticados debido a la naturaleza no normal de las distribuciones involucradas y las funciones de enlace no lineales.\nLa estimación en GLM se basa en el principio de máxima verosimilitud, que proporciona un marco teórico unificado para todos los modelos de la familia exponencial. Este enfoque no solo garantiza propiedades estadísticas deseables de los estimadores, sino que también permite el desarrollo de algoritmos computacionales eficientes para encontrar las soluciones.\nEn esta sección exploraremos los fundamentos teóricos de la estimación por máxima verosimilitud, el algoritmo iterativo IRLS (Iteratively Reweighted Least Squares) que implementan los software estadísticos, y los problemas prácticos que pueden surgir durante el proceso de estimación. Comprender estos aspectos es crucial para interpretar correctamente los resultados y diagnosticar posibles problemas en el ajuste de los modelos.\n\n6.2.1 Método de máxima verosimilitud\nA diferencia de la regresión lineal que utiliza el método de mínimos cuadrados, los GLM emplean el método de máxima verosimilitud para estimar los parámetros del modelo. Este cambio metodológico es necesario debido a que las distribuciones de la familia exponencial no siempre tienen una relación lineal directa con los predictores, y además porque la varianza de la variable respuesta depende de su media, violando el supuesto de homocedasticidad que requieren los mínimos cuadrados.\nEl principio de máxima verosimilitud consiste en encontrar los valores de los parámetros \\(\\boldsymbol{\\beta}\\) que hacen más probable observar los datos que tenemos. Para una muestra de \\(n\\) observaciones independientes \\(y_1, y_2, \\ldots, y_n\\), la función de verosimilitud se define como la probabilidad conjunta de observar estos datos dado un conjunto de parámetros:\n\\[L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} f(y_i; \\theta_i, \\phi)\\]\ndonde \\(f(y_i; \\theta_i, \\phi)\\) es la función de densidad (o masa) de probabilidad de la observación \\(i\\). En la práctica, es más conveniente trabajar con el logaritmo de esta función, conocida como log-verosimilitud:\n\\[\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\log f(y_i; \\theta_i, \\phi)\\]\nLa ventaja de usar el logaritmo es que convierte productos en sumas, simplificando considerablemente los cálculos matemáticos y numéricos.\nLa clave para entender los GLM radica en reconocer que todas las distribuciones que podemos usar (binomial, Poisson, gamma, etc.) pertenecen a la familia exponencial. Estas distribuciones pueden expresarse en una forma matemática unificada:\n\\[f(y; \\theta, \\phi) = \\exp\\left\\{\\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi)\\right\\}\\]\ndonde \\(\\theta\\) es el parámetro natural o canónico que está relacionado directamente con la media de la distribución, \\(\\phi\\) es el parámetro de dispersión que controla la variabilidad, y \\(b(\\theta)\\), \\(a(\\phi)\\) y \\(c(y, \\phi)\\) son funciones específicas de cada distribución que determinan sus propiedades particulares.\nEsta forma unificada tiene propiedades matemáticas muy convenientes que hacen que los GLM sean tanto elegantes teóricamente como computacionalmente eficientes. La esperanza de \\(Y\\) se obtiene como \\(E(Y) = \\mu = b'(\\theta)\\) (la derivada de \\(b\\) respecto a \\(\\theta\\)), y la varianza como \\(\\text{Var}(Y) = a(\\phi) b''(\\theta) = a(\\phi) V(\\mu)\\), donde \\(V(\\mu)\\) es la función de varianza que caracteriza cómo la varianza depende de la media en cada tipo de distribución.\n\n\n\n\n\n\nLa Familia Exponencial: Un Vistazo General\n\n\n\nLa elegancia de los GLM reside en que muchas distribuciones aparentemente distintas comparten una estructura matemática común. Esto permite una teoría unificada. Aquí están los miembros más importantes:\n\n\n\n\n\n\n\n\n\nDistribución\nUso Típico\nFunción de Varianza \\(V(\\mu)\\)\nEnlace Canónico \\(g(\\mu)\\)\n\n\n\n\nNormal\nDatos continuos simétricos\n\\(1\\)\nIdentidad: \\(\\mu\\)\n\n\nBinomial\nProporciones, datos binarios (éxito/fracaso)\n\\(\\mu(1-\\mu)\\)\nLogit: \\(\\log(\\frac{\\mu}{1-\\mu})\\)\n\n\nPoisson\nConteos de eventos\n\\(\\mu\\)\nLog: \\(\\log(\\mu)\\)\n\n\nGamma\nDatos continuos positivos y asimétricos (tiempos, costos)\n\\(\\mu^2\\)\nInverso: \\(1/\\mu\\)\n\n\nInversa Gaussiana\nTiempos hasta un evento, datos muy asimétricos\n\\(\\mu^3\\)\nInverso al cuadrado: \\(1/\\mu^2\\)\n\n\n\nLa función de varianza \\(V(\\mu)\\) es la “firma” de cada distribución, ya que define la relación teórica entre la media y la varianza de la respuesta. El enlace canónico es la función de enlace que surge de forma natural de la estructura matemática de la distribución, aunque en la práctica se pueden usar otros enlaces.\n\n\nEsta relación entre media y varianza es fundamental para entender las diferencias entre los diversos GLM. En la regresión lineal clásica, la varianza es constante (\\(V(\\mu) = 1\\)), pero en otros GLM la función de varianza toma formas específicas:\n\nDistribución binomial: \\(V(\\mu) = \\mu(1-\\mu)\\) - la varianza es máxima cuando \\(\\mu = 0.5\\) y mínima en los extremos.\nDistribución de Poisson: \\(V(\\mu) = \\mu\\) - la varianza aumenta linealmente con la media.\nDistribución gamma: \\(V(\\mu) = \\mu^2\\) - la varianza aumenta cuadráticamente con la media.\n\nEstas funciones de varianza no solo determinan la heterocedasticidad inherente de cada distribución, sino que también influyen directamente en los pesos del algoritmo IRLS y en la precisión de las estimaciones. Por ejemplo, en regresión logística, las observaciones con probabilidades cercanas a 0.5 tienen mayor varianza y, por tanto, menor peso en la estimación, mientras que en regresión de Poisson, las observaciones con conteos más altos contribuyen con mayor peso al ajuste del modelo.\n\n6.2.1.1 Algoritmo de Newton-Raphson (IRLS)\nPara encontrar los valores de \\(\\boldsymbol{\\beta}\\) que maximizan la log-verosimilitud, los GLM utilizan un algoritmo iterativo conocido como Iteratively Reweighted Least Squares (IRLS), que es una implementación especializada del método de Newton-Raphson. La necesidad de un algoritmo iterativo surge porque, a diferencia de la regresión lineal donde existe una solución analítica cerrada, en los GLM las ecuaciones de verosimilitud no tienen solución directa debido a la presencia de funciones no lineales.\nEl algoritmo IRLS se basa en la idea de que podemos aproximar la función de enlace y la varianza de la distribución en torno a un valor central (la media) y luego aplicar mínimos cuadrados de manera iterativa para ajustar los parámetros del modelo. Los pasos básicos del algoritmo son:\n\nInicialización: Establecer valores iniciales para los parámetros \\(\\boldsymbol{\\beta}^{(0)}\\).\nIteración \\(t\\):\n\nCalcular el predictor lineal: \\(\\eta_i^{(t)} = \\mathbf{x}_i^T \\boldsymbol{\\beta}^{(t)}\\).\nCalcular la media estimada: \\(\\mu_i^{(t)} = g^{-1}(\\eta_i^{(t)})\\).\nCalcular los pesos: \\(w_i^{(t)} = \\frac{1}{\\text{Var}(\\mu_i^{(t)})} \\left(\\frac{d\\mu_i}{d\\eta_i}\\right)^2\\).\nCalcular la variable dependiente ajustada: \\(z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)}) \\frac{d\\eta_i}{d\\mu_i}\\).\n\nActualización: Actualizar los parámetros del modelo:\n\\[\\boldsymbol{\\beta}^{(t+1)} = (\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{z}^{(t)}\\]\nConvergencia: Repetir el proceso hasta que la diferencia entre iteraciones sucesivas sea menor que un umbral predefinido.\n\n\n\n\n\n\n\nEjemplo: Convergencia del algoritmo IRLS\n\n\n\n\n\n\n# Ejemplo de seguimiento de la convergencia en regresión logística\nlibrary(MASS)\ndata(Pima.tr)\n\n# Función para mostrar el proceso iterativo\nmostrar_convergencia &lt;- function() {\n  # Ajustar modelo con seguimiento de iteraciones\n  modelo &lt;- glm(type ~ glu + bmi, data = Pima.tr, family = binomial,\n                control = glm.control(trace = TRUE, maxit = 10))\n  \n  cat(\"Número de iteraciones necesarias:\", modelo$iter, \"\\n\")\n  cat(\"¿Convergió?\", modelo$converged, \"\\n\")\n  cat(\"Log-likelihood final:\", logLik(modelo), \"\\n\")\n  \n  return(modelo)\n}\n\n# Ejecutar y mostrar convergencia\nmodelo_ejemplo &lt;- mostrar_convergencia()\n\nDeviance = 199.36 Iterations - 1\nDeviance = 198.4772 Iterations - 2\nDeviance = 198.4704 Iterations - 3\nDeviance = 198.4704 Iterations - 4\nNúmero de iteraciones necesarias: 4 \n¿Convergió? TRUE \nLog-likelihood final: -99.23522 \n\n\n\n\n\n\n\n6.2.1.2 Propiedades de los estimadores de máxima verosimilitud\nLos estimadores de máxima verosimilitud en GLM poseen propiedades estadísticas muy atractivas que los convierten en la elección preferida para la estimación de parámetros. Estas propiedades son asintóticas, lo que significa que se cumplen cuando el tamaño de la muestra tiende a infinito, pero en la práctica proporcionan una excelente aproximación para muestras moderadamente grandes.\n1. Consistencia: \\[\\hat{\\boldsymbol{\\beta}} \\xrightarrow{p} \\boldsymbol{\\beta} \\text{ cuando } n \\to \\infty\\]\nLa consistencia garantiza que a medida que aumentamos el tamaño de la muestra, nuestros estimadores se acercan cada vez más al valor verdadero de los parámetros. Esto significa que con suficientes datos, los estimadores de máxima verosimilitud convergerán al valor real de \\(\\boldsymbol{\\beta}\\), eliminando el sesgo de estimación. Esta propiedad es fundamental porque nos asegura que no estamos introduciendo errores sistemáticos en nuestras estimaciones.\n2. Normalidad asintótica: \\[\\sqrt{n}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}) \\xrightarrow{d} N(\\mathbf{0}, \\mathbf{I}^{-1}(\\boldsymbol{\\beta}))\\]\nLa normalidad asintótica establece que la distribución de los estimadores, apropiadamente escalada, se aproxima a una distribución normal multivariada cuando el tamaño de la muestra es grande. Esta propiedad es crucial porque:\n\nPermite construir intervalos de confianza para los parámetros usando la distribución normal\nFacilita la realización de pruebas de hipótesis sobre los coeficientes\nProporciona la base teórica para los estadísticos de Wald utilizados en las pruebas de significancia\n\nLa matriz de covarianza asintótica \\(\\mathbf{I}^{-1}(\\boldsymbol{\\beta})\\) nos permite calcular los errores estándar de nuestras estimaciones, que son esenciales para la inferencia estadística.\n3. Eficiencia:\nLos estimadores MV alcanzan la cota de Cramér-Rao, siendo asintóticamente eficientes. Esto significa que:\n\nEntre todos los estimadores insesgados posibles, los de máxima verosimilitud tienen la menor varianza asintótica\nNo existe otro método de estimación que, bajo las mismas condiciones, produzca estimadores con menor incertidumbre\nUtilizan la información disponible en los datos de manera óptima\n\nEn términos prácticos, esta eficiencia se traduce en intervalos de confianza más estrechos y pruebas de hipótesis más poderosas comparado con otros métodos de estimación.\n\n\n6.2.1.3 Matriz de información y errores estándar\nLa implementación práctica de estas propiedades teóricas requiere el cálculo de la matriz de información, que cuantifica la cantidad de información que contienen los datos sobre los parámetros del modelo.\nLa matriz de información de Fisher se define teóricamente como:\n\\[\\mathbf{I}(\\boldsymbol{\\beta}) = E\\left[-\\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T}\\right]\\]\nSin embargo, en la práctica utilizamos la matriz de información observada, que se calcula directamente de nuestros datos:\n\\[\\mathbf{I}(\\hat{\\boldsymbol{\\beta}}) = -\\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T}\\bigg|_{\\boldsymbol{\\beta}=\\hat{\\boldsymbol{\\beta}}}\\]\nEsta matriz representa las segundas derivadas de la log-verosimilitud evaluadas en nuestras estimaciones. Intuitivamente, mide qué tan “puntiaguda” es la función de verosimilitud alrededor del máximo: una función más puntiaguda indica mayor información y, por tanto, menor incertidumbre en la estimación.\nPara los GLM, el algoritmo IRLS proporciona una aproximación computacionalmente eficiente:\n\\[\\mathbf{I}(\\hat{\\boldsymbol{\\beta}}) \\approx \\mathbf{X}^T \\mathbf{W} \\mathbf{X}\\]\ndonde \\(\\mathbf{W}\\) es la matriz diagonal de pesos calculada en la última iteración del algoritmo. Esta aproximación es exacta para la distribución normal y muy buena para otras distribuciones de la familia exponencial.\nLos errores estándar de los coeficientes individuales se obtienen como las raíces cuadradas de los elementos diagonales de la matriz de covarianza:\n\\[\\text{SE}(\\hat{\\beta}_j) = \\sqrt{[\\mathbf{I}^{-1}(\\hat{\\boldsymbol{\\beta}})]_{jj}}\\]\nEstos errores estándar son fundamentales para:\n\nIntervalos de confianza: \\(\\hat{\\beta}_j \\pm z_{\\alpha/2} \\cdot \\text{SE}(\\hat{\\beta}_j)\\)\nEstadísticos de prueba: \\(z_j = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)}\\)\nEvaluación de la precisión de nuestras estimaciones\n\nEs importante recordar que estos errores estándar son válidos bajo los supuestos del modelo GLM y que violaciones serias de estos supuestos (como sobredispersión en modelos de Poisson) pueden hacer que sean inadecuados.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema5.html#bondad-de-ajuste-en-glms",
    "href": "tema5.html#bondad-de-ajuste-en-glms",
    "title": "6  Modelos de regresión generalizada",
    "section": "6.3 Bondad de ajuste en GLMs",
    "text": "6.3 Bondad de ajuste en GLMs\nLos Modelos Lineales Generalizados requieren métodos específicos para evaluar la calidad del ajuste que van más allá de las métricas tradicionales de la regresión lineal. Mientras que en la regresión lineal clásica utilizamos el coeficiente de determinación (\\(R^2\\)) y la suma de cuadrados residuales como medidas principales de bondad de ajuste, en los GLMs estas métricas no son apropiadas debido a las diferentes distribuciones subyacentes y las funciones de enlace no lineales.\nLa evaluación de la bondad de ajuste en GLMs se basa fundamentalmente en conceptos de verosimilitud y deviance, que proporcionan una base teórica sólida para comparar modelos y evaluar su calidad de ajuste. Esta aproximacion basada en la verosimilitud es coherente con el método de estimación utilizado en estos modelos.\n\n6.3.1 La deviance como medida de bondad de ajuste\nLa deviance (o desviación) es la medida principal de bondad de ajuste en los Modelos Lineales Generalizados. Conceptualmente, representa una generalización de la suma de cuadrados residuales de la regresión lineal para distribuciones no normales, pero su interpretación y cálculo son fundamentalmente diferentes.\nLa deviance se basa en el principio de máxima verosimilitud y mide qué tan bien el modelo propuesto se ajusta a los datos comparado con el mejor ajuste posible. Para entender este concepto, es importante distinguir entre dos tipos de modelos:\n\nModelo Saturado: Un modelo hipotético que tiene tantos parámetros como observaciones, por lo que puede predecir perfectamente cada valor observado. Este modelo representa el “ajuste perfecto” teórico.\nModelo Propuesto: El modelo que estamos evaluando, con un número limitado de parámetros basado en nuestras variables predictoras.\n\nLa deviance mide la diferencia en log-verosimilitud entre estos dos modelos:\n\\[\nD = 2 \\sum_{i=1}^{n} \\left[ \\ell(y_i; y_i) - \\ell(y_i; \\hat{\\mu}_i) \\right]\n\\]\ndonde:\n\n\\(\\ell(y_i; y_i)\\) = Log-verosimilitud del modelo saturado para la observación \\(i\\).\n\\(\\ell(y_i; \\hat{\\mu}_i)\\) = Log-verosimilitud del modelo propuesto para la observación \\(i\\).\nEl factor 2 se incluye para que la deviance siga aproximadamente una distribución chi-cuadrado bajo ciertas condiciones.\n\nInterpretación práctica:\n\nDeviance = 0: Modelo perfecto que ajusta exactamente todos los datos observados\nDeviance baja: Buen ajuste del modelo a los datos\nDeviance alta: Mal ajuste del modelo, sugiere que el modelo no captura adecuadamente los patrones en los datos\n\nComparación relativa: La deviance es más útil para comparar modelos que para evaluación absoluta. Un modelo con menor deviance indica mejor ajuste, pero el valor absoluto depende del tamaño de la muestra y la naturaleza de los datos.\nDeviance residual vs. deviance nula:\n\nDeviance nula: Deviance del modelo que solo incluye el intercepto (sin predictores)\nDeviance residual: Deviance del modelo con todos los predictores incluidos\nLa diferencia entre ambas indica cuánto mejora el modelo al incluir las variables predictoras\n\n\n\n6.3.2 Test de la razón de verosimilitudes\nEl test de la razón de verosimilitudes es la herramienta principal para comparar modelos anidados en GLMs y para evaluar la significancia global del modelo. Se basa en el principio de que si un modelo más complejo no mejora significativamente el ajuste, debemos preferir el modelo más simple por parsimonia.\nCuando comparamos dos modelos anidados (donde uno es un caso especial del otro), la diferencia en sus deviances sigue aproximadamente una distribución chi-cuadrado:\n\\[\nLRT = D_{\\text{modelo reducido}} - D_{\\text{modelo completo}} \\sim \\chi^2_{df}\n\\]\ndonde \\(df\\) es la diferencia en grados de libertad (número de parámetros) entre los modelos.\nInterpretación del test:\n\nHipótesis nula (\\(H_0\\)): El modelo reducido es adecuado (los parámetros adicionales no son necesarios)\nHipótesis alternativa (\\(H_1\\)): El modelo completo es significativamente mejor\nDecisión: Si \\(p\\)-valor &lt; \\(\\alpha\\) (típicamente 0.05), rechazamos \\(H_0\\) y preferimos el modelo completo\n\nAplicaciones principales:\n\nSignificancia global del modelo: Comparar el modelo completo con el modelo nulo (solo intercepto) para determinar si las variables predictoras aportan información significativa.\nSelección de variables: Evaluar si la inclusión o exclusión de variables específicas mejora significativamente el ajuste del modelo.\nComparación de especificaciones: Decidir entre diferentes formas funcionales o distribuciones para el mismo conjunto de datos.\n\n\n\n\n\n\n\nEjemplo: Comparando Modelos con el Test de Razón de Verosimilitudes\n\n\n\n\n\nSupongamos que queremos determinar si añadir la variable disp (cilindrada) a un modelo de regresión logística que ya contiene wt (peso) mejora significativamente la predicción de si un coche tiene transmisión automática (am).\n\n# Usaremos el dataset mtcars\ndata(mtcars)\n\n# Modelo Reducido: solo contiene 'wt'\nmodelo_reducido &lt;- glm(am ~ wt, data = mtcars, family = binomial)\n\n# Modelo Completo: contiene 'wt' y 'disp'\nmodelo_completo &lt;- glm(am ~ wt + disp, data = mtcars, family = binomial)\n\n# Realizamos el Test de Razón de Verosimilitudes (LRT)\n# En R, esto se hace con la función anova() especificando el test\nlrt_resultado &lt;- anova(modelo_reducido, modelo_completo, test = \"LRT\")\n\n# Mostramos los resultados\nprint(lrt_resultado)\n\nAnalysis of Deviance Table\n\nModel 1: am ~ wt\nModel 2: am ~ wt + disp\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1        30     19.176                     \n2        29     17.785  1   1.3913   0.2382\n\n\nInterpretación del resultado:\nEl test compara la Deviance de ambos modelos. La hipótesis nula (\\(H\\_0\\)) es que el modelo reducido es suficiente (es decir, \\(\\\\beta\\_{disp} = 0\\)).\n\nLa diferencia en deviance es de 1.391 con 1 grado de libertad (el parámetro adicional).\nEl p-valor asociado es 0.238.\n\nDado que el p-valor es mayor que 0.05, no rechazamos la hipótesis nula. Esto significa que, una vez que tenemos en cuenta el peso del coche (wt), añadir la cilindrada (disp) no aporta una mejora estadísticamente significativa al modelo. Nos quedaríamos con el modelo reducido por el principio de parsimonia.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema5.html#diagnosis-de-glms",
    "href": "tema5.html#diagnosis-de-glms",
    "title": "6  Modelos de regresión generalizada",
    "section": "6.4 Diagnosis de GLMs",
    "text": "6.4 Diagnosis de GLMs\nLa diagnosis de GLMs implica evaluar los supuestos del modelo y detectar problemas potenciales que puedan afectar la validez de las inferencias. A diferencia de la regresión lineal, donde los residuos ordinarios proporcionan información diagnóstica directa, los GLMs requieren herramientas especializadas debido a la heterocedasticidad inherente y las diferentes distribuciones subyacentes.\nEn lugar de una simple lista de comprobación, abordaremos el diagnóstico respondiendo a tres preguntas clave que un analista se haría, utilizando diferentes tipos de residuos para obtener las respuestas.\n\n6.4.1 Tipos de Residuos en GLMs\nLa elección del tipo de residuo apropiado es crucial. Los residuos “crudos” (\\(y_i - \\hat{\\mu}_i\\)) no son homocedásticos, por lo que se utilizan versiones estandarizadas. Los más importantes son:\n\nResiduos Pearson: Estandarizan el residuo crudo dividiendo por la desviación estándar predicha por el modelo. Son un análogo directo a los residuos estandarizados en regresión lineal. \\[\n  r_i = \\frac{y_i - \\hat{\\mu}_i}{\\sqrt{V(\\hat{\\mu}_i)}}\n  \\]\nResiduos Estudentizados: Son una mejora de los residuos Pearson que también tienen en cuenta el leverage (\\(h_i\\)) de cada observación. Son más fiables para la detección de outliers. \\[\n  r_{S_i} = \\frac{y_i - \\hat{\\mu}_i}{\\sqrt{V(\\hat{\\mu}_i)(1-h_i)}}\n  \\]\nResiduos deviance: Son los más recomendados para la inspección visual en gráficos diagnósticos. Su construcción, basada en la contribución de cada punto a la deviance total, les confiere propiedades muy deseables: su distribución se aproxima mejor a la normalidad y su varianza es más estable que la de otros residuos. \\[\n  d_i = \\text{sign}(y_i - \\hat{\\mu}_i) \\sqrt{2[l_i(y_i) - l_i(\\hat{\\mu}_i)]}\n  \\]\n\n\n\n6.4.2 ¿La forma del modelo es correcta? (Linealidad y Enlace)\nEsta primera pregunta evalúa si la estructura básica del modelo, \\(g(\\mu) = X\\beta\\), es adecuada para los datos. Para ello, utilizamos varias herramientas de diagnóstico:\n\nEl gráfico de residuos vs. valores ajustados es la herramienta fundamental. Se grafican los residuos (idealmente, deviance) contra los valores predichos en la escala del predictor lineal (\\(\\hat{\\eta}_i\\)). Si la forma del modelo es correcta, no deberíamos ver ningún patrón sistemático. Una tendencia curvilínea es una señal clara de que la forma funcional o la función de enlace son incorrectas.\nLos gráficos de residuos parciales son esenciales para evaluar si la función de enlace es apropiada para cada predictor individualmente. Un patrón no lineal en este gráfico sugiere que la relación de esa variable específica con la respuesta no es la que asume el modelo.\nEl test de especificación de enlace (como el Linktest de Pregibon) ofrece una prueba formal. La idea es ajustar un segundo modelo que incluye el predictor lineal al cuadrado (\\(\\hat{\\eta}^2\\)) como una variable adicional. Si este término cuadrático resulta significativo, es una fuerte evidencia de que la función de enlace está mal especificada.\n\nSi estos diagnósticos revelan problemas, las estrategias de corrección incluyen aplicar transformaciones a los predictores (ej. logaritmo, términos polinómicos), incluir términos de interacción para capturar relaciones no aditivas, o directamente cambiar la función de enlace a una que se ajuste mejor a los datos.\n\n\n6.4.3 ¿La distribución que elegimos es la correcta? (Varianza y Normalidad)\nEsta pregunta evalúa si la elección de la familia de distribución (Poisson, Binomial, etc.) fue acertada, lo que implica verificar la relación entre la media y la varianza, así como la forma general de los errores.\nUn primer aspecto clave al verificar la distribución es la sobredispersión. Imagina que tu modelo es como una regla estricta sobre el comportamiento de los datos. El modelo de Poisson, por ejemplo, impone que la varianza de los conteos debe ser igual a su media (\\(Var(Y) = \\mu\\)). La sobredispersión ocurre cuando tus datos reales son más “desordenados” o variables de lo que esta regla permite.\nPara detectar este problema de forma objetiva, calculamos el Estadístico de dispersión (\\(\\hat{\\phi}\\)), que compara la varianza observada (a través de los residuos Pearson) con la esperada: \\[\\hat{\\phi} = \\frac{X^2_{\\text{Pearson}}}{n-p} = \\frac{\\sum r_i^2}{n-p}\\] La interpretación es directa:\n\nSi \\(\\hat{\\phi} \\approx 1\\): ¡Perfecto! La dispersión de los datos es la que el modelo esperaba.\nSi \\(\\hat{\\phi} &gt; 1\\): Tienes sobredispersión. El modelo está subestimando la variabilidad real de los datos, lo que invalida las inferencias (errores estándar, p-valores).\n\nLa estrategia para corregirlo no es forzar los datos, sino cambiar a un modelo más flexible. El caso clásico es pasar del modelo de Poisson al modelo Binomial Negativo. Este último funciona porque su fórmula para la varianza incluye un parámetro de dispersión adicional (\\(\\alpha\\)) que le permite modelar esa variabilidad extra que el modelo de Poisson no puede capturar: \\[Var(Y) = \\mu + \\alpha\\mu^2\\]\nEste término adicional se ajusta a la variabilidad de los datos, proporcionando estimaciones y errores estándar mucho más fiables.\nUn segundo aspecto es la forma general de la distribución, que se evalúa con el Gráfico Q-Q de residuos deviance. Aunque los errores de un GLM no son estrictamente normales, los residuos deviance sí deberían tener una distribución aproximadamente normal si el modelo está bien especificado. Desviaciones sistemáticas de la línea diagonal en el gráfico Q-Q pueden indicar que la distribución asumida para los datos es incorrecta.\n\n\n6.4.4 ¿Hay observaciones que distorsionan el modelo? (Atípicos e Influyentes)\nFinalmente, buscamos identificar puntos individuales que tienen una influencia desproporcionada en el modelo. Las herramientas matemáticas para ello son generalizaciones de las vistas en regresión lineal:\n\nLeverage generalizado (\\(h_i\\)): Mide el potencial de una observación para ser influyente debido a su posición en el espacio de los predictores. \\[\n  h_i = w_i \\mathbf{x}_i^T (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{x}_i\n  \\] donde \\(w_i\\) y \\(\\mathbf{W}\\) son el peso y la matriz de pesos de la última iteración del algoritmo IRLS.\nDistancia de Cook para GLMs (\\(D_i\\)): Mide la influencia global de una observación en todos los coeficientes. Utiliza el residuo Pearson (\\(r_i\\)). \\[\n  D_i = \\frac{r_i^2 h_i}{p(1-h_i)^2}\n  \\]\nDFBETAS: Mide la influencia de la observación \\(i\\) en cada coeficiente individual \\(\\beta_j\\). Es útil para ver si un punto influyente está afectando a una variable de interés particular. \\[\n  \\text{DFBETA}_{j,i} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(-i)}}{\\text{SE}(\\hat{\\beta}_{j(-i)})} \\approx \\frac{(\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}_{jj}x_{ij}(y_i-\\hat{\\mu}_i)}{\\text{SE}(\\hat{\\beta}_j)\\sqrt{1-h_i}}\n  \\]\n\nLa herramienta visual que consolida esta información es el gráfico de residuos vs. leverage. La estrategia ante estas observaciones no es eliminarlas automáticamente, sino investigarlas para entender su naturaleza.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema5.html#regresión-logística",
    "href": "tema5.html#regresión-logística",
    "title": "6  Modelos de regresión generalizada",
    "section": "6.5 Regresión logística",
    "text": "6.5 Regresión logística\nLa regresión logística es una herramienta fundamental para modelar la probabilidad de eventos binarios en una variedad de contextos, desde la medicina hasta la economía y el marketing (Hosmer Jr, Lemeshow, y Sturdivant 2013). La correcta interpretación de los coeficientes mediante odds ratios, así como la evaluación del ajuste del modelo mediante curvas ROC y matrices de confusión, son esenciales para extraer conclusiones válidas de los datos.\n\n6.5.1 Fundamentos de la regresión logística\nLa regresión logística es una técnica estadística utilizada para modelar la probabilidad de ocurrencia de un evento binario, es decir, cuando la variable dependiente toma solo dos posibles valores (por ejemplo, éxito/fracaso, sí/no, enfermo/sano). A diferencia de la regresión lineal, que modela una relación lineal entre variables, la regresión logística utiliza una función logística para asegurar que las predicciones estén en el rango [0,1], lo cual es necesario para interpretar los resultados como probabilidades.\nLa función Logística (Sigmoide)\nLa función logística transforma cualquier valor real en un valor comprendido entre 0 y 1. La forma matemática de la función logística es:\n\\[\nP(Y = 1 | X_1, \\dots, X_p) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p)}}\n\\]\nDonde:\n\n\\(P(Y = 1 | X_1, \\dots, X_p)\\) es la probabilidad de que el evento ocurra.\n\\(\\beta_0\\) es el intercepto y \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\) son los coeficientes asociados a las variables independientes \\(X_1, X_2, \\dots, X_p\\).\n\nLa curva sigmoide que representa esta función tiene forma de “S”, lo que refleja que para valores muy pequeños o muy grandes del predictor, la probabilidad se aplana hacia 0 o 1, respectivamente.\nFunción de enlace Logit\nEn la regresión logística, la relación entre el predictor lineal y la probabilidad se establece mediante la función de enlace logit. El logit de una probabilidad \\(p\\) se define como:\n\\[\n\\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n\\]\nEsta transformación convierte una probabilidad en una escala que va de \\(-\\infty\\) a \\(+\\infty\\), lo que permite ajustar un modelo lineal a los datos. El modelo logístico puede expresarse como:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\n\n\n6.5.2 Estimación por máxima verosimilitud en regresión logística\nLa estimación de los parámetros en regresión logística se basa en el método de máxima verosimilitud, adaptado específicamente para la distribución binomial con función de enlace logit.\nPara una muestra de \\(n\\) observaciones independientes, donde \\(y_i \\in \\{0,1\\}\\) representa el resultado binario para la observación \\(i\\), la función de verosimilitud se define como:\n\\[L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}\\]\ndonde \\(p_i = P(Y_i = 1|\\mathbf{x}_i) = \\frac{1}{1 + e^{-\\mathbf{x}_i^T\\boldsymbol{\\beta}}}\\) es la probabilidad estimada para la observación \\(i\\).\nLa log-verosimilitud correspondiente es:\n\\[\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[y_i \\log(p_i) + (1-y_i) \\log(1-p_i)\\right]\\]\nSustituyendo la expresión de \\(p_i\\) y simplificando:\n\\[\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[y_i \\mathbf{x}_i^T\\boldsymbol{\\beta} - \\log(1 + e^{\\mathbf{x}_i^T\\boldsymbol{\\beta}})\\right]\\]\nPara encontrar los valores de \\(\\boldsymbol{\\beta}\\) que maximizan la log-verosimilitud, derivamos respecto a cada parámetro:\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^{n} x_{ij}(y_i - p_i) = 0\\]\ndonde \\(x_{ij}\\) es el valor de la variable \\(j\\) para la observación \\(i\\).\nEsta ecuación tiene una interpretación intuitiva: los estimadores de máxima verosimilitud se obtienen cuando la suma de los residuos ponderados por cada variable predictora es igual a cero.\n\n6.5.2.1 Implementación del algoritmo IRLS\nDado que las ecuaciones de verosimilitud no tienen solución analítica cerrada, se utiliza el algoritmo IRLS. Para regresión logística, los elementos específicos son:\nPesos: \\[w_i = p_i(1-p_i)\\]\nVariable dependiente ajustada: \\[z_i = \\mathbf{x}_i^T\\boldsymbol{\\beta}^{(t)} + \\frac{y_i - p_i^{(t)}}{p_i^{(t)}(1-p_i^{(t)})}\\]\nActualización de parámetros: \\[\\boldsymbol{\\beta}^{(t+1)} = (\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{z}^{(t)}\\]\n\n\n\n\n\n\nEjemplo: Estimación paso a paso en regresión logística\n\n\n\n\n\n\n# Demostración del proceso de estimación por máxima verosimilitud\nlibrary(MASS)\ndata(Pima.tr)\n\n# Función para calcular log-verosimilitud manualmente\nlog_likelihood_logistic &lt;- function(beta, X, y) {\n  eta &lt;- X %*% beta\n  p &lt;- 1 / (1 + exp(-eta))\n  # Evitar problemas numéricos\n  p &lt;- pmax(pmin(p, 1-1e-15), 1e-15)\n  sum(y * log(p) + (1-y) * log(1-p))\n}\n\n# Preparar datos\nX &lt;- model.matrix(type ~ glu + bmi, data = Pima.tr)\ny &lt;- as.numeric(Pima.tr$type == \"Yes\")\n\n# Ajuste con glm para comparación\nmodelo_glm &lt;- glm(type ~ glu + bmi, data = Pima.tr, family = binomial)\nbeta_glm &lt;- coef(modelo_glm)\n\ncat(\"=== ESTIMACIÓN POR MÁXIMA VEROSIMILITUD ===\\n\")\n\n=== ESTIMACIÓN POR MÁXIMA VEROSIMILITUD ===\n\ncat(\"Coeficientes estimados por glm():\\n\")\n\nCoeficientes estimados por glm():\n\nprint(beta_glm)\n\n(Intercept)         glu         bmi \n-8.21610630  0.03571601  0.09001639 \n\n# Verificar que estos coeficientes maximizan la log-verosimilitud\nll_optimo &lt;- log_likelihood_logistic(beta_glm, X, y)\ncat(\"\\nLog-verosimilitud en el óptimo:\", round(ll_optimo, 4), \"\\n\")\n\n\nLog-verosimilitud en el óptimo: -99.2352 \n\n# Comparar con valores ligeramente diferentes\nbeta_test &lt;- beta_glm + c(0.1, 0, 0)\nll_test &lt;- log_likelihood_logistic(beta_test, X, y)\ncat(\"Log-verosimilitud con perturbación:\", round(ll_test, 4), \"\\n\")\n\nLog-verosimilitud con perturbación: -99.3995 \n\ncat(\"Diferencia:\", round(ll_optimo - ll_test, 4), \"\\n\")\n\nDiferencia: 0.1643 \n\n# Mostrar información de convergencia\ncat(\"\\nInformación del algoritmo IRLS:\\n\")\n\n\nInformación del algoritmo IRLS:\n\ncat(\"Iteraciones necesarias:\", modelo_glm$iter, \"\\n\")\n\nIteraciones necesarias: 4 \n\ncat(\"¿Convergió?\", modelo_glm$converged, \"\\n\")\n\n¿Convergió? TRUE \n\n# Matriz de información y errores estándar\nvcov_matrix &lt;- vcov(modelo_glm)\ncat(\"\\nErrores estándar de los coeficientes:\\n\")\n\n\nErrores estándar de los coeficientes:\n\nprint(sqrt(diag(vcov_matrix)))\n\n(Intercept)         glu         bmi \n1.346965130 0.006311023 0.031268458 \n\n\n\n\n\n\n\n\n6.5.3 Interpretación de coeficientes y odds ratios\nUno de los aspectos más importantes de la regresión logística es la interpretación de los coeficientes. Dado que los coeficientes están en la escala del logit, su interpretación directa no es tan intuitiva como en la regresión lineal. Sin embargo, podemos interpretarlos utilizando odds y odds ratios.\nEl odds o razón de probabilidades de que ocurra un evento es el cociente entre la probabilidad de que ocurra el evento y la probabilidad de que no ocurra:\n\\[\n\\text{odds} = \\frac{p}{1 - p}\n\\]\nPor ejemplo, si la probabilidad de éxito es 0.8, el odds sería:\n\\[\n\\text{odds} = \\frac{0.8}{1 - 0.8} = 4\n\\]\nEsto significa que el evento es 4 veces más probable que no ocurra.\nEl odds ratio (OR) mide el cambio en los odds cuando una variable independiente aumenta en una unidad. Se calcula como el exponencial del coeficiente de la regresión logística:\n\\[\n\\text{OR} = e^{\\beta}\n\\]\nInterpretación de OR:\n\nSi OR &gt; 1, el evento es más probable a medida que aumenta la variable independiente.\nSi OR &lt; 1, el evento es menos probable a medida que aumenta la variable independiente.\nSi OR = 1, no hay efecto.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nSupongamos que ajustamos un modelo de regresión logística para predecir la probabilidad de tener diabetes en función del índice de masa corporal (BMI). El coeficiente asociado a BMI es 0.08.\n\\[\n\\text{OR} = e^{0.08} \\approx 1.083\n\\]\nEsto significa que por cada incremento de 1 unidad en el BMI, la odds de tener diabetes aumentan en un 8.3%.\n\n\n\n\n\n6.5.4 Bondad de ajuste del modelo logístico\nLa evaluación de la bondad de ajuste en regresión logística presenta desafíos únicos debido a la naturaleza binaria de la variable dependiente. A diferencia de la regresión lineal, donde el \\(R^2\\) tradicional proporciona una medida directa de la varianza explicada, en la regresión logística necesitamos adoptar enfoques alternativos que se basen en la verosimilitud del modelo y que sean apropiados para datos categóricos.\nLa bondad de ajuste en regresión logística se evalúa principalmente a través de dos enfoques complementarios: la deviance y los pseudo R². Ambos métodos nos permiten cuantificar qué tan bien nuestro modelo captura los patrones subyacentes en los datos binarios.\nLa deviance en regresión logística se calcula utilizando la distribución binomial subyacente. Para cada observación, comparamos la probabilidad que predice nuestro modelo con la “probabilidad perfecta” que asignaría un modelo saturado. Matemáticamente, esto se expresa como:\n\\[D = 2 \\sum_{i=1}^{n} \\left[ y_i \\log\\left(\\frac{y_i}{\\hat{p}_i}\\right) + (1-y_i) \\log\\left(\\frac{1-y_i}{1-\\hat{p}_i}\\right) \\right]\\]\ndonde \\(y_i \\in \\{0,1\\}\\) representa el resultado observado y \\(\\hat{p}_i\\) es la probabilidad estimada por nuestro modelo. Es importante notar que cuando \\(y_i = 0\\) o \\(y_i = 1\\), algunos términos en esta expresión se anulan automáticamente, lo que refleja la naturaleza discreta de los datos binarios.\nLa interpretación de la deviance sigue principios similares a los discutidos anteriormente, pero es crucial recordar que ahora estamos tratando con datos binarios. La deviance nos ayuda a entender qué tan bien nuestro modelo logístico se ajusta a los datos de respuesta binaria en comparación con un modelo que simplemente predice la media.\nLos pseudo R² son medidas alternativas que intentan capturar la proporción de variabilidad explicada por el modelo, similar al \\(R^2\\) en regresión lineal, pero adaptadas a la naturaleza de los datos binarios y la verosimilitud del modelo. Estas medidas son útiles para evaluar y comparar modelos, aunque su interpretación no es tan directa como el \\(R^2\\) tradicional.\nEl McFadden’s R² es quizás el más utilizado y se define como:\n\\[R^2_{\\text{McFadden}} = 1 - \\frac{\\log L_{\\text{modelo}}}{\\log L_{\\text{modelo nulo}}}\\]\nEste pseudo R² compara la log-verosimilitud de nuestro modelo con la de un modelo que solo incluye el intercepto. Los valores típicamente oscilan entre 0 y 1, aunque raramente alcanzan valores tan altos como el \\(R^2\\) en regresión lineal. En contextos aplicados, valores de McFadden entre 0.2 y 0.4 se consideran indicativos de un buen ajuste.\nEl Nagelkerke R² representa una versión normalizada que garantiza que el valor máximo sea 1:\n\\[R^2_{\\text{Nagelkerke}} = \\frac{1 - \\left(\\frac{L_{\\text{nulo}}}{L_{\\text{modelo}}}\\right)^{2/n}}{1 - (L_{\\text{nulo}})^{2/n}}\\]\nFinalmente, el Cox-Snell R² se define como:\n\\[R^2_{\\text{Cox-Snell}} = 1 - \\left(\\frac{L_{\\text{nulo}}}{L_{\\text{modelo}}}\\right)^{2/n}\\]\nAunque este último tiene la limitación de que su valor máximo teórico es menor que 1, razón por la cual Nagelkerke propuso su corrección.\nEs crucial entender que estos pseudo R² no deben interpretarse exactamente como el \\(R^2\\) tradicional. Mientras que en regresión lineal el \\(R^2\\) representa la proporción de varianza explicada, en regresión logística estos índices reflejan más bien la mejora en la verosimilitud que aporta nuestro modelo comparado con el modelo nulo. Sin embargo, proporcionan una herramienta valiosa para evaluar y comparar diferentes especificaciones de modelo.\nLa evaluación integral de la bondad de ajuste en regresión logística requiere considerar tanto la deviance como los pseudo R² en conjunto, complementando esta información con pruebas de significancia global del modelo mediante tests de razón de verosimilitudes, que permiten determinar si la inclusión de las variables predictoras mejora significativamente el ajuste comparado con el modelo nulo.\n\n\n6.5.5 Validación del modelo logístico\nUna vez evaluada la bondad de ajuste del modelo logístico, el siguiente paso fundamental es validar su capacidad predictiva y su rendimiento en la clasificación de nuevas observaciones. La validación en regresión logística presenta características particulares debido a la naturaleza categórica de la variable dependiente, lo que requiere métricas y enfoques específicos que van más allá de las medidas tradicionales de error de predicción.\nLa validación del modelo logístico se centra en dos aspectos complementarios: la capacidad discriminativa del modelo (qué tan bien puede distinguir entre las dos clases) y la precisión de clasificación (qué proporción de predicciones son correctas). Estas evaluaciones se realizan típicamente mediante la construcción de una matriz de confusión y el análisis de curvas ROC.\nLa matriz de confusión constituye la herramienta fundamental para evaluar el rendimiento de clasificación en regresión logística. Esta matriz organiza las predicciones del modelo en una tabla de contingencia 2×2 que compara los resultados predichos con los valores reales observados. Para construir esta matriz, primero debemos convertir las probabilidades estimadas por el modelo en predicciones de clase mediante un umbral de decisión, típicamente 0.5.\nLa clasificación de cada observación resulta en una de cuatro categorías:\n\nVerdaderos Positivos (VP): Predijo positivo y es positivo.\nFalsos Positivos (FP): Predijo positivo pero es negativo.\nVerdaderos Negativos (VN): Predijo negativo y es negativo.\nFalsos Negativos (FN): Predijo negativo pero es positivo.\n\nA partir de esta clasificación, podemos calcular métricas fundamentales de rendimiento:\n\nPrecisión (Accuracy): \\(\\frac{VP + VN}{\\text{Total}}\\) - representa la proporción total de predicciones correctas\nSensibilidad: \\(\\frac{VP}{VP + FN}\\) - mide la capacidad del modelo para identificar correctamente los casos positivos\n\nEspecificidad: \\(\\frac{VN}{VN + FP}\\) - evalúa la capacidad para identificar correctamente los casos negativos\n\nEs importante reconocer que estas métricas pueden verse influenciadas por el umbral de decisión elegido y por el balance de clases en los datos. Un modelo puede tener alta precisión global pero pobre capacidad para detectar la clase minoritaria, especialmente en datasets desbalanceados.\nLa Curva ROC (Receiver Operating Characteristic) proporciona una evaluación más comprehensiva del rendimiento del modelo al examinar la relación entre la sensibilidad y la especificidad a través de todos los posibles umbrales de decisión. Esta curva grafica la tasa de verdaderos positivos contra la tasa de falsos positivos (1 - especificidad) para cada umbral posible.\nUn modelo perfecto produciría una curva ROC que pasaría por la esquina superior izquierda del gráfico (100% sensibilidad, 0% falsos positivos), mientras que un modelo sin capacidad discriminativa produciría una línea diagonal de 45 grados. La AUC (Área Bajo la Curva ROC) cuantifica esta capacidad discriminativa en un solo número que varía entre 0.5 (sin capacidad discriminativa) y 1.0 (discriminación perfecta).\nLa interpretación de la AUC sigue convenciones establecidas:\n\n0.9 - 1.0: Excelente discriminación\n0.8 - 0.9: Buena discriminación\n\n0.7 - 0.8: Discriminación aceptable\n0.6 - 0.7: Discriminación pobre\n0.5 - 0.6: Sin capacidad discriminativa útil\n\nLa validación efectiva del modelo logístico requiere considerar tanto las métricas puntuales derivadas de la matriz de confusión como la capacidad discriminativa global capturada por la curva ROC y la AUC. Además, es crucial evaluar estas métricas tanto en los datos de entrenamiento como en conjuntos de validación independientes para detectar posibles problemas de sobreajuste y asegurar que el modelo generalizará adecuadamente a nuevos datos.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nVamos a aplicar la regresión logística en R utilizando el conjunto de datos Pima.tr del paquete MASS, que contiene información sobre mujeres pima y si tienen o no diabetes.\n\n# Cargar la librería y el conjunto de datos\nlibrary(MASS)\ndata(Pima.tr)\n\n# Ajustar el modelo de regresión logística\nmodelo_logistico &lt;- glm(type ~ npreg + glu + bmi, data = Pima.tr, family = binomial)\n\n# Resumen del modelo\nsummary(modelo_logistico)\n\n\nCall:\nglm(formula = type ~ npreg + glu + bmi, family = binomial, data = Pima.tr)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.718723   1.411080  -6.179 6.46e-10 ***\nnpreg        0.149213   0.051833   2.879  0.00399 ** \nglu          0.033879   0.006327   5.355 8.55e-08 ***\nbmi          0.094817   0.032405   2.926  0.00343 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 256.41  on 199  degrees of freedom\nResidual deviance: 189.89  on 196  degrees of freedom\nAIC: 197.89\n\nNumber of Fisher Scoring iterations: 5\n\n# Predicciones de probabilidad\npredicciones_prob &lt;- predict(modelo_logistico, type = \"response\")\n\n# Clasificación con un umbral de 0.5\npredicciones_clase &lt;- ifelse(predicciones_prob &gt; 0.5, \"Yes\", \"No\")\n\n# Crear matriz de confusión\ntabla_confusion &lt;- table(Predicted = predicciones_clase, Actual = Pima.tr$type)\nprint(tabla_confusion)\n\n         Actual\nPredicted  No Yes\n      No  114  29\n      Yes  18  39\n\n# Calcular precisión\naccuracy &lt;- sum(diag(tabla_confusion)) / sum(tabla_confusion)\nprint(paste(\"Precisión:\", round(accuracy, 3)))\n\n[1] \"Precisión: 0.765\"\n\n# Cargar librería para curvas ROC\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n# Curva ROC\nroc_obj &lt;- roc(Pima.tr$type, predicciones_prob)\n\nSetting levels: control = No, case = Yes\n\n\nSetting direction: controls &lt; cases\n\nplot(roc_obj, main = \"Curva ROC para Regresión Logística\")\n\n\n\n\n\n\n\n# Calcular AUC\nauc_valor &lt;- auc(roc_obj)\nprint(paste(\"AUC:\", round(auc_valor, 3)))\n\n[1] \"AUC: 0.831\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema5.html#regresión-de-poisson",
    "href": "tema5.html#regresión-de-poisson",
    "title": "6  Modelos de regresión generalizada",
    "section": "6.6 Regresión de Poisson",
    "text": "6.6 Regresión de Poisson\nLa regresión de Poisson es una técnica estadística utilizada para modelar datos de conteo, es decir, situaciones en las que la variable dependiente representa el número de veces que ocurre un evento en un período de tiempo o espacio específico (Coxe, West, y Aiken 2009). Este tipo de modelo es adecuado cuando la variable dependiente toma valores enteros no negativos (\\(0, 1, 2, \\dots\\)) y sigue una distribución de Poisson.\nLa distribución de Poisson describe la probabilidad de que ocurra un número determinado de eventos en un intervalo fijo, dado que estos eventos ocurren de forma independiente y a una tasa constante.\nLa función de probabilidad de la distribución de Poisson es:\n\\[\nP(Y = y) = \\frac{e^{-\\lambda} \\lambda^y}{y!}\n\\]\nDonde:\n\n\\(Y\\) es la variable aleatoria que representa el número de eventos.\n\\(\\lambda\\) es la tasa media de ocurrencia de los eventos (esperanza de \\(Y\\)).\n\\(y\\) es el número de eventos observados (\\(y = 0, 1, 2, \\dots\\)).\n\n\n6.6.1 Modelo de regresión de Poisson\nEn la regresión de Poisson, el objetivo es modelar la relación entre la tasa de ocurrencia de los eventos (\\(\\lambda\\)) y un conjunto de variables predictoras \\(X_1, X_2, \\dots, X_p\\).\nLa forma funcional del modelo de Poisson es:\n\\[\n\\log(\\lambda) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\nDonde:\n\n\\(\\log(\\lambda)\\) es la función de enlace logarítmica que asegura que la tasa \\(\\lambda\\) sea siempre positiva.\n\\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) son los coeficientes del modelo que describen la influencia de cada predictor sobre la tasa de eventos.\n\nEl modelo puede expresarse en términos de la tasa esperada de eventos como:\n\\[\n\\lambda = e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p}\n\\]\n\n\n6.6.2 Supuestos y limitaciones de la regresión de Poisson\nTal y como ocurre en el modelo de regresión lineal, para que la regresión de Poisson sea adecuada, se deben cumplir ciertos supuestos:\n\nIndependencia de los eventos: Los eventos deben ocurrir de manera independiente unos de otros.\nDistribución de Poisson de la variable dependiente: La variable de respuesta debe seguir una distribución de Poisson, donde la media y la varianza son iguales:\n\n\\[\n   E(Y) = Var(Y) = \\lambda\n   \\]\n\nNo sobredispersión: Uno de los problemas comunes en los datos de conteo es la sobredispersión, que ocurre cuando la varianza de los datos es mayor que la media (\\(Var(Y) &gt; E(Y)\\)). La presencia de sobredispersión indica que el modelo de Poisson puede no ser adecuado, y puede ser necesario considerar modelos alternativos como la regresión binomial negativa.\nNo exceso de ceros: Si hay demasiados ceros en los datos (por ejemplo, en el número de accidentes en diferentes localidades donde muchas tienen cero accidentes), puede ser necesario utilizar modelos de Poisson inflados en ceros (ZIP) (Lambert 1992).\n\n\n\n6.6.3 Interpretación de los resultados\nLa interpretación de los coeficientes en la regresión de Poisson difiere de la regresión lineal debido al uso de la función de enlace logarítmica.\nLos coeficientes \\(\\beta\\) representan el logaritmo de la tasa de eventos asociados con un cambio en la variable independiente. Para interpretar en términos de la tasa de ocurrencia, se utiliza el exponencial de los coeficientes:\n\\[\n  e^{\\beta_i}\n\\]\nEsto representa el factor de cambio multiplicativo en la tasa de eventos por cada unidad adicional en la variable \\(X_i\\).\n\n\n\n\n\n\nInterpretando el coeficiente de Poisson: Incidence Rate Ratio (IRR)\n\n\n\nEn la regresión de Poisson, el coeficiente \\(\\beta_j\\) está en la escala logarítmica de la tasa. Para una interpretación práctica, lo exponenciamos para obtener el Incidence Rate Ratio (IRR):\n\\[\n\\text{IRR} = e^{\\beta_j}\n\\]\nEl IRR es un factor multiplicativo que nos dice cuánto cambia la tasa de eventos esperada por cada incremento de una unidad en el predictor \\(X_j\\).\n\nSi IRR &gt; 1: Un incremento en \\(X_j\\) se asocia con un aumento en la tasa de eventos. Un IRR de 1.25 significa que por cada unidad que aumenta \\(X_j\\), la tasa de eventos esperada se multiplica por 1.25 (es decir, aumenta un 25%).\nSi IRR &lt; 1: Un incremento en \\(X_j\\) se asocia con una disminución en la tasa de eventos. Un IRR de 0.80 significa que por cada unidad que aumenta \\(X_j\\), la tasa de eventos esperada se multiplica por 0.80 (es decir, disminuye un 20%).\nSi IRR = 1: La variable \\(X_j\\) no tiene efecto sobre la tasa de eventos.\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nVamos a utilizar R para ajustar un modelo de regresión de Poisson. Supongamos que tenemos datos sobre el número de accidentes de tráfico en diferentes intersecciones de una ciudad, junto con variables como el volumen de tráfico y la visibilidad.\n\n# Simulación de datos para el número de accidentes\nset.seed(456)  # Seed diferente para evitar duplicación\nn &lt;- 100  # Número de observaciones\n\n# Variables predictoras\ntrafico_nuevo &lt;- rnorm(n, mean = 1000, sd = 300)  # Volumen de tráfico en vehículos por día\nvisibilidad_nueva &lt;- rnorm(n, mean = 5, sd = 2)   # Visibilidad en kilómetros\n\n# Generar la tasa de accidentes (lambda) usando un modelo logarítmico\nlambda_nuevo &lt;- exp(0.01 * trafico_nuevo - 0.2 * visibilidad_nueva)\n\n# Generar el número de accidentes como una variable de Poisson\naccidentes_nuevo &lt;- rpois(n, lambda = lambda_nuevo)\n\n# Crear el data frame\ndatos_ejemplo &lt;- data.frame(accidentes = accidentes_nuevo, trafico = trafico_nuevo, visibilidad = visibilidad_nueva)\nhead(datos_ejemplo)\n\n  accidentes   trafico visibilidad\n1        140  596.9436    5.236303\n2      37153 1186.5327    6.739805\n3      92909 1240.2624    4.816128\n4        117  583.3323    5.137798\n5       1793  785.6929    1.635146\n6       1935  902.7817    7.233911\n\n# Ajustar el modelo de regresión de Poisson\nmodelo_ejemplo &lt;- glm(accidentes ~ trafico + visibilidad, data = datos_ejemplo, family = poisson)\n\n# Resumen del modelo\nsummary(modelo_ejemplo)\n\n\nCall:\nglm(formula = accidentes ~ trafico + visibilidad, family = poisson, \n    data = datos_ejemplo)\n\nCoefficients:\n              Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)  9.607e-04  2.316e-03     0.415    0.678    \ntrafico      9.999e-03  1.360e-06  7350.127   &lt;2e-16 ***\nvisibilidad -2.000e-01  1.012e-04 -1976.897   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1.6856e+08  on 99  degrees of freedom\nResidual deviance: 8.9300e+01  on 97  degrees of freedom\nAIC: 1220.4\n\nNumber of Fisher Scoring iterations: 3\n\n\nEl coeficiente asociado a trafico indica cómo el volumen de tráfico afecta la tasa de accidentes.\nEl coeficiente asociado a visibilidad muestra cómo la visibilidad afecta la frecuencia de accidentes.\n\n\n\n\n\n6.6.4 Estimación por máxima verosimilitud en regresión de Poisson\nLa estimación de parámetros en regresión de Poisson utiliza también el método de máxima verosimilitud, pero adaptado específicamente para la distribución de Poisson con función de enlace logarítmica. Este enfoque garantiza que las estimaciones aprovechen de manera óptima la información contenida en los datos de conteo.\nPara una muestra de \\(n\\) observaciones independientes, donde \\(y_i\\) representa el conteo de eventos para la observación \\(i\\), la función de verosimilitud se define como:\n\\[L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}\\]\ndonde \\(\\lambda_i = e^{\\mathbf{x}_i^T\\boldsymbol{\\beta}}\\) es la tasa esperada para la observación \\(i\\).\nLa log-verosimilitud correspondiente es:\n\\[\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[y_i \\log(\\lambda_i) - \\lambda_i - \\log(y_i!)\\right]\\]\nSustituyendo \\(\\lambda_i = e^{\\mathbf{x}_i^T\\boldsymbol{\\beta}}\\):\n\\[\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[y_i \\mathbf{x}_i^T\\boldsymbol{\\beta} - e^{\\mathbf{x}_i^T\\boldsymbol{\\beta}} - \\log(y_i!)\\right]\\]\nPara encontrar los valores de \\(\\boldsymbol{\\beta}\\) que maximizan la log-verosimilitud, derivamos respecto a cada parámetro:\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^{n} x_{ij}(y_i - \\lambda_i) = 0\\]\nEsta ecuación indica que los estimadores de máxima verosimilitud se obtienen cuando la suma de los residuos (observado menos esperado) ponderados por cada variable predictora es cero.\n\n6.6.4.1 Implementación del algoritmo IRLS\nDado que las ecuaciones de verosimilitud no tienen solución analítica cerrada, se utiliza el algoritmo IRLS adaptado para regresión de Poisson. Los elementos específicos son:\nPesos: \\[w_i = \\lambda_i\\]\nVariable dependiente ajustada: \\[z_i = \\log(\\lambda_i^{(t)}) + \\frac{y_i - \\lambda_i^{(t)}}{\\lambda_i^{(t)}}\\]\nActualización de parámetros: \\[\\boldsymbol{\\beta}^{(t+1)} = (\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{z}^{(t)}\\]\n\n\n6.6.4.2 Propiedades específicas de la estimación Poisson\nLa estimación por máxima verosimilitud en regresión de Poisson tiene características particulares que la distinguen de otros GLMs:\n\nEquidispersión: El modelo asume que \\(E(Y_i) = \\text{Var}(Y_i) = \\lambda_i\\), lo que significa que la varianza aumenta linealmente con la media.\nConvergencia: Generalmente requiere menos iteraciones que la regresión logística debido a la naturaleza más estable de la función de enlace logarítmica.\nEstabilidad numérica: La función de enlace logarítmica garantiza automáticamente que \\(\\lambda_i &gt; 0\\), evitando problemas de valores negativos en las tasas estimadas.\nInterpretación multiplicativa: Los coeficientes se interpretan como efectos multiplicativos sobre la tasa, lo que es natural para datos de conteo.\n\n\n\n\n\n\n\nEjemplo: Estimación paso a paso en regresión de Poisson\n\n\n\n\n\n\n# Demostración del proceso de estimación por máxima verosimilitud\n# Usar los datos simulados anteriores\nset.seed(123)\nn &lt;- 100\ntrafico &lt;- rnorm(n, mean = 1000, sd = 300)\nvisibilidad &lt;- rnorm(n, mean = 5, sd = 2)\nlambda &lt;- exp(-2 + 0.001*trafico - 0.2*visibilidad)\naccidentes &lt;- rpois(n, lambda)\ndatos_accidentes &lt;- data.frame(accidentes, trafico, visibilidad)\n\n# Función para calcular log-verosimilitud manualmente\nlog_likelihood_poisson &lt;- function(beta, X, y) {\n  eta &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  sum(y * log(lambda) - lambda - lgamma(y + 1))\n}\n\n# Preparar datos\nX &lt;- model.matrix(accidentes ~ trafico + visibilidad, data = datos_accidentes)\ny &lt;- datos_accidentes$accidentes\n\n# Ajuste con glm\nmodelo_glm_pois &lt;- glm(accidentes ~ trafico + visibilidad, \n                       data = datos_accidentes, family = poisson)\nbeta_glm_pois &lt;- coef(modelo_glm_pois)\n\ncat(\"=== ESTIMACIÓN POR MÁXIMA VEROSIMILITUD - POISSON ===\\n\")\n\n=== ESTIMACIÓN POR MÁXIMA VEROSIMILITUD - POISSON ===\n\ncat(\"Coeficientes estimados por glm():\\n\")\n\nCoeficientes estimados por glm():\n\nprint(beta_glm_pois)\n\n(Intercept)     trafico visibilidad \n-5.75588458  0.00284373  0.13082078 \n\n# Verificar optimización\nll_optimo_pois &lt;- log_likelihood_poisson(beta_glm_pois, X, y)\ncat(\"\\nLog-verosimilitud en el óptimo:\", round(ll_optimo_pois, 4), \"\\n\")\n\n\nLog-verosimilitud en el óptimo: -39.65 \n\n# Interpretación multiplicativa\ncat(\"\\nInterpretación multiplicativa (exp(coeficientes)):\\n\")\n\n\nInterpretación multiplicativa (exp(coeficientes)):\n\nexp_coefs &lt;- exp(beta_glm_pois)\nprint(exp_coefs)\n\n(Intercept)     trafico visibilidad \n0.003164106 1.002847777 1.139763495 \n\ncat(\"\\nInterpretación:\\n\")\n\n\nInterpretación:\n\ncat(\"- Intercepto: Tasa base =\", round(exp_coefs[1], 4), \"accidentes\\n\")\n\n- Intercepto: Tasa base = 0.0032 accidentes\n\ncat(\"- Tráfico: Por cada vehículo adicional, la tasa se multiplica por\", round(exp_coefs[2], 6), \"\\n\")\n\n- Tráfico: Por cada vehículo adicional, la tasa se multiplica por 1.002848 \n\ncat(\"- Visibilidad: Por cada km adicional de visibilidad, la tasa se multiplica por\", round(exp_coefs[3], 4), \"\\n\")\n\n- Visibilidad: Por cada km adicional de visibilidad, la tasa se multiplica por 1.1398 \n\n# Información de convergencia\ncat(\"\\nInformación del algoritmo IRLS:\\n\")\n\n\nInformación del algoritmo IRLS:\n\ncat(\"Iteraciones necesarias:\", modelo_glm_pois$iter, \"\\n\")\n\nIteraciones necesarias: 6 \n\ncat(\"¿Convergió?\", modelo_glm_pois$converged, \"\\n\")\n\n¿Convergió? TRUE \n\n# Verificar supuesto de equidispersión\nmedia_y &lt;- mean(y)\nvar_y &lt;- var(y)\ncat(\"\\nVerificación de equidispersión:\\n\")\n\n\nVerificación de equidispersión:\n\ncat(\"Media observada:\", round(media_y, 3), \"\\n\")\n\nMedia observada: 0.15 \n\ncat(\"Varianza observada:\", round(var_y, 3), \"\\n\")\n\nVarianza observada: 0.149 \n\ncat(\"Razón varianza/media:\", round(var_y/media_y, 3), \"\\n\")\n\nRazón varianza/media: 0.993 \n\n\n\n\n\n\n\n\n6.6.5 Bondad de ajuste en la regresión de Poisson\nAl igual que en la regresión logística, la bondad de ajuste en los modelos de Poisson se aleja del \\(R^2\\) tradicional y se centra en medidas basadas en la verosimilitud. El objetivo es cuantificar si el modelo captura adecuadamente la estructura de los datos de conteo.\nLa deviance sigue siendo la métrica fundamental. Para la regresión de Poisson, se calcula comparando la log-verosimilitud del modelo ajustado con la de un modelo saturado (donde \\(\\hat{\\mu}_i = y_i\\)). La fórmula específica es: \\[D = 2 \\sum_{i=1}^{n} \\left[ y_i \\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - (y_i - \\hat{\\mu}_i) \\right]\\] Donde el término \\(y_i \\log(y_i)\\) se considera cero si \\(y_i = 0\\). Al igual que en otros GLMs, la deviance es clave para comparar modelos anidados mediante el Test de la Razón de Verosimilitudes (LRT).\nSin embargo, para los modelos de Poisson, la prueba de bondad de ajuste más importante en la práctica es la evaluación de la sobredispersión. Un buen ajuste del modelo de Poisson implica que se cumple el supuesto de equidispersión (\\(Var(Y) = \\mu\\)). Por lo tanto, el estadístico de dispersión (\\(\\hat{\\phi}\\)) se convierte en una medida de facto de la bondad de ajuste: \\[\\hat{\\phi} = \\frac{X^2_{\\text{Pearson}}}{n-p}\\] Un valor de \\(\\hat{\\phi}\\) cercano a 1 indica que el supuesto de la distribución de Poisson se cumple y que el ajuste del modelo es adecuado. Si \\(\\hat{\\phi}\\) es significativamente mayor que 1, el modelo no se ajusta bien a la variabilidad de los datos, y este es el principal indicador para buscar alternativas como la regresión binomial negativa.\nAunque los pseudo R² (como el de McFadden) pueden calcularse, son menos utilizados e informativos en el contexto de la regresión de Poisson en comparación con el análisis de la dispersión.\n\n\n6.6.6 Validación del modelo de Poisson\nA diferencia de la regresión logística, donde la validación se centra en la capacidad de clasificación, la validación de un modelo de Poisson se enfoca en su capacidad de predicción: ¿qué tan cerca están los conteos predichos por el modelo de los conteos reales observados?\nEl proceso de validación suele implicar la división de los datos en un conjunto de entrenamiento (train) y uno de prueba (test). El modelo se ajusta con los datos de entrenamiento y su rendimiento predictivo se evalúa sobre los datos de prueba, que el modelo no ha visto antes. Esto nos da una estimación honesta de cómo generalizará el modelo a nuevos datos.\nLas métricas de validación principales para modelos de conteo son:\n\nRaíz del Error Cuadrático Medio (RMSE): Es una de las métricas más comunes y mide la desviación estándar de los residuos. Penaliza más los errores grandes. \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{\\mu}_i)^2}\n\\]\nError Absoluto Medio (MAE): Mide la magnitud promedio de los errores, siendo menos sensible a valores atípicos que el RMSE. \\[\n\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{\\mu}_i|\n\\]\n\nAmbas métricas se expresan en las mismas unidades que la variable de respuesta (por ejemplo, “accidentes”, “compras”), lo que facilita su interpretación. Un modelo con valores de RMSE y MAE más bajos en el conjunto de prueba se considera que tiene un mejor rendimiento predictivo.\nUna herramienta visual clave para la validación es el gráfico de valores predichos vs. valores reales en el conjunto de prueba. En un modelo con buena capacidad predictiva, los puntos deberían agruparse cerca de la línea diagonal \\(y=x\\), indicando que las predicciones (\\(\\hat{\\mu}_i\\)) son cercanas a los valores observados (\\(y_i\\)).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema5.html#otros-glms",
    "href": "tema5.html#otros-glms",
    "title": "6  Modelos de regresión generalizada",
    "section": "6.7 Otros GLMs",
    "text": "6.7 Otros GLMs\nLa regresión binomial negativa y los modelos basados en distribuciones como Gamma e Inversa Gaussiana amplían la capacidad de los Modelos Lineales Generalizados (GLM) para adaptarse a una amplia variedad de situaciones del mundo real. Estos modelos son especialmente útiles cuando los datos presentan características como sobredispersión, sesgo o restricciones en el dominio (por ejemplo, solo valores positivos). La elección adecuada del modelo y la función de enlace garantiza predicciones precisas y válidas, contribuyendo a la toma de decisiones informadas en campos como la salud, la ingeniería y la economía.\n\n6.7.1 Regresión binomial negativa\nTal y como hemos visto en apartados anteriores, la sobredispersión ocurre cuando la varianza de los datos de conteo es mayor que la media, lo cual viola uno de los supuestos clave de la regresión de Poisson, que asume que la media y la varianza son iguales (\\(E(Y) = Var(Y) = \\lambda\\)). La sobredispersión puede surgir por varias razones:\n\nHeterogeneidad no modelada: Existen factores que afectan la variable dependiente pero no han sido incluidos en el modelo.\nDependencia entre eventos: Los eventos no ocurren de forma independiente.\nExceso de ceros: Hay más ceros en los datos de los que predice la distribución de Poisson.\n\nCuando la sobredispersión está presente, la regresión de Poisson subestima los errores estándar, lo que puede llevar a conclusiones incorrectas sobre la significancia de los predictores.\nLa regresión binomial negativa es una extensión de la regresión de Poisson que introduce un parámetro adicional para manejar la sobredispersión. Este modelo permite que la varianza sea mayor que la media:\n\\[\nVar(Y) = \\lambda + \\alpha \\lambda^2\n\\]\nDonde \\(\\alpha\\) es el parámetro de dispersión. Si \\(\\alpha = 0\\), el modelo se reduce a la regresión de Poisson.\nLa forma funcional del modelo binomial negativo es similar al de Poisson:\n\\[\n\\log(\\lambda) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\nPero la varianza ahora incluye el término adicional \\(\\alpha\\) para capturar la sobredispersión.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Instalar y cargar la librería MASS que contiene la función glm.nb\nlibrary(MASS)\n\n# Usar los datos de accidentes del ejemplo anterior\n# Ajuste de un modelo binomial negativo\nmodelo_binom_neg &lt;- glm.nb(accidentes ~ trafico + visibilidad, data = datos_accidentes)\n\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\n\n# Resumen del modelo\nsummary(modelo_binom_neg)\n\n\nCall:\nglm.nb(formula = accidentes ~ trafico + visibilidad, data = datos_accidentes, \n    init.theta = 2158.536326, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.7560791  1.5223127  -3.781 0.000156 ***\ntrafico      0.0028439  0.0009831   2.893 0.003818 ** \nvisibilidad  0.1308306  0.1402185   0.933 0.350795    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2158.536) family taken to be 1)\n\n    Null deviance: 59.679  on 99  degrees of freedom\nResidual deviance: 50.680  on 97  degrees of freedom\nAIC: 87.301\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2159 \n          Std. Err.:  49427 \nWarning while fitting theta: iteration limit reached \n\n 2 x log-likelihood:  -79.301 \n\n# Comparar la dispersión con el modelo de Poisson\ncat(\"Dispersión en Poisson (deviance/df.residual):\", round(modelo_glm_pois$deviance / modelo_glm_pois$df.residual, 3), \"\\n\")\n\nDispersión en Poisson (deviance/df.residual): 0.523 \n\ncat(\"Parámetro theta en Binomial Negativa:\", round(modelo_binom_neg$theta, 3), \"\\n\")\n\nParámetro theta en Binomial Negativa: 2158.536 \n\n# Comparar AIC\ncat(\"AIC Poisson:\", round(AIC(modelo_glm_pois), 2), \"\\n\")\n\nAIC Poisson: 85.3 \n\ncat(\"AIC Binomial Negativa:\", round(AIC(modelo_binom_neg), 2), \"\\n\")\n\nAIC Binomial Negativa: 87.3 \n\n\n\n\n\nInterpretación del parámetro \\(\\theta\\) en binomial negativa:\n\nEl parámetro \\(\\theta\\) controla el grado de sobredispersión en el modelo\nValores altos de \\(\\theta\\) (ej: \\(\\theta &gt; 100\\)): Poca sobredispersión, el modelo se aproxima a Poisson\nValores bajos de \\(\\theta\\) (ej: \\(\\theta &lt; 10\\)): Mucha sobredispersión, diferencia significativa respecto a Poisson\nRegla práctica: Si \\(\\theta\\) es pequeño, confirma que había sobredispersión y que el modelo binomial negativa es más apropiado que Poisson\n\nComparación de modelos:\n\nSi AIC de binomial negativa &lt; AIC de Poisson → preferir binomial negativa\nEl modelo binomial negativa corrige la subestimación de errores estándar que ocurre en Poisson con sobredispersión\n\n\n\n6.7.2 Modelos para variables continuas no normales\nExisten situaciones en las que la variable dependiente es continua, pero no sigue una distribución normal. En estos casos, los Modelos Lineales Generalizados (GLM) permiten utilizar distribuciones alternativas como Gamma o Inversa Gaussiana, junto con funciones de enlace específicas.\n\n6.7.2.1 Regresión gamma para datos positivos y sesgados\nLa regresión Gamma es adecuada para modelar variables continuas que son positivas y tienen una distribución sesgada a la derecha. Ejemplos típicos incluyen tiempos de espera, costos médicos o duración de procesos.\n\nLa distribución Gamma asume que la variable dependiente es continua y positiva.\nLa varianza de la variable dependiente aumenta proporcionalmente al cuadrado de la media.\n\nFunción de Enlace Común: \\[\\log(\\mu) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Simulación de costos médicos\nset.seed(123)\nn &lt;- 100\ningresos &lt;- rnorm(n, mean = 50000, sd = 10000)\nedad &lt;- rnorm(n, mean = 45, sd = 10)\ncostos &lt;- rgamma(n, shape = 2, rate = 0.00005 * ingresos + 0.01 * edad)\n\n# Ajuste del modelo Gamma\nmodelo_gamma &lt;- glm(costos ~ ingresos + edad, family = Gamma(link = \"log\"))\n\n# Resumen del modelo\nsummary(modelo_gamma)\n\n\nCall:\nglm(formula = costos ~ ingresos + edad, family = Gamma(link = \"log\"))\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  3.440e-01  5.294e-01   0.650   0.5173  \ningresos    -1.807e-05  7.804e-06  -2.316   0.0227 *\nedad         3.584e-03  7.366e-03   0.487   0.6277  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.5010938)\n\n    Null deviance: 60.771  on 99  degrees of freedom\nResidual deviance: 58.345  on 97  degrees of freedom\nAIC: 105.47\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nLos coeficientes muestran cómo los ingresos y la edad afectan los costos médicos esperados.\nEl enlace logarítmico asegura que las predicciones sean siempre positivas.\n\n\n\n\n\n\n6.7.2.2 Regresión inversa gaussiana\nLa regresión Inversa Gaussiana es útil para modelar tiempos de respuesta o variables donde la varianza disminuye rápidamente a medida que la media aumenta. Este modelo se aplica en campos como la ingeniería, donde se analizan tiempos hasta fallas de sistemas.\nFunción de Enlace Común: \\[\\frac{1}{\\mu^2} = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Instalar y cargar la librería correcta\nlibrary(statmod)\n\n# Simulación de datos\nset.seed(123)\nn &lt;- 100\ncarga_trabajo &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Generar tiempos hasta el fallo usando la distribución inversa gaussiana\n# Aseguramos que los valores de carga_trabajo sean positivos para evitar problemas numéricos\ncarga_trabajo[carga_trabajo &lt;= 0] &lt;- 1\ntiempo_fallo &lt;- rinvgauss(n, mean = 100 / carga_trabajo, dispersion = 1)\n\n# Ajuste del modelo Inversa Gaussiana con enlace logarítmico\nmodelo_inversa_gauss &lt;- glm(tiempo_fallo ~ carga_trabajo, family = inverse.gaussian(link = \"1/mu^2\"),start = c(0.01, 0.01))\n\nWarning in sqrt(eta): NaNs produced\n\n\nWarning: step size truncated due to divergence\n\n# Resumen del modelo\nsummary(modelo_inversa_gauss)\n\n\nCall:\nglm(formula = tiempo_fallo ~ carga_trabajo, family = inverse.gaussian(link = \"1/mu^2\"), \n    start = c(0.01, 0.01))\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   -0.106143   0.462230  -0.230    0.819\ncarga_trabajo  0.008261   0.009623   0.859    0.393\n\n(Dispersion parameter for inverse.gaussian family taken to be 1.348442)\n\n    Null deviance: 94.085  on 99  degrees of freedom\nResidual deviance: 93.171  on 98  degrees of freedom\nAIC: 294.2\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\n\n\n\n¿Qué GLM debo usar?\n\n\n\nLa elección del modelo correcto depende casi exclusivamente de la naturaleza de tu variable respuesta (\\(Y\\)). Aquí tienes una guía rápida:\n\n¿Tu variable respuesta es binaria (Sí/No, 0/1, Éxito/Fracaso)?\n\nUsa Regresión Logística.\n\n¿Tu variable respuesta es un conteo de eventos (nº de accidentes, nº de clientes, nº de fallos)?\n\nEmpieza con una Regresión de Poisson.\nImportante: Después de ajustar el modelo, comprueba si hay sobredispersión.\nSi la hay (estadístico de dispersión \\(\\hat{\\phi} &gt; 1.5\\) o la teoría lo sugiere), cambia a una Regresión Binomial Negativa.\n\n¿Tu variable respuesta es continua y estrictamente positiva, con una distribución asimétrica hacia la derecha (ej. tiempos, costos, reclamaciones de seguros)?\n\nUsa Regresión Gamma. Es una excelente alternativa a transformar la variable con logaritmos y usar un modelo lineal.\n\n¿Tu variable respuesta es un tiempo hasta un evento y tiene una asimetría muy pronunciada?\n\nConsidera una Regresión Inversa Gaussiana.\n\n\n\n\n\n\n\n\nCoxe, Stefany, Stephen G West, y Leona S Aiken. 2009. «The analysis of count data: A gentle introduction to Poisson regression and its alternatives». Journal of personality assessment 91 (2): 121-36.\n\n\nHosmer Jr, David W, Stanley Lemeshow, y Rodney X Sturdivant. 2013. Applied logistic regression. John Wiley & Sons.\n\n\nLambert, Diane. 1992. «Zero-inflated Poisson regression, with an application to defects in manufacturing». Technometrics 34 (1): 1-14.\n\n\nNelder, John Ashworth, y Robert WM Wedderburn. 1972. «Generalized linear models». Journal of the Royal Statistical Society Series A: Statistics in Society 135 (3): 370-84.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "conclusiones.html",
    "href": "conclusiones.html",
    "title": "7  Conclusiones",
    "section": "",
    "text": "7.1 Resumen de los aprendizajes\nA lo largo de seis capítulos hemos presentado el material de la asignatura de Modelos Estadísticos para la Predicción del Grado en Matemáticas.\nEn este capítulo final, te invitamos a reflexionar sobre las principales lecciones aprendidas durante el curso y a destacar cómo el rigor matemático que caracteriza vuestra formación se convierte en una herramienta indispensable para dominar el modelado estadístico. Además, animamos a los estudiantes a continuar explorando y ampliando sus conocimientos en cursos posteriores, consolidando así una base sólida para afrontar los desafíos de la ciencia de datos moderna desde una perspectiva analítica y fundamentada.\nAl finalizar este recorrido, queda patente que el modelado estadístico es mucho más que una colección de técnicas; es un marco de pensamiento estructurado para comprender la incertidumbre y extraer conocimiento a partir de los datos. Hemos transitado desde los axiomas teóricos de la regresión hasta su aplicación computacional, equipando a los futuros matemáticos con las herramientas para construir, validar e interpretar modelos robustos.\nA lo largo de este manual, hemos construido un conocimiento progresivo sobre el modelado predictivo, cubriendo los siguientes pilares:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "conclusiones.html#resumen-de-los-aprendizajes",
    "href": "conclusiones.html#resumen-de-los-aprendizajes",
    "title": "7  Conclusiones",
    "section": "",
    "text": "Fundamentos del modelado lineal: simple y múltiple: Hemos partido de la formulación teórica del modelo lineal, estableciendo sus componentes axiomáticos y los supuestos de Gauss-Markov que garantizan las propiedades óptimas de los estimadores de Mínimos Cuadrados Ordinarios (MCO). Se ha hecho hincapié en la transición del modelo simple al múltiple, destacando el principio de ceteris paribus para la interpretación de coeficientes, el diagnóstico de la multicolinealidad mediante el VIF y la evaluación de la bondad de ajuste a través de la descomposición ANOVA y el \\(R^2\\) ajustado.\nIngeniería de características y flexibilidad del modelo: Exploramos cómo superar las limitaciones de un modelo estrictamente lineal. Aprendimos a diagnosticar y corregir violaciones de los supuestos mediante transformaciones de variables (logarítmica, Box-Cox), a incorporar predictores no numéricos a través de la codificación de variables categóricas, y, fundamentalmente, a modelar relaciones complejas mediante la inclusión de términos de interacción, entendiendo cómo el efecto de un predictor puede depender del valor de otro.\nSelección de variables, regularización y validación: Abordamos el crucial dilema sesgo-varianza y la necesidad de construir modelos parsimoniosos que generalicen bien a datos no observados. Se presentaron los criterios de información (AIC, BIC) y los métodos por pasos (stepwise) como herramientas para comparar y seleccionar modelos. Profundizamos en los métodos de regularización (Ridge, Lasso y Elastic Net), que ofrecen una alternativa moderna y robusta para manejar la multicolinealidad y realizar selección de variables de forma simultánea, especialmente en contextos de alta dimensionalidad. Finalmente, se consolidó la importancia de la validación cruzada como el estándar para una evaluación honesta del rendimiento predictivo del modelo.\nModelos lineales generalizados (GLM): Expandimos el marco de la regresión más allá de la asunción de normalidad en la variable respuesta. A través de la introducción de la familia exponencial de distribuciones y las funciones de enlace, entendimos cómo adaptar el modelo lineal a diferentes tipos de datos. Nos centramos en dos de los GLM más importantes: la Regresión Logística para modelar resultados binarios y la Regresión de Poisson para datos de conteo, aprendiendo a interpretar sus coeficientes en términos de odds ratios y tasas de eventos, respectivamente.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "conclusiones.html#reflexiones-finales",
    "href": "conclusiones.html#reflexiones-finales",
    "title": "7  Conclusiones",
    "section": "7.2 Reflexiones finales",
    "text": "7.2 Reflexiones finales\nEl estudio de los Modelos Estadísticos para la Predicción dota al matemático de un puente entre la teoría abstracta y la resolución de problemas del mundo real. A lo largo de este curso, hemos visto cómo conceptos rigurosos —espacios vectoriales en la geometría de MCO, optimización en la estimación de parámetros, y teoría de la probabilidad en la inferencia— se materializan en herramientas prácticas para la toma de decisiones bajo incertidumbre.\nHemos aprendido que construir un modelo no es un acto mecánico, sino un proceso iterativo de diagnóstico, crítica y refinamiento. La capacidad para evaluar la validez de los supuestos, interpretar los resultados con cautela y comunicar tanto las fortalezas como las limitaciones de un modelo es lo que distingue a un analista competente. La interpretabilidad y la validación rigurosa no son meros pasos finales, sino el núcleo de una práctica estadística honesta y efectiva.\nEn un mundo saturado de datos, la habilidad para construir modelos que no solo predicen, sino que también explican y ofrecen certidumbre cuantificable, es más valiosa que nunca.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "conclusiones.html#proyección-futura-el-valor-del-rigor-matemático",
    "href": "conclusiones.html#proyección-futura-el-valor-del-rigor-matemático",
    "title": "7  Conclusiones",
    "section": "7.3 Proyección futura: El valor del rigor matemático",
    "text": "7.3 Proyección futura: El valor del rigor matemático\nLas competencias adquiridas en esta asignatura son la culminación de vuestra carrera, el punto donde el álgebra lineal, el cálculo y la optimización se convierten en el motor de la modelización estadística aplicada. Vuestra formación matemática os proporciona una ventaja fundamental: la capacidad de ir más allá de la aplicación mecánica de un algoritmo para comprender en profundidad los supuestos que lo sustentan, la geometría de su funcionamiento y la incertidumbre inherente a sus conclusiones.\nConceptos como la regularización y la validación cruzada son el lenguaje compartido con el Aprendizaje Automático. Mientras que el Aprendizaje Automático a menudo se centra en la potencia predictiva de algoritmos complejos, este curso os ha proporcionado la “gramática” estadística para construir modelos interpretables, diagnosticar su validez y cuantificar la fiabilidad de sus resultados. Esta base teórica es indispensable para aplicar, y en un futuro desarrollar, cualquier técnica de modelado de forma rigurosa y responsable.\nEsta habilidad para analizar críticamente los modelos es precisamente lo que el mercado y el mundo académico demandan. Os posiciona de manera ideal para roles avanzados como Científico de Datos, Analista Cuantitativo (‘Quant’) en el sector financiero, o Bioestadístico, así como para continuar vuestra formación con estudios de postgrado (Máster o Doctorado) donde la investigación y el desarrollo de nuevos métodos es primordial.\nEn definitiva, habéis adquirido un conjunto de herramientas analíticas que os permitirá traducir problemas complejos en modelos manejables y basados en evidencia.\n¡Mucha suerte en vuestra trayectoria profesional!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliografía",
    "section": "",
    "text": "Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015.\n“Fitting Linear Mixed-Effects Models Using Lme4.”\nJournal of Statistical Software 67 (1): 1–48.\n\n\nBox, George EP, and David R Cox. 1964. “An Analysis of\nTransformations.” Journal of the Royal Statistical Society:\nSeries B (Methodological) 26 (2): 211–43.\n\n\nCarroll, Raymond J, and David Ruppert. 1988. “Transformation and\nWeighting in Regression.” Monographs on Statistics and\nApplied Probability.\n\n\nCoxe, Stefany, Stephen G West, and Leona S Aiken. 2009. “The\nAnalysis of Count Data: A Gentle Introduction to Poisson Regression and\nIts Alternatives.” Journal of Personality Assessment 91\n(2): 121–36.\n\n\nDraper, NR. 1998. Applied Regression Analysis. McGraw-Hill.\nInc.\n\n\nFox, John, and Sanford Weisberg. 2018. An r Companion to Applied\nRegression. Sage publications.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in\nHereditary Stature.” The Journal of the Anthropological\nInstitute of Great Britain and Ireland 15: 246–63.\n\n\nHarrell, Frank E., Jr. 2015. Regression Modeling Strategies: With\nApplications to Linear Models, Logistic and Ordinal Regression, and\nSurvival Analysis. Second. Springer.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H\nFriedman. 2009. The Elements of Statistical Learning: Data Mining,\nInference, and Prediction. Vol. 2. Springer.\n\n\nHosmer Jr, David W, Stanley Lemeshow, and Rodney X Sturdivant. 2013.\nApplied Logistic Regression. John Wiley & Sons.\n\n\nJaccard, James, and Robert Turrisi. 2003. Interaction Effects in\nMultiple Regression. Sage.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112.\nSpringer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning with Applications in\nr. Second. Springer.\n\n\nKuhn, Max, and Kjell Johnson. 2019. Feature Engineering and\nSelection: A Practical Approach for Predictive Models. CRC Press.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li.\n2005. Applied Linear Statistical Models. McGraw-hill.\n\n\nLambert, Diane. 1992. “Zero-Inflated Poisson Regression, with an\nApplication to Defects in Manufacturing.” Technometrics\n34 (1): 1–14.\n\n\nMarquardt, Donald W, and Ronald D Snee. 1975. “Ridge Regression in\nPractice.” The American Statistician 29 (1): 3–20.\n\n\nNelder, John Ashworth, and Robert WM Wedderburn. 1972.\n“Generalized Linear Models.” Journal of the Royal\nStatistical Society Series A: Statistics in Society 135 (3):\n370–84.\n\n\nPinheiro, José C., and Douglas M. Bates. 2000. Mixed-Effects Models\nin s and s-PLUS. New York: Springer.\n\n\nPotdar, Kedar, Taher S Pardawala, and Chinmay D Pai. 2017. “A\nComparative Study of Categorical Variable Encoding Techniques for Neural\nNetwork Classifiers.” International Journal of Computer\nApplications 175 (4): 7–9.\n\n\nRanstam, Jonas, and Jonathan A Cook. 2018. “LASSO\nRegression.” Journal of British Surgery 105 (10):\n1348–48.\n\n\nShmueli, Galit. 2010. “To Explain or to Predict?”\nStatistical Science 25 (3): 289–310.\n\n\nWeisberg, S. 2005. “Applied Linear Regression.” Wiley.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction\nwith r. Second. Chapman; Hall/CRC.\n\n\nYeo, In-Kwon, and Richard A Johnson. 2000. “A New Family of Power\nTransformations to Improve Normality or Symmetry.”\nBiometrika 87 (4): 954–59.\n\n\nZheng, Alice, and Amanda Casari. 2018. Feature Engineering for\nMachine Learning: Principles and Techniques for Data Scientists.\nO’Reilly Media.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable\nSelection via the Elastic Net.” Journal of the Royal\nStatistical Society Series B: Statistical Methodology 67 (2):\n301–20.",
    "crumbs": [
      "Bibliografía"
    ]
  }
]